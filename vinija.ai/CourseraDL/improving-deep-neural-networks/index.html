<!DOCTYPE html>
<html lang="en">

  
<!-- Mirrored from vinija.ai/CourseraDL/improving-deep-neural-networks/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 12 Jul 2025 14:02:12 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Vinija's Notes • Coursera-DL • Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Vinija's detailed AI Notes">
  <link rel="canonical" href="index.html">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Vinija's AI Notes" href="../../feed.xml">
  
  <link href="../../favicon.html" rel="shortcut icon" />

  <!-- Google ads -->
  <script async src="../../../pagead2.googlesyndication.com/pagead/js/ff0a8.txt?client=ca-pub-5905744527956213"
     crossorigin="anonymous"></script>
</head>



    <body>

      <script src="../../../unpkg.com/vanilla-back-to-top%407.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../index-2.html">Vinija's AI Notes</a>

  <a class="site-link" href="../../index.html">Back to vinija.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="../../js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://vinija.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Coursera-DL • Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#practical-aspects-of-dl" id="markdown-toc-practical-aspects-of-dl">Practical aspects of DL</a></li>
  <li><a href="#bias--variance" id="markdown-toc-bias--variance">Bias / Variance</a>    <ul>
      <li><a href="#examples" id="markdown-toc-examples">Examples</a></li>
    </ul>
  </li>
  <li><a href="#diagnosing-and-treating-bias-and-variance-in-neural-networks-the-recipe-" id="markdown-toc-diagnosing-and-treating-bias-and-variance-in-neural-networks-the-recipe-">Diagnosing and Treating Bias and Variance in Neural Networks: The Recipe-</a></li>
  <li><a href="#weights-and-bias-vs-bias-and-variance" id="markdown-toc-weights-and-bias-vs-bias-and-variance">Weights and Bias vs Bias and Variance</a></li>
  <li><a href="#regularization" id="markdown-toc-regularization">Regularization</a></li>
  <li><a href="#intuition-behind-dropout" id="markdown-toc-intuition-behind-dropout">Intuition Behind Dropout</a>    <ul>
      <li><a href="#potential-drawbacks-and-considerations" id="markdown-toc-potential-drawbacks-and-considerations">Potential Drawbacks and Considerations</a></li>
    </ul>
  </li>
  <li><a href="#inverted-dropout" id="markdown-toc-inverted-dropout">Inverted dropout</a>    <ul>
      <li><a href="#implementation-of-inverted-dropout" id="markdown-toc-implementation-of-inverted-dropout">Implementation of Inverted Dropout</a>        <ul>
          <li><a href="#forward-propagation" id="markdown-toc-forward-propagation">Forward Propagation</a></li>
          <li><a href="#backward-propagation" id="markdown-toc-backward-propagation">Backward Propagation</a></li>
          <li><a href="#test-time" id="markdown-toc-test-time">Test Time</a></li>
          <li><a href="#notes" id="markdown-toc-notes">Notes</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#other-techniques-for-reducing-overfitting-in-neural-networks" id="markdown-toc-other-techniques-for-reducing-overfitting-in-neural-networks">Other Techniques for Reducing Overfitting in Neural Networks</a>    <ul>
      <li><a href="#normalizing-input-features" id="markdown-toc-normalizing-input-features">Normalizing Input Features</a></li>
    </ul>
  </li>
  <li><a href="#vanishing--exploding-gradients" id="markdown-toc-vanishing--exploding-gradients">Vanishing / Exploding gradients</a></li>
  <li><a href="#gradient-checking-in-neural-networks---overview-and-practical-tips" id="markdown-toc-gradient-checking-in-neural-networks---overview-and-practical-tips">Gradient Checking in Neural Networks - Overview and Practical Tips</a></li>
  <li><a href="#mini-batch-gradient-descent-explained" id="markdown-toc-mini-batch-gradient-descent-explained">Mini-batch Gradient Descent Explained</a>    <ul>
      <li><a href="#batch-gradient-descent-vs-stochastic-gradient-descent" id="markdown-toc-batch-gradient-descent-vs-stochastic-gradient-descent">Batch Gradient Descent vs. Stochastic Gradient Descent</a></li>
      <li><a href="#behavior-of-gradient-descent-algorithms" id="markdown-toc-behavior-of-gradient-descent-algorithms">Behavior of Gradient Descent Algorithms</a></li>
      <li><a href="#choosing-mini-batch-size" id="markdown-toc-choosing-mini-batch-size">Choosing Mini-batch Size</a></li>
      <li><a href="#efficiency--speed" id="markdown-toc-efficiency--speed">Efficiency &amp; Speed</a></li>
    </ul>
  </li>
  <li><a href="#gradient-descent-with-momentum" id="markdown-toc-gradient-descent-with-momentum">Gradient descent with momentum</a></li>
  <li><a href="#rmsprop-root-mean-square-prop" id="markdown-toc-rmsprop-root-mean-square-prop">RMSprop (Root Mean Square prop)</a>    <ul>
      <li><a href="#adam-optimization-algorithm" id="markdown-toc-adam-optimization-algorithm">Adam Optimization Algorithm</a>        <ul>
          <li><a href="#concepts" id="markdown-toc-concepts">Concepts</a></li>
          <li><a href="#bias-correction" id="markdown-toc-bias-correction">Bias Correction</a></li>
          <li><a href="#parameter-update" id="markdown-toc-parameter-update">Parameter Update</a></li>
          <li><a href="#hyperparameters" id="markdown-toc-hyperparameters">Hyperparameters</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#learning-rate-decay" id="markdown-toc-learning-rate-decay">Learning Rate Decay</a>    <ul>
      <li><a href="#methods-for-implementing-learning-rate-decay" id="markdown-toc-methods-for-implementing-learning-rate-decay">Methods for Implementing Learning Rate Decay</a></li>
      <li><a href="#considerations" id="markdown-toc-considerations">Considerations</a></li>
    </ul>
  </li>
  <li><a href="#hyperparameter-tuning-guidelines" id="markdown-toc-hyperparameter-tuning-guidelines">Hyperparameter Tuning Guidelines</a>    <ul>
      <li><a href="#sampling-hyperparameters" id="markdown-toc-sampling-hyperparameters">Sampling Hyperparameters</a></li>
      <li><a href="#organizing-your-hyperparameter-search-process" id="markdown-toc-organizing-your-hyperparameter-search-process">Organizing Your Hyperparameter Search Process</a></li>
      <li><a href="#batchnorm" id="markdown-toc-batchnorm">BatchNorm</a></li>
    </ul>
  </li>
  <li><a href="#how-does-batch-normalization-work" id="markdown-toc-how-does-batch-normalization-work">How does Batch Normalization work?</a></li>
  <li><a href="#the-mechanics-of-batch-normalization" id="markdown-toc-the-mechanics-of-batch-normalization">The Mechanics of Batch Normalization:</a>    <ul>
      <li><a href="#batch-normalization-batchnorm-in-deep-neural-networks" id="markdown-toc-batch-normalization-batchnorm-in-deep-neural-networks">Batch Normalization (BatchNorm) in Deep Neural Networks:</a></li>
      <li><a href="#softmax-regression-and-multi-class-classification" id="markdown-toc-softmax-regression-and-multi-class-classification">Softmax Regression and Multi-Class Classification</a></li>
      <li><a href="#deepening-understanding-of-softmax-classification-and-training" id="markdown-toc-deepening-understanding-of-softmax-classification-and-training">Deepening Understanding of Softmax Classification and Training</a></li>
    </ul>
  </li>
</ul>

<h2 id="overview">Overview</h2>
<ul>
  <li>In this article, we will look to learn practical and best practices while training a deep neural network.</li>
  <li>We will look at topics such as bias/variance, L2, dropout regularization, hyperparameter tuning etc.</li>
</ul>

<h2 id="practical-aspects-of-dl">Practical aspects of DL</h2>
<ol>
  <li><strong>Iterative Process of Model Development:</strong>
    <ul>
      <li>Developing deep learning models often requires continuous iteration, especially when you’re figuring out the best architecture or hyperparameters.</li>
      <li>Factors to iterate over can include network depth, number of neurons per layer, learning rates, regularization terms, and activation functions.</li>
    </ul>
  </li>
  <li><strong>Domain-specific Intuition:</strong>
    <ul>
      <li>Transferability of knowledge across domains is limited. For instance, the hyperparameters that work best for Natural Language Processing (NLP) models might not be optimal for Computer Vision (CV) tasks.</li>
      <li>Different applications have unique characteristics; an NLP task might be sequential, while a CV task often deals with spatial hierarchies.</li>
    </ul>
  </li>
  <li><strong>Data Splitting in the Age of Big Data:</strong>
    <ul>
      <li>In traditional machine learning, a common split was 70/30 or 60/20/20 for train/dev/test sets. However, with large datasets in the deep learning era, it’s more common to see splits like 98/1/1 or even 99.5/0.25/0.25.</li>
      <li>The goal is to have dev and test sets large enough to provide reliable performance evaluations without unnecessarily reducing the size of the training set.</li>
    </ul>
  </li>
  <li><strong>Ensuring Consistent Distributions for Dev/Test Sets:</strong>
    <ul>
      <li>While training data might come from different sources (e.g., web-crawled images vs. user-uploaded images), it’s critical that dev and test datasets come from the same distribution to ensure that model improvements generalize well to unseen data.</li>
      <li>This consistency ensures that performance metrics on the dev set are representative of the performance on the test set.</li>
    </ul>
  </li>
  <li><strong>Test Set: Not Always Mandatory:</strong>
    <ul>
      <li>While the test set provides an unbiased evaluation of the final model, it’s sometimes omitted when such an evaluation isn’t crucial.</li>
      <li>Without a test set, the risk is overfitting to the dev set, making it essential to use the dev set judiciously and avoid using it excessively for hyperparameter tuning.</li>
    </ul>
  </li>
  <li><strong>Efficiency in Model Development:</strong>
    <ul>
      <li>Rapid iteration is vital. The faster a model can be trained, evaluated, and refined, the quicker optimal performance can be achieved.</li>
      <li>Efficient dataset setup, including thoughtful allocation between train/dev/test sets and ensuring consistency in data distributions, is foundational for rapid model iteration.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>This deeper dive emphasizes the underlying technical nuances involved in setting up, training, and evaluating deep learning models in real-world scenarios.</li>
</ul>

<h2 id="bias--variance">Bias / Variance</h2>

<p><img src="../assets/practicalDL/1.png" alt="" /></p>

<p>Th<strong>e Essence of Bias and Variance:</strong></p>
<ul>
  <li>Bias: It is the error due to overly simplistic assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant patterns, leading to errors on both the training data (underfitting) and test data.</li>
  <li>Variance: It refers to the error due to too much complexity in the learning algorithm. High variance can cause an algorithm to model random noise in the training data, leading to a model that’s overly tailored to that data and performs poorly on new, unseen data (overfitting).</li>
</ul>

<ol>
  <li><strong>The Evolution in Deep Learning:</strong>
    <ul>
      <li>In the past, there was a widely accepted notion that there was an unavoidable trade-off between bias and variance.</li>
      <li>However, with the advent of deep learning and the ability to build very large neural networks, we’re finding that we can often reduce both bias and variance simultaneously, shifting away from the traditional trade-off paradigm.</li>
    </ul>
  </li>
  <li><strong>Visual Representation:</strong>
    <ul>
      <li>For datasets with two dimensions:
        <ul>
          <li>A high bias model might draw a straight line through a dataset that clearly has a curve or nonlinear shape. It doesn’t capture the true distribution and performs poorly.</li>
          <li>A high variance model might draw a wiggly curve that passes through every single data point, even the outliers, making it tailored too closely to the training data and risking poor performance on new data.</li>
          <li>The ideal model strikes a balance, capturing the underlying trend without being swayed by noise or outliers.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Diagnosing Using Errors:</strong>
    <ul>
      <li>Training Set Error: A low training error might indicate the model fits the training data well, but it doesn’t reveal if the model is overfitting.</li>
      <li>Development Set Error (Dev Error): This is crucial for understanding how well the model generalizes to unseen data. A significant increase in dev error compared to training error typically indicates overfitting.</li>
      <li>Scenarios:
        <ul>
          <li>High Training &amp; Dev Errors: The model is underfitting, indicating high bias.</li>
          <li>Low Training Error but High Dev Error: The model is overfitting, suggesting high variance.</li>
          <li>Both Errors are High &amp; Close: The model may have both high bias and variance.</li>
          <li>Both Errors are Low: Ideally, this indicates a well-performing model.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>The Role of Base Error:</strong>
    <ul>
      <li>This concept acknowledges that some tasks inherently have a limit to how well a model can perform. For example, if images are blurry, even humans might not achieve perfect accuracy. In such cases, what might appear as high bias in a model could just be approaching this base error.</li>
    </ul>
  </li>
  <li><strong>Regions of Bias and Variance:</strong>
    <ul>
      <li>Especially in models dealing with high-dimensional data, certain features or dimensions might be underfit, while others might be overfit. It’s not always a uniform issue across all aspects of the data.</li>
    </ul>
  </li>
  <li><strong>The Way Forward:</strong>
    <ul>
      <li>Recognizing and diagnosing whether a machine learning model suffers from bias, variance, or both is crucial. Depending on the diagnosis, different strategies and remedies can be applied to improve the model, ensuring it not only fits the training data well but also generalizes effectively to new, unseen data.</li>
    </ul>
  </li>
</ol>

<h3 id="examples">Examples</h3>
<ol>
  <li><strong>Bias:</strong>
    <ul>
      <li><strong>Situation:</strong> Imagine you’re trying to predict house prices based on their square footage.</li>
      <li><strong>High Bias Model:</strong> You assume that the relationship between square footage and price is strictly linear. Regardless of the data, your model always predicts house prices as a simple linear function of their size.</li>
      <li><strong>Reality:</strong> Houses in downtown areas might be more expensive per square foot than those in the suburbs, and luxury homes might have additional factors that increase their price. Your model doesn’t consider any of these nuances.</li>
      <li><strong>Result:</strong> Both on your training data and new data, the predictions are consistently off, underestimating prices for luxury homes and overestimating for some small downtown apartments.</li>
    </ul>
  </li>
  <li><strong>Variance:</strong>
    <ul>
      <li><strong>Situation:</strong> Again, predicting house prices based on multiple features including square footage, number of rooms, proximity to services, age of the house, etc.</li>
      <li><strong>High Variance Model:</strong> You use a complex model, like a deep neural network with many layers or a high-degree polynomial regression. It gives great results on your training data, capturing even tiny fluctuations.</li>
      <li><strong>Reality:</strong> Some of those fluctuations in your training data were just noise (maybe a seller was particularly desperate, or a buyer particularly uninformed).</li>
      <li><strong>Result:</strong> When you try to predict prices for new houses not in your training set, the model gives wild predictions. For instance, it might give extremely high price predictions for houses that just slightly resemble a luxury house in the training set or very low predictions for those resembling a cheaper sale, even if these resemblances are coincidental.</li>
    </ul>
  </li>
  <li><strong>Both High Bias and High Variance:</strong>
    <ul>
      <li><strong>Situation:</strong> Predicting weather based on various factors like temperature, humidity, wind speed, and so on.
     - Model with Both High Bias and High Variance:** You decide that temperature is the only factor that really matters, so you focus only on that (high bias). However, for temperatures around 70°F (21°C), you’ve noticed some very specific patterns in your training data, so you make your model extremely sensitive to tiny fluctuations in this range (high variance).</li>
      <li><strong>Reality:</strong> Other factors like humidity and wind speed play a significant role in predicting the weather. Also, the specific patterns you noticed around 70°F were mostly coincidences.</li>
      <li><strong>Result:</strong> The model consistently makes poor predictions (due to ignoring important factors) and occasionally makes bizarre predictions for temperatures around 70°F.</li>
    </ul>
  </li>
  <li><strong>Low Bias and Low Variance:</strong>
    <ul>
      <li><strong>Situation:</strong> Classifying emails as spam or not spam based on their content.</li>
      <li><strong>Balanced Model:</strong> You use a model that considers word frequencies, certain key phrases, sender reputation, and user feedback. The model fits well to the training data without being too rigid or too flexible.</li>
      <li><strong>Reality:</strong> Most spam emails have certain common patterns, but there’s always some variation and new techniques spammers employ.</li>
      <li><strong>Result:</strong> The model works well on both training data and new, unseen emails, catching most spam while rarely misclassified legitimate emails.</li>
    </ul>
  </li>
</ol>

<h2 id="diagnosing-and-treating-bias-and-variance-in-neural-networks-the-recipe-">Diagnosing and Treating Bias and Variance in Neural Networks: The Recipe-</h2>

<ol>
  <li><strong>Diagnosis:</strong>
    <ul>
      <li><strong>High Bias Check:</strong> One must first assess the model’s performance on the training set. If the model doesn’t achieve satisfactory results on this set, it’s an indication of high bias. It means the model hasn’t even captured the nuances of the data it was trained on.</li>
      <li><strong>High Variance Check:</strong> To determine if there’s a variance issue, one should evaluate how the model’s performance varies between the training set and the dev set. A significant drop in performance on the dev set (compared to the training set) indicates high variance, meaning the model may be overfitting to the training data.</li>
    </ul>
  </li>
  <li><strong>Addressing High Bias:</strong>
    <ul>
      <li>High bias indicates that the model is too simplistic and is underfitting the data.</li>
      <li><strong>Solutions:</strong>
        <ul>
          <li><strong>Enlarge the Neural Network:</strong> This can be achieved by adding more hidden layers or increasing the number of hidden units. A bigger network has the potential to capture more complex patterns in the data.</li>
          <li><strong>Prolonged Training:</strong> Training the model for a longer time can sometimes allow it to better fit the training set.</li>
          <li><strong>Advanced Optimization Algorithms:</strong> Later in the course, they would discuss more advanced optimization techniques. These methods can speed up convergence and help the model fit better.</li>
          <li><strong>Experiment with Network Architecture:</strong> Different problems can benefit from various neural network structures. It’s often worth experimenting to find the best fit, even though it might require some trial and error.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Addressing High Variance:</strong>
    <ul>
      <li>High variance indicates the model might be too complex, capturing noise rather than the underlying pattern, hence overfitting.</li>
      <li><strong>Solutions:</strong>
        <ul>
          <li><strong>More Data:</strong> One of the most effective ways to combat high variance is by increasing the training data. This gives the model more examples to generalize from.</li>
          <li><strong>Regularization:</strong> This technique can help penalize overly complex models, thereby reducing overfitting.</li>
          <li><strong>Network Architecture Tweaking:</strong> Similar to the bias problem, sometimes tweaking the network architecture can also help in reducing variance.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Bias-Variance Tradeoff: An Evolving Perspective:**
    <ul>
      <li>Historically, there was a prevalent belief in machine learning that reducing bias would invariably increase variance and vice versa. This was the essence of the bias-variance tradeoff.</li>
      <li>However, with the advent of deep learning, this dynamic has shifted. Now, with appropriately regularized large networks, it’s possible to reduce bias without a substantial increase in variance and vice versa. The ability to train bigger networks and procure more data has granted practitioners tools to address these problems more independently.</li>
    </ul>
  </li>
  <li><strong>Regularization’s Role:</strong>
    <ul>
      <li>Regularization, a cornerstone technique in machine learning, can be a pivotal tool in the fight against variance. While it primarily helps in curbing overfitting, it’s essential to strike a balance. Over-regularization can lead the model back into the realm of high bias.</li>
    </ul>
  </li>
</ol>

<h2 id="weights-and-bias-vs-bias-and-variance">Weights and Bias vs Bias and Variance</h2>
<ol>
  <li><strong>Weights and Biases</strong>:
    <ul>
      <li><strong>Weights</strong>: These are the values in a neural network that get adjusted through training. When input data is fed into a neural network, it gets multiplied by these weights, which are then adjusted through backpropagation based on the error of the model’s predictions.</li>
      <li><strong>Biases</strong>: These are also adjustable values in a neural network, but they are added to the weighted input to shift the output of a neuron. Essentially, they allow the activation function to shift left or right.</li>
    </ul>
  </li>
  <li><strong>Bias and Variance (Bias-Variance Tradeoff)</strong>:
    <ul>
      <li><strong>Bias</strong>: This refers to the error introduced by approximating a real-world problem (which might be complex) by a too-simplistic model. High bias means that the model makes strong assumptions about the data and can’t capture the underlying patterns, leading to systematic errors regardless of the specific dataset. In other words, it underfits the data.</li>
      <li><strong>Variance</strong>: This refers to the error introduced by using a model that’s too complex. High variance means the model is sensitive to fluctuations in the training data and captures the noise along with the signal, leading to errors when faced with new, unseen data. It essentially overfits the training data.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>When designing machine learning models, there’s often a tradeoff:</li>
  <li><strong>Low Bias, High Variance</strong>: The model fits the training data very well but doesn’t generalize well to new data.</li>
  <li>
    <p><strong>High Bias, Low Variance</strong>: The model doesn’t fit the training data well and also doesn’t generalize well, but its predictions are consistent across different datasets.</p>
  </li>
  <li>
    <p>A good model aims to balance these two types of errors to achieve low bias and low variance, leading to accurate predictions that generalize well to new, unseen data.</p>
  </li>
  <li>In summary, “weights” and “biases” are components of the model that are adjusted during training, while “bias” and “variance” in the context of the bias-variance tradeoff refer to types of errors related to how the model generalizes from training data to unseen data.</li>
</ul>

<h2 id="regularization">Regularization</h2>

<ol>
  <li><strong>Introduction to Regularization:</strong>
    <ul>
      <li>Regularization is a technique often used to prevent overfitting in machine learning models.</li>
      <li>While obtaining more training data can help with high variance, it might not always be feasible. Regularization is a reliable alternative.</li>
    </ul>
  </li>
  <li><strong>L2 Regularization in Logistic Regression:</strong>
    <ul>
      <li>In logistic regression, the cost function \(J\) is minimized to make accurate predictions. This cost function is based on the difference between the model’s predictions and the actual outcomes for training data.</li>
      <li>L2 regularization introduces an additional term to this cost function: \(\frac{\lambda}{2m} \times ||w||^2\). Here:
\(\lambda\) is the regularization parameter.
\(||w||^2\) represents the square of the Euclidean norm (or L2 norm) of the weight vector \(w\).</li>
      <li>Regularization mostly affects \(w\) and not \(b\) because \(w\) typically has many more parameters.</li>
    </ul>
  </li>
  <li><strong>L1 vs L2 Regularization:</strong>
    <ul>
      <li>L1 regularization adds the absolute values of the weight parameters, leading to sparse weight vectors (many zero values). This can help compress the model, but its primary use is not for compression.</li>
      <li>L2 regularization is more commonly used in practice. L2 regularization sometimes goes by another name: “weight decay”.</li>
    </ul>
  </li>
  <li><strong>Regularization in Neural Networks:</strong>
    <ul>
      <li>For neural networks, the cost function encompasses parameters across all layers. The regularization term sums up the “squared norms” of all weight matrices.</li>
      <li>The squared norm for a matrix, which sums up the squares of all its entries, is called the Frobenius norm.</li>
      <li>When implementing gradient descent with regularization, the update rule for weights slightly changes: after computing the standard gradient from backpropagation, you subtract a fraction of the weights themselves. This results in the weights shrinking in each step, hence the term “weight decay”.</li>
    </ul>
  </li>
  <li>
    <p><strong>Role of \(\lambda\):</strong>
 \(\lambda\), the regularization parameter, balances the trade-off between fitting the training data well and keeping the weights small to prevent overfitting. The best value for \(\lambda\) is usually determined through cross-validation.</p>
  </li>
  <li><strong>Understanding Weight Decay:</strong>
    <ul>
      <li>Weight decay effectively multiplies the weights by a value slightly less than one during each gradient descent iteration. This ensures that weights don’t grow too large, which would make the model more likely to overfit to training data.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Regularization, especially L2 regularization, serves as a powerful tool in the machine learning practitioner’s arsenal, offering a solution to high variance problems when acquiring additional training data is not feasible.</li>
</ul>

<ol>
  <li><strong>Intuition Behind Regularization:</strong>
    <ul>
      <li>Overfitting: When a model captures noise instead of the underlying trend in data, it tends to perform poorly on new, unseen data. This typically occurs when the model has too many parameters relative to the number of training examples, causing it to fit the training data too closely.</li>
      <li>Regularization Impact: Regularization discourages overly complex models by penalizing large coefficients. This can help prevent the model from fitting the training data too closely, which reduces the risk of overfitting.</li>
    </ul>
  </li>
  <li><strong>How Does Regularization Work?:</strong>
    <ul>
      <li>By adding a penalty term based on the magnitude of the coefficients (typically the L2 or Frobenius norm of the weights) to the loss function, the model is discouraged from assigning too much importance to any individual feature.</li>
      <li>If the regularization term (lambda) is set very large, it can push the weights towards very small values. This makes the neural network simpler, potentially resembling a logistic regression model, which is less prone to overfitting.</li>
    </ul>
  </li>
  <li><strong>Activation Function Intuition:</strong>
    <ul>
      <li>With the tanh activation function as an example, when the weights are small (due to regularization), the input values (Z) for the activation function tend to be small as well. This means the function operates in its linear region.</li>
      <li>If a neural network operates mainly in this linear region across all layers, it effectively becomes a linear model, which inherently is less likely to overfit compared to a complex, non-linear model.</li>
    </ul>
  </li>
  <li><strong>Practical Consideration:</strong>
    <ul>
      <li>When implementing regularization in gradient descent, it’s essential to ensure that the newly defined cost function, including the regularization term, is used for monitoring and debugging. This is because, with the added term, the cost function should ideally decrease monotonically with each iteration of gradient descent.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>The main takeaway is that regularization helps in adding constraints to the model, thereby preventing it from becoming overly complex and fitting to the noise in the data. The added penalty for large weights pushes the model towards simplicity, which can significantly reduce the risk of overfitting, especially in deep neural networks with many parameters.</li>
</ul>

<h2 id="intuition-behind-dropout">Intuition Behind Dropout</h2>

<p><img src="../assets/practicalDL/2.png" alt="" /></p>

<ol>
  <li>
    <p><strong>Random Selection of Nodes:</strong> As you’ve mentioned, dropout essentially involves going through every node in each layer during training and temporarily “dropping it out” or deactivating it with a certain probability. This means that during training, the network becomes somewhat different on every pass. As a result, each node becomes less reliant or dependent on specific features from the previous layer because they might be deactivated. In other words, it’s as if each node has to be useful on its own and can’t rely too much on its neighboring nodes.</p>
  </li>
  <li>
    <p><strong>Ensemble Learning Analogy:</strong> One way to think about dropout is akin to ensemble learning. When you use dropout, each iteration trains a different neural network with a different subset of nodes. Then, when you’re not using dropout during testing, it’s like averaging the output of all these networks. We know that ensemble methods, where you train multiple models and average their outputs, can be more robust than any single model. Dropout can be thought of as an inexpensive ensemble method, where you train a vast ensemble of networks all sharing the same weights.</p>
  </li>
  <li>
    <p><strong>Reduction of Overfitting:</strong> By randomly dropping out nodes, you’re essentially thinning your network, making it less complex. This can help prevent the network from fitting noise in the training data (overfitting) since it’s always working with a reduced capacity. This makes the model more general and robust, leading to better performance on unseen data.</p>
  </li>
  <li>
    <p><strong>Cost-Efficiency:</strong> Traditional ensemble methods require training multiple models which can be computationally expensive. However, with dropout, while you’re essentially training a vast ensemble, the computational cost is not much higher than training one network.</p>
  </li>
</ol>

<h3 id="potential-drawbacks-and-considerations">Potential Drawbacks and Considerations</h3>

<ol>
  <li>
    <p><strong>Increased Training Time:</strong> Even though each iteration might be faster because of the thinned network, dropout often requires more epochs for the network to converge because on each pass, only a subset of the network is getting trained.</p>
  </li>
  <li>
    <p><strong>Not Always Beneficial:</strong> Dropout can be very effective for large networks and big datasets where overfitting is a concern. However, for smaller networks or smaller datasets, dropout might not always provide a benefit and can sometimes even harm performance.</p>
  </li>
  <li>
    <p><strong>Tuning Dropout Rate:</strong> The probability of dropping a node (keep probability) is a hyperparameter that might need tuning. Common values are between 0.5 and 1 (e.g., 0.5, 0.8), but the optimal value can vary depending on the dataset and network architecture.</p>
  </li>
</ol>

<ul>
  <li>Dropout provides a unique and effective approach to regularization in neural networks. It’s particularly useful for complex networks trained on large datasets, where overfitting is a primary concern. Like all techniques, understanding when and how to use it effectively is key.</li>
</ul>

<h2 id="inverted-dropout">Inverted dropout</h2>

<ul>
  <li>Inverted dropout is the most commonly used version of dropout, primarily because it makes the test time behavior of the network simpler. Instead of scaling activations at test time, the scaling is done at training time.</li>
</ul>

<h3 id="implementation-of-inverted-dropout">Implementation of Inverted Dropout</h3>

<h4 id="forward-propagation">Forward Propagation</h4>

<p>For each layer <code class="language-plaintext highlighter-rouge">l</code> where you wish to apply dropout:</p>

<ol>
  <li>Create a dropout matrix:
    <ul>
      <li>Create a matrix <code class="language-plaintext highlighter-rouge">D[l]</code> of the same dimension as <code class="language-plaintext highlighter-rouge">A[l]</code> (the activations). This is done by random sampling each entry of <code class="language-plaintext highlighter-rouge">D[l]</code> to be 1 with probability <code class="language-plaintext highlighter-rouge">keep_prob</code> or 0 with probability <code class="language-plaintext highlighter-rouge">1-keep_prob</code>.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">D</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">keep_prob</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Drop out units:
    <ul>
      <li>Zero out the activations for which <code class="language-plaintext highlighter-rouge">D[l]</code> is zero.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">D</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Scale the activations:
    <ul>
      <li>This step ensures that the expected value of the activations remains the same, avoiding the need to do any scaling at test time.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">/</span> <span class="n">keep_prob</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<h4 id="backward-propagation">Backward Propagation</h4>

<p>For layers where dropout was applied:</p>

<ol>
  <li>Apply the dropout mask to the gradients:
    <ul>
      <li>This ensures that the units that were dropped out during forward propagation don’t contribute to the gradients.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dA</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">D</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Scale the gradients:
    <ul>
      <li>This step scales down the gradients by the same factor as during forward propagation.
        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dA</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">dA</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">/</span> <span class="n">keep_prob</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

<h4 id="test-time">Test Time</h4>

<ul>
  <li>There’s no need to apply dropout or scale the activations. The network is used as-is.</li>
</ul>

<h4 id="notes">Notes</h4>

<ol>
  <li>
    <p>The dropout rate (i.e., <code class="language-plaintext highlighter-rouge">1 - keep_prob</code>) is typically set between 0.2 and 0.5. However, you should experiment with different values for your specific problem and network architecture.</p>
  </li>
  <li>
    <p>Dropout is more commonly used in fully connected layers rather than convolutional layers.</p>
  </li>
  <li>
    <p>Remember to turn off dropout during validation/testing. The idea is that dropout is a regularization technique applied only during training.</p>
  </li>
  <li>
    <p>The “inverted” part of inverted dropout refers to the scaling done during training rather than during testing. Regular dropout without this inversion scales the activations at test time, which is computationally inefficient.</p>
  </li>
</ol>

<ul>
  <li>In frameworks like TensorFlow and PyTorch, dropout (including inverted dropout) is often available as a built-in layer or function, simplifying its application.</li>
  <li>Dropout is a regularization technique where units in a neural network are randomly deactivated during training. Here’s a breakdown of the primary insights and considerations:</li>
</ul>

<ol>
  <li><strong>Intuition Behind Dropout:</strong>
    <ul>
      <li>Network Perspective: By randomly deactivating units, the network behaves as a smaller version of itself during each iteration, adding a regularizing effect.</li>
      <li>Unit Perspective: A specific unit can’t overly rely on any single input because that input could be deactivated at any given time. This forces the unit to distribute its weights among all its inputs, which tends to reduce the magnitude of weights and, as a result, acts as a form of regularization similar to L2.</li>
    </ul>
  </li>
  <li><strong>Dropout as Adaptive L2 Regularization:</strong>
    <ul>
      <li>Dropout can be shown to resemble L2 regularization but in a more adaptive form. The penalty on different weights can differ based on the activations.</li>
    </ul>
  </li>
  <li><strong>Layer-specific Dropout:</strong>
    <ul>
      <li>Different layers can have different dropout rates (<code class="language-plaintext highlighter-rouge">keep_prob</code>). For example, layers with many parameters that are more prone to overfitting can have a higher dropout rate.</li>
      <li>You can also apply dropout to the input layer, though this isn’t as common.</li>
    </ul>
  </li>
  <li><strong>Implementation Tips:</strong>
    <ul>
      <li>Dropout has been particularly beneficial in computer vision because of the vast input size relative to the available training data. However, its utility might differ in other domains.</li>
      <li>One of the main challenges with dropout is that it makes the cost function J not well-defined, making it harder to verify if gradient descent is working correctly. It’s common practice to turn off dropout (set <code class="language-plaintext highlighter-rouge">keep_prob</code> to 1) to ensure that J decreases monotonically, then reintroduce dropout.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>The takeaway is that while dropout offers a powerful regularization technique, especially useful in scenarios like computer vision where overfitting is common, it needs to be used judiciously. Proper tuning and understanding its impact on the model’s performance are essential.</li>
</ul>

<h2 id="other-techniques-for-reducing-overfitting-in-neural-networks">Other Techniques for Reducing Overfitting in Neural Networks</h2>

<ol>
  <li><strong>Data Augmentation:</strong>
    <ul>
      <li>Can be useful when acquiring more data is difficult or expensive.</li>
      <li>Examples:
        <ul>
          <li>Flipping images horizontally to double the training set size.</li>
          <li>Random crops and distortions of the image.</li>
          <li>Not as effective as acquiring new, independent examples, but it’s cost-efficient.</li>
          <li>The core idea is that certain transformations of data still represent the same class. E.g., a flipped image of a cat is still an image of a cat.</li>
          <li>For optical character recognition, applying random rotations and distortions to digits is an example of data augmentation.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Early Stopping:</strong>
    <ul>
      <li>Monitor the training error and development (dev) set error.</li>
      <li>Training error typically decreases monotonically.</li>
      <li>Dev set error decreases initially and may increase when the model starts overfitting.</li>
      <li>Stop the training when the dev set error starts increasing.</li>
      <li>Works because early in training, parameters (w) will be close to zero (small), resulting in a simpler model. As training continues, parameters may grow, leading to a potentially more complex model that might overfit.</li>
      <li>The term “early stopping” refers to halting the neural network training earlier than planned.</li>
    </ul>
  </li>
</ol>

<ul>
  <li><strong>Issues with Early Stopping:</strong>
    <ul>
      <li><strong>Orthogonalization:</strong> It’s beneficial to have separate tools to (a) minimize cost function and (b) prevent overfitting. Early stopping combines these tasks, complicating the optimization process.</li>
      <li>An alternative is using L2 regularization and training the neural network extensively.</li>
      <li>A potential downside to L2 regularization is the need to try many values of the regularization parameter (lambda), making it computationally expensive.</li>
      <li>Early stopping’s advantage is it allows for trying small, mid-size, and large parameter values without varying lambda.</li>
    </ul>
  </li>
</ul>

<h3 id="normalizing-input-features">Normalizing Input Features</h3>

<ol>
  <li><strong>Why Normalize:</strong>
    <ul>
      <li>Normalization of inputs can speed up the convergence of training algorithms like gradient descent, resulting in quicker training times.</li>
      <li>When features have different scales (e.g., one feature ranges from 1-1000 while another from 0-1), the cost function will be stretched in certain directions, leading to a more elongated and inefficient path for the optimization algorithm.</li>
    </ul>
  </li>
  <li>
    <p><strong>Steps to Normalize:</strong></p>

    <p>a. <strong>Zero-Centering:</strong>
     - Compute the mean \(\mu\) of the training set, where:
         \(\mu = \frac{1}{M} \sum_{i} x_i\)
     - Subtract the mean \(\mu\) from every training example. This translates the dataset such that the mean is centered at zero:
         \(x = x - \mu\)</p>

    <p>b. <strong>Normalize Variances:</strong>
     - After zero-centering, compute the variance. For each feature, you square its value, and then take the average across all examples:
         \(\sigma^2 = \frac{1}{M} \sum x_i^2\)
     - Here, \(x_i^2\) is element-wise squaring.
     - Normalize the data by dividing each feature by its standard deviation (which is the square root of variance). Essentially, this scales the data to have a unit variance:
         \(x = \frac{x}{\sigma^2}\)</p>
  </li>
  <li><strong>Effects on Cost Function:</strong>
    <ul>
      <li>Without normalization, the cost function may resemble an elongated bowl, making optimization algorithms take longer zig-zag paths to find the minimum.</li>
      <li>Post-normalization, the cost function is more symmetric, resembling a more circular bowl, allowing optimization algorithms to find the minimum more directly and quickly.</li>
    </ul>
  </li>
  <li><strong>Using Same Parameters for Test Set:</strong>
    <ul>
      <li>It’s crucial to use the same normalization parameters (mean and variance) derived from the training set to normalize the test set.</li>
      <li>Do not compute these parameters separately for the test set; always use the ones from the training set to ensure consistent data transformation.</li>
    </ul>
  </li>
  <li><strong>Rationale:</strong>
    <ul>
      <li>When features are on different scales, the parameters associated with each feature will need to cover very different ranges, causing the elongated shape of the cost function. By normalizing, all features and their associated parameters are on similar scales, making the cost function more symmetric.</li>
      <li>Normalization makes your algorithm less sensitive to the scale of features, making training more robust.</li>
    </ul>
  </li>
  <li><strong>When to Normalize:</strong>
    <ul>
      <li>If features have very different scales or ranges, normalization is crucial.</li>
      <li>If they’re already on similar scales, normalization might be less critical but rarely does any harm. Often, it’s still beneficial for ensuring consistent training behavior.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Normalizing input features is a preprocessing step that can considerably speed up the training process by reshaping the cost function, making it easier and faster for optimization algorithms to converge.</li>
</ul>

<h2 id="vanishing--exploding-gradients">Vanishing / Exploding gradients</h2>
<ul>
  <li>
    <p>In training deep neural networks, a prominent challenge faced is the vanishing and exploding gradient problem. This issue arises when the gradients (or derivatives) become extremely small (vanishing) or extremely large (exploding) as the network gets deeper. These extremes can severely hinder the training process, making it difficult or even impossible for the network to converge to a solution.</p>
  </li>
  <li><strong>The Problem:</strong>
    <ul>
      <li>For very deep networks, the activation outputs can grow (explode) or reduce (vanish) rapidly, growing or shrinking exponentially with the depth of the network.</li>
      <li>Using a linear activation function as an example, and ignoring biases, the output \(Y\) can be expressed as a product of all the weight matrices from \(W_1\) to \(W_L\) times the input \(X\).</li>
      <li>If each weight matrix is slightly larger than the identity (e.g., scaled by 1.5), the activations grow exponentially with the depth \(L\). This results in the exploding gradient problem.</li>
      <li>Conversely, if each weight matrix is slightly smaller than the identity (e.g., scaled by 0.5), the activations shrink exponentially, leading to the vanishing gradient problem.</li>
      <li>Similar exponential growth or reduction can also be observed in the gradients (derivatives) when backpropagating through the network, making the learning process ineffective.</li>
    </ul>
  </li>
  <li><strong>Significance:</strong>
    <ul>
      <li>Deep networks, such as those with 150 or more layers, have been employed with success in recent years. However, if gradients explode or vanish exponentially based on the network’s depth, they can either become too big (causing numerical instability) or too small (making learning extremely slow).</li>
      <li>For a long time, this challenge presented a significant barrier in training very deep networks.</li>
    </ul>
  </li>
  <li><strong>Partial Solution:</strong>
    <ul>
      <li>A potential solution to mitigate these problems is careful weight initialization. Choosing the right strategy to initialize weights can significantly alleviate the vanishing and exploding gradient issues.</li>
    </ul>
  </li>
  <li>
    <p>Deep neural networks can be afflicted by vanishing or exploding gradients, making training challenging. A partial remedy lies in a judicious choice of random weight initialization.</p>
  </li>
  <li><strong>Weight Initialization for a Single Neuron:</strong>
    <ul>
      <li>Taking a neuron with \(n\) inputs (features) as an example:
        <ul>
          <li>The weighted sum \(z\) is the summation of the product of each input \(x_i\) and its corresponding weight \(w_i\).</li>
          <li>To ensure \(z\) doesn’t grow extremely large or small, the more inputs (or the larger \(n\) is), the smaller each weight \(w_i\) should be. This ensures that the sum remains in a reasonable range.</li>
          <li>A suggested approach is to set the variance of each \(w_i\) as \(\frac{1}{n}\).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Weight Initialization for Deep Networks:</strong>
    <ul>
      <li>The weight matrix \(W\) for a layer can be initialized using the formula: <code class="language-plaintext highlighter-rouge">np.random.randn(shape) * sqrt(2/n_(l-1))</code>, where \(n_(l-1)\) is the number of inputs to the neuron in the current layer.</li>
      <li>For a ReLu activation function, it’s been found that using a variance of \(\frac{2}{n}\) is more effective.</li>
      <li>The reasoning for this initialization approach is to ensure the weighted sum \(z\) remains at a scale that neither vanishes nor explodes too quickly, thus helping mitigate the gradient problem.</li>
    </ul>
  </li>
  <li><strong>Variations in Initialization:</strong>
    <ul>
      <li>For a TanH activation function: Initialization called “Xavier initialization” suggests using \(\frac{1}{n}\) instead of \(\frac{2}{n}\).</li>
      <li>There are other methods suggested in research, but for ReLu (most common activation function), the above method is recommended. For TanH, Xavier initialization is suggested.</li>
      <li>The variance in the initialization can be treated as a hyperparameter. Adjusting this hyperparameter can sometimes improve performance, but it’s not typically the first choice in tuning.</li>
    </ul>
  </li>
  <li>Proper weight initialization can help in keeping gradients in check for deep networks, making training faster and more effective.</li>
</ul>

<h2 id="gradient-checking-in-neural-networks---overview-and-practical-tips">Gradient Checking in Neural Networks - Overview and Practical Tips</h2>

<ul>
  <li>**Gradient Checking Overview: **
    <ul>
      <li>Gradient checking is a technique used to verify if the gradients (derivatives) calculated by the backpropagation algorithm are correct. The idea is to approximate the gradient of each parameter by tweaking it slightly and observing the change in the cost function. The computed gradient and this approximation should be close if backpropagation is implemented correctly.</li>
    </ul>
  </li>
  <li>
    <p><strong>Practical Tips:</strong></p>

    <ol>
      <li>
        <p>Not for Training: Don’t use gradient checking during regular training because of its computational intensity. It’s designed mainly for debugging. Once you’ve ensured your backpropagation gradients are close to the approximated ones, deactivate gradient checking.</p>
      </li>
      <li>
        <p>Debugging with Component Analysis: If gradient checking fails (i.e., if the approximated and computed gradients differ significantly), analyze each component. Examining specific components, like \(dW\) or \(db\) for various layers, can hint at where a potential error lies.</p>
      </li>
      <li>
        <p>Remember Regularization: If using regularization in your cost function, make sure the gradient checking process incorporates the regularization term. The gradients should be computed for the entire cost function, including this term.</p>
      </li>
      <li>
        <p>Dropout Challenges: Gradient checking is inconsistent with dropout. Since dropout randomly turns off certain nodes during iterations, a consistent cost function is challenging to compute. Generally, turn off dropout when performing gradient checking and then turn it back on for regular training.</p>
      </li>
      <li>
        <p>Potential Limitations with Large Weights and Biases: On rare occasions, backpropagation might be accurate only when weights (W) and biases (b) are near their initial values. To counter this, you might run gradient checking after several training iterations, allowing weights and biases to adjust from their initial values.</p>
      </li>
    </ol>
  </li>
  <li>In summary, gradient checking is an invaluable tool for verifying the accuracy of the backpropagation algorithm. However, due to its computational demand, use it judiciously and typically without dropout. Remember to include any regularization terms when performing checks.</li>
</ul>

<h2 id="mini-batch-gradient-descent-explained">Mini-batch Gradient Descent Explained</h2>

<ul>
  <li>When training neural networks, especially with large datasets, the process can be slow using traditional methods like batch gradient descent which processes the entire training set before making an update. An alternative and faster method is mini-batch gradient descent. This technique divides the dataset into smaller “mini-batches” and performs an update for each mini-batch.</li>
  <li>For instance, if you have a dataset of 5 million examples, instead of processing all of them at once, you might break them into mini-batches of 1,000 examples each. This means that for every pass through the dataset (an “epoch”), you would make 5,000 updates with mini-batch gradient descent instead of just one with the traditional method.</li>
  <li>To implement mini-batch gradient descent:
    <ol>
      <li>Split the data into mini-batches, e.g., X1 through X1000, X1001 through X2000, and so on.</li>
      <li>For each mini-batch, perform forward propagation using only the data in that mini-batch.</li>
      <li>Compute the cost function for that mini-batch.</li>
      <li>Implement backpropagation to compute gradients.</li>
      <li>Update the weights and biases using the gradients.
        <ul>
          <li>Enables progress in gradient descent even when only partially through the training set.</li>
          <li>With batch gradient descent, cost should decrease every iteration.</li>
          <li>For mini-batch gradient descent, cost might not decrease every iteration due to different training batches.</li>
          <li>Cost function J should generally trend downwards but may oscillate due to varying difficulty of mini-batches.</li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h3 id="batch-gradient-descent-vs-stochastic-gradient-descent">Batch Gradient Descent vs. Stochastic Gradient Descent</h3>
<ul>
  <li>When mini-batch size \(m\) equals training set size, it’s batch gradient descent.</li>
  <li>When mini-batch size is 1, it’s called stochastic gradient descent.
    <ul>
      <li>Each example is its own mini-batch.</li>
      <li>Takes gradient descent steps with one example at a time.</li>
    </ul>
  </li>
</ul>

<h3 id="behavior-of-gradient-descent-algorithms">Behavior of Gradient Descent Algorithms</h3>
<ul>
  <li>Batch gradient descent: Takes consistent steps toward minimum.</li>
  <li>Stochastic gradient descent: Noisy, oscillates around the minimum but won’t converge to it.</li>
  <li>Mini-batch gradient descent: Somewhere in-between; tends to head toward the minimum but might oscillate slightly.</li>
</ul>

<h3 id="choosing-mini-batch-size">Choosing Mini-batch Size</h3>
<ul>
  <li>For small training sets (e.g., &lt;2000), use batch gradient descent.</li>
  <li>For larger sets, sizes between 64 to 512 (preferably powers of 2) are typical.
    <ul>
      <li>Ensures compatibility with computer memory layout.</li>
    </ul>
  </li>
  <li>Important: Ensure each mini-batch fits in CPU/GPU memory.</li>
</ul>

<h3 id="efficiency--speed">Efficiency &amp; Speed</h3>
<ul>
  <li>Mini-batch gradient descent can be faster due to vectorization across examples.</li>
  <li>Progress is made without processing the entire training set.</li>
  <li>Adjusting learning rate or “learning rate decay” can refine the algorithm, but that’s covered later.</li>
  <li>Mini-batch size is a hyperparameter; trying different values can optimize performance.</li>
</ul>

<h2 id="gradient-descent-with-momentum">Gradient descent with momentum</h2>

<ul>
  <li>Momentum, or gradient descent with momentum, typically works faster than standard gradient descent.</li>
  <li>It computes an exponentially weighted average of the gradients, and then uses that gradient to update the weights.</li>
  <li><strong>Problem with Standard Gradient Descent:</strong>
    <ul>
      <li>For certain cost functions, gradient descent might oscillate a lot before reaching the minimum. This oscillation slows down the process and prevents using a larger learning rate.</li>
    </ul>
  </li>
  <li><strong>Gradient Descent with Momentum:</strong>
    <ul>
      <li>During iteration, you compute the usual derivatives (<code class="language-plaintext highlighter-rouge">dw</code>, <code class="language-plaintext highlighter-rouge">db</code>) based on the mini-batch.</li>
      <li>Then you compute a moving average of these derivatives:
        <ul>
          <li>
\[vdW = \beta * vdW + (1-\beta) \times dW\]
          </li>
          <li>
\[vdb = \beta * vdb + (1-\beta) \times db\]
          </li>
        </ul>
      </li>
      <li>Instead of updating the weights using <code class="language-plaintext highlighter-rouge">dW</code>, you update them with <code class="language-plaintext highlighter-rouge">vdW</code>.</li>
    </ul>
  </li>
  <li><strong>Advantages of Momentum:</strong>
    <ul>
      <li>Momentum averages out the oscillations.
        <ul>
          <li>In a vertical direction, oscillations are reduced because positive and negative numbers average out.</li>
          <li>In a horizontal direction, the movement remains aggressive, which speeds up the descent.</li>
        </ul>
      </li>
      <li>An analogy: Think of a ball rolling down a hill. Derivatives provide acceleration while momentum terms represent velocity. The ball accelerates down the hill, gaining momentum.</li>
    </ul>
  </li>
  <li><strong>Implementation Details:</strong>
    <ul>
      <li>Two hyperparameters are important: the learning rate (<code class="language-plaintext highlighter-rouge">α</code>) and <code class="language-plaintext highlighter-rouge">β</code>, which controls the exponentially weighted average.</li>
      <li><code class="language-plaintext highlighter-rouge">β = 0.9</code> is a common and robust choice, effectively averaging the gradients over the last ten iterations.</li>
      <li>Bias correction (dividing by <code class="language-plaintext highlighter-rouge">1-β^t</code>) isn’t usually done after about ten iterations.</li>
      <li>Initial values: <code class="language-plaintext highlighter-rouge">vdW</code> is initialized as a matrix of zeroes with the same dimension as <code class="language-plaintext highlighter-rouge">dW</code>, and <code class="language-plaintext highlighter-rouge">vdb</code> as a vector of zeroes, same dimension as <code class="language-plaintext highlighter-rouge">db</code>.</li>
    </ul>
  </li>
  <li><strong>Different Formulations:</strong>
    <ul>
      <li>Some literature omits the <code class="language-plaintext highlighter-rouge">(1-β)</code> term. The impact of this is that <code class="language-plaintext highlighter-rouge">vdW</code> and <code class="language-plaintext highlighter-rouge">vdb</code> are scaled differently, affecting the optimal learning rate.</li>
      <li>The prefered version of the formulation is with the following <code class="language-plaintext highlighter-rouge">(1-β)</code> term.</li>
    </ul>
  </li>
  <li>**Conclusion: **
    <ul>
      <li>Gradient descent with momentum typically performs better than standard gradient descent.</li>
      <li>There are still other optimization methods to explore.</li>
    </ul>
  </li>
</ul>

<h2 id="rmsprop-root-mean-square-prop">RMSprop (Root Mean Square prop)</h2>
<ul>
  <li>RMSprop is another algorithm that can enhance gradient descent. If gradient descent is visualized, large oscillations can occur in the vertical direction while progress is made in the horizontal direction. For intuitive understanding, let’s label the vertical axis as <code class="language-plaintext highlighter-rouge">b</code> and the horizontal axis as <code class="language-plaintext highlighter-rouge">w</code>.</li>
  <li><strong>RMSprop Mechanism:</strong>
    <ol>
      <li>On iteration <code class="language-plaintext highlighter-rouge">t</code>, compute the derivative <code class="language-plaintext highlighter-rouge">dW</code> and <code class="language-plaintext highlighter-rouge">db</code> for the current mini-batch.</li>
      <li>Maintain an exponentially weighted average of the squared derivatives:
        <ul>
          <li>
\[S_{dW} = \beta S_{dW} + (1-\beta) dW^2\]
          </li>
          <li>
\[S_{db} = \beta S_{db} + (1-\beta) db^2\]
          </li>
          <li><em>Note:</em> Squaring is element-wise.</li>
        </ul>
      </li>
      <li>Update the parameters:
        <ul>
          <li>
\[W = W - \alpha \frac{dW}{\sqrt{S_{dW} + \epsilon}}\]
          </li>
          <li>
\[b = b - \alpha \frac{db}{\sqrt{S_{db} + \epsilon}}\]
          </li>
        </ul>
      </li>
    </ol>
  </li>
  <li><strong>Intuition Behind RMSprop:</strong>
    <ul>
      <li>We aim to fast-track learning in the <code class="language-plaintext highlighter-rouge">w</code> (horizontal) direction and dampen oscillations in the <code class="language-plaintext highlighter-rouge">b</code> (vertical) direction.</li>
      <li>With <code class="language-plaintext highlighter-rouge">S_{dW}</code> and <code class="language-plaintext highlighter-rouge">S_{db}</code>, if derivatives are larger vertically (<code class="language-plaintext highlighter-rouge">b</code> direction), <code class="language-plaintext highlighter-rouge">S_{db}</code> will be large, leading to smaller updates in the <code class="language-plaintext highlighter-rouge">b</code> direction. Conversely, updates in the <code class="language-plaintext highlighter-rouge">w</code> direction will be faster since <code class="language-plaintext highlighter-rouge">S_{dW}</code> would be smaller.</li>
      <li>RMSprop will cause updates that dampen oscillations in directions with high oscillations.</li>
    </ul>
  </li>
  <li>**Clarification: **
    <ul>
      <li>The vertical and horizontal directions (<code class="language-plaintext highlighter-rouge">b</code> and <code class="language-plaintext highlighter-rouge">w</code>) are used for illustration. In real scenarios, there are high-dimensional spaces of parameters. The essence is that, in dimensions with oscillations, a more considerable weighted average of squared derivatives is computed, damping those oscillations.</li>
    </ul>
  </li>
  <li><strong>RMSprop Details:</strong>
    <ul>
      <li>The algorithm is termed RMSprop because it squares the derivatives and then takes their square root.</li>
      <li>To differentiate from the momentum’s hyperparameter, RMSprop’s hyperparameter is labeled <code class="language-plaintext highlighter-rouge">beta2</code>.</li>
      <li>For numerical stability, a small epsilon is added to the denominator to avoid division by zero. A default could be \(10^{-8}\).</li>
    </ul>
  </li>
  <li><strong>Impact of RMSprop:</strong>
    <ul>
      <li>It dampens oscillations in gradient descent, especially in mini-batch gradient descent.</li>
      <li>It allows for potentially larger learning rates, <code class="language-plaintext highlighter-rouge">α</code>, and accelerates the learning process.</li>
      <li>Trivia:</li>
      <li>RMSprop was first introduced in a Coursera course by Jeff Hinton, not in a research paper. It gained popularity from this course.</li>
    </ul>
  </li>
</ul>

<h3 id="adam-optimization-algorithm">Adam Optimization Algorithm</h3>

<ul>
  <li>Adam (Adaptive Moment Estimation) is a fusion of the ideas behind Momentum and RMSprop, two optimization techniques. It is widely used due to its robustness and adaptability to various deep learning architectures.</li>
</ul>

<h4 id="concepts">Concepts</h4>

<ol>
  <li>Momentum
    <ul>
      <li>Aims to speed up the optimization by considering the past gradient.
\(V_{dw} = \beta_1 V_{dw} + (1-\beta_1) \times dw\)
\(V_{db} = \beta_1 V_{db} + (1-\beta_1) \times db\)</li>
    </ul>
  </li>
  <li>RMSprop
    <ul>
      <li>Adjusts the learning rate by using the moving average of the squared gradient.
\(S_{dw} = \beta_2 S_{dw} + (1-\beta_2) \times dw^2\)
\(S_{db} = \beta_2 S_{db} + (1-\beta_2) \times db^2\)</li>
    </ul>
  </li>
</ol>

<h4 id="bias-correction">Bias Correction</h4>

<ul>
  <li>
    <p>For <code class="language-plaintext highlighter-rouge">V</code> (Momentum):
 \(V_{dw\_corrected} = \frac{V_{dw}}{1 - \beta_1^T}\)
 \(V_{db\_corrected} = \frac{V_{db}}{1 - \beta_1^T}\)</p>
  </li>
  <li>
    <p>For <code class="language-plaintext highlighter-rouge">S</code> (RMSprop):
 \(S_{dw\_corrected} = \frac{S_{dw}}{1 - \beta_2^T}\)
 \(S_{db\_corrected} = \frac{S_{db}}{1 - \beta_2^T}\)</p>
  </li>
</ul>

<h4 id="parameter-update">Parameter Update</h4>

<ul>
  <li>The weights and biases are updated using both corrected momentum and RMSprop terms.
\(W = W - \alpha \times \frac{V_{dw\_corrected}}{\sqrt{S_{dw\_corrected}} + \epsilon}\)
\(b = b - \alpha \times \frac{V_{db\_corrected}}{\sqrt{S_{db\_corrected}} + \epsilon}\)</li>
</ul>

<h4 id="hyperparameters">Hyperparameters</h4>

<ul>
  <li>\(\alpha\): Learning rate (needs tuning).</li>
  <li>\(\beta_1\): Typically 0.9 (Momentum term).</li>
  <li>\(\beta_2\): Recommended 0.999 (RMSprop term).</li>
  <li>\(\epsilon\): Usually \(10^{-8}\) (avoids division by zero).</li>
</ul>

<h2 id="learning-rate-decay">Learning Rate Decay</h2>

<ul>
  <li><strong>Purpose:</strong> Slowly reduce the learning rate over time.</li>
  <li><strong>Reason:</strong> When implementing mini-batch gradient descent with small mini-batches, the steps can be noisy and the algorithm might never converge.</li>
  <li><strong>Intuition:</strong> Start with larger steps initially (when learning rate is high) and take smaller steps as you approach convergence.</li>
</ul>

<h3 id="methods-for-implementing-learning-rate-decay">Methods for Implementing Learning Rate Decay</h3>

<ol>
  <li><strong>Standard Decay:</strong>
    <ul>
      <li>Formula:</li>
      <li>Example:
        <ul>
          <li>Given: \(\alpha_0 = 0.2\) and decay-rate = 1</li>
          <li>First epoch: \(\alpha = 0.1\)</li>
          <li>Second epoch: \(\alpha = 0.067\)</li>
          <li>and so on…</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Exponential Decay:</strong>
    <ul>
      <li>Formula: \(\alpha = 0.95^{\text{epoch-num}} \times \alpha_0\)</li>
      <li>This results in a rapid decay of the learning rate.</li>
    </ul>
  </li>
  <li><strong>Other Formulae:</strong>
    <ul>
      <li>
\[\alpha = \frac{k}} \times \alpha_0}\]
      </li>
      <li>\(\alpha = \frac{k}} \times \alpha_0\) (Where t is the mini-batch number).</li>
    </ul>
  </li>
  <li><strong>Discrete Staircase:</strong>
    <ul>
      <li>Learning rate decreases in discrete steps. For example, after a certain number of steps, the learning rate is halved.</li>
    </ul>
  </li>
  <li><strong>Manual Decay:</strong>
    <ul>
      <li>If training a single model over many days, some practitioners manually adjust the learning rate when they perceive the learning has slowed down.</li>
    </ul>
  </li>
</ol>

<h3 id="considerations">Considerations</h3>

<ul>
  <li>Learning rate decay is lower in the priority list of hyperparameters to adjust. First, focus on setting an appropriate initial learning rate.</li>
  <li>Systematic methods for choosing among various hyperparameters will be discussed later.</li>
  <li>While learning rate decay can help speed up training, tuning the primary learning rate has a more significant impact.</li>
</ul>

<h2 id="hyperparameter-tuning-guidelines">Hyperparameter Tuning Guidelines</h2>

<ol>
  <li><strong>Overview of Hyperparameters in Neural Networks:</strong>
    <ul>
      <li>Deep learning requires tuning several hyperparameters:
        <ul>
          <li>Learning rate (α)</li>
          <li>Momentum term (β)</li>
          <li>Hyperparameters for the Adam Optimization Algorithm: β1, β2, and ε</li>
          <li>Number of layers</li>
          <li>Number of hidden units</li>
          <li>Learning rate decay</li>
          <li>Mini-batch size</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Hyperparameter Importance:</strong>
    <ul>
      <li>Primary Importance: Learning rate (α) is the most crucial hyperparameter.</li>
      <li>Secondary Importance: Momentum term, mini-batch size, and number of hidden units.</li>
      <li>Tertiary Importance: Number of layers and learning rate decay.</li>
      <li>Seldom Tuned (for Adam algorithm): β1 (usually 0.9), β2 (usually 0.999), and ε (usually 10^-8).</li>
    </ul>
  </li>
  <li><strong>Hyperparameter Sampling Strategies:</strong>
    <ul>
      <li>Traditional Sampling: Earlier machine learning algorithms sampled hyperparameters in a grid format. For two hyperparameters, all combinations in the grid were tried.</li>
      <li>Random Sampling: For deep learning, it’s recommended to choose hyperparameter combinations randomly. This is more effective because:
        <ul>
          <li>Some hyperparameters have a more significant effect than others.</li>
          <li>By sampling randomly, you can explore a broader range of values for the essential hyperparameters.</li>
        </ul>
      </li>
      <li>Coarse to Fine Sampling Scheme: Start with a broad range (coarse sampling). Once you identify a promising region, zoom in and sample more densely within that region (fine sampling).</li>
    </ul>
  </li>
  <li><strong>Advantage of Random Sampling:</strong>
    <ul>
      <li>Allows a richer exploration of crucial hyperparameters.</li>
      <li>It is especially useful when you don’t know which hyperparameters will be most critical for your problem.</li>
    </ul>
  </li>
  <li><strong>Coarse to Fine Sampling:</strong>
    <ul>
      <li>Identify potential regions where hyperparameters seem to work best.</li>
      <li>Zoom into these regions and sample more densely.</li>
      <li>Helps to refine and get closer to optimal hyperparameter values.</li>
    </ul>
  </li>
  <li><strong>Key Takeaways:</strong>
    <ul>
      <li>Prefer random sampling over grid-based sampling for hyperparameters in deep learning.</li>
      <li>Consider using a coarse to fine sampling strategy.</li>
      <li>The next topic will discuss choosing the right scale for sampling hyperparameters.</li>
    </ul>
  </li>
</ol>

<h3 id="sampling-hyperparameters">Sampling Hyperparameters</h3>

<ol>
  <li><strong>Introduction to Hyperparameter Sampling:</strong>
    <ul>
      <li>It’s important to note that when sampling hyperparameters randomly, it doesn’t always mean sampling uniformly over the range of valid values.</li>
      <li>The scale on which hyperparameters are explored can significantly affect the efficiency of the search.</li>
    </ul>
  </li>
  <li><strong>Examples of Uniform Sampling:</strong>
    <ul>
      <li>In cases like determining the number of hidden units within a layer or the number of layers in a neural network, uniform sampling might be appropriate. For instance:
        <ul>
          <li>If considering between 50 and 100 hidden units, simply pick numbers randomly from this range.</li>
          <li>If deciding between 2 to 4 layers, sampling randomly between these values or even using grid search can work.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>The Case of the Learning Rate (alpha):</strong>
    <ul>
      <li>When sampling the learning rate, a range between 0.0001 and 1 is discussed.</li>
      <li>If sampled uniformly, around 90% of the chosen values will fall between 0.1 and 1. This isn’t efficient.</li>
      <li>A more effective method is to sample on a logarithmic scale. For instance, consider values like 0.0001, 0.001, 0.01, 0.1, and 1, and sample randomly on this log scale.</li>
      <li>In Python, you’d achieve this by setting <code class="language-plaintext highlighter-rouge">r = -4 * np.random.rand()</code> and then determining alpha with <code class="language-plaintext highlighter-rouge">alpha = 10  r</code>.</li>
    </ul>
  </li>
  <li><strong>Generalized Method for Logarithmic Sampling:</strong>
    <ul>
      <li>If trying to sample between two values on a log scale, first identify the powers of 10 that define your range.</li>
      <li>Use the log base 10 of your smallest and largest values to define your range (a to b).</li>
      <li>Sample <code class="language-plaintext highlighter-rouge">r</code> randomly between these values (<code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>).</li>
      <li>Your hyperparameter is then set to <code class="language-plaintext highlighter-rouge">10^r</code>.</li>
    </ul>
  </li>
  <li><strong>Sampling the Exponential Weighting Hyperparameter (Beta):</strong>
    <ul>
      <li>Beta is used for calculating exponentially weighted averages.</li>
      <li>If considering beta values between 0.9 and 0.999, it’s inefficient to sample linearly.</li>
      <li>Instead, focus on 1-beta which will have a range between 0.1 to 0.001.</li>
      <li>For efficient sampling, the idea is to use the logarithmic approach again, but this time inverted, with larger values on the left.</li>
      <li>Sample <code class="language-plaintext highlighter-rouge">r</code> from -3 to -1. Then calculate <code class="language-plaintext highlighter-rouge">1-beta</code> as <code class="language-plaintext highlighter-rouge">10^r</code> and subsequently find beta with <code class="language-plaintext highlighter-rouge">beta = 1 - 10^r</code>.</li>
      <li>This approach gives a balanced exploration for beta values, distributing resources evenly over the range.</li>
    </ul>
  </li>
  <li><strong>Rationale Behind the Sampling Process:</strong>
    <ul>
      <li>When beta is near 1, even minor variations can have significant impacts. For instance:
        <ul>
          <li>Shifting beta from 0.9 to 0.9005 has minimal effect.</li>
          <li>But shifting beta from 0.999 to 0.9995 drastically changes the results.</li>
        </ul>
      </li>
      <li>The formula <code class="language-plaintext highlighter-rouge">1 / (1 - beta)</code> is highly sensitive to minor changes in beta when it’s near 1.</li>
      <li>This specialized sampling approach allows for denser sampling when beta is near 1, ensuring efficient hyperparameter search.</li>
    </ul>
  </li>
  <li><strong>Concluding Thoughts:</strong>
    <ul>
      <li>The main goal is to sample hyperparameters efficiently, focusing on regions with higher sensitivity.</li>
      <li>Even if the sampling isn’t perfect, using a “coarse to fine” method can help. Start broad and refine the range in subsequent iterations based on insights.</li>
    </ul>
  </li>
</ol>

<h3 id="organizing-your-hyperparameter-search-process">Organizing Your Hyperparameter Search Process</h3>

<ol>
  <li><strong>Introduction:</strong>
    <ul>
      <li>Hyperparameters play a critical role in deep learning model performance.</li>
      <li>Deep learning applications vary, and hyperparameters suitable for one domain might not be ideal for another.</li>
    </ul>
  </li>
  <li><strong>Cross-fertilization among Domains:</strong>
    <ul>
      <li>Ideas from one domain often inspire improvements in another.</li>
    </ul>
    <ul>
      <li>Examples: Techniques developed in computer vision (like Confonets or ResNets) have been applied to speech. Similarly, methods originated in speech have found applications in NLP.
     - Deep learning researchers increasingly explore findings across various domains for innovative solutions.</li>
    </ul>
  </li>
  <li><strong>Changing Nature of Hyperparameter Intuitions:</strong>
    <ul>
      <li>Intuitions about the best hyperparameters can become outdated due to various reasons:</li>
    </ul>
    <ul>
      <li>Continued algorithm development.</li>
      <li>Gradual changes in data over time.</li>
      <li>Infrastructure changes, like server upgrades.
     - Regularly re-evaluate hyperparameters (every few months) to ensure they remain optimal.</li>
    </ul>
  </li>
  <li>
    <p><strong>Two Predominant Approaches to Hyperparameter Search:</strong></p>

    <p>a. <strong>Babysitting One Model (Panda Approach):</strong></p>
    <ul>
      <li>Most suitable for scenarios with vast data but limited computational resources.</li>
      <li>Process:
        <ol>
          <li>Begin with random parameter initialization.</li>
          <li>Monitor the model’s learning curve, adjusting hyperparameters as required.</li>
          <li>Make incremental changes daily or even hourly, observing the impact on the model’s performance.</li>
          <li>If certain hyperparameters prove detrimental, revert to a previous configuration.</li>
          <li>It’s an ongoing, intensive process where the model’s performance is actively observed and adjusted.</li>
        </ol>
      </li>
    </ul>

    <p>b. <strong>Training Many Models in Parallel (Caviar Approach):</strong></p>
    <ul>
      <li>Ideal for those with significant computational resources.</li>
      <li>Process:
        <ol>
          <li>Simultaneously train multiple models with varied hyperparameters.</li>
          <li>Each model produces a distinct learning curve.</li>
          <li>At the end of the training period, compare the performances and select the best model.</li>
          <li>Analogously, this approach is likened to how certain fish lay numerous eggs in hopes a few survive, compared to mammals that have fewer offspring but tend to them closely.</li>
        </ol>
      </li>
    </ul>
  </li>
  <li><strong>Choosing Between the Approaches:</strong>
    <ul>
      <li>The best approach depends on available computational resources.</li>
      <li>The caviar approach is preferable when abundant computational capacity is available to test multiple hyperparameters at once.</li>
      <li>However, in scenarios with massive datasets and resource-intensive models, the panda approach, where one invests significantly in fine-tuning a single model, might be more feasible.</li>
    </ul>
  </li>
  <li><strong>Final Thoughts:</strong>
    <ul>
      <li>Both approaches have their strengths and applications.</li>
      <li>Regardless of the chosen approach, it’s essential to be adaptive and flexible in the search for optimal hyperparameters.</li>
    </ul>
  </li>
</ol>

<h3 id="batchnorm">BatchNorm</h3>

<ul>
  <li>Batch normalization (often abbreviated as BatchNorm) is a vital technique in deep learning. Introduced by Sergey Ioffe and Christian Szegedy, this method streamlines the hyperparameter search, enhances the robustness of neural networks, and broadens the range of efficient hyperparameters. Furthermore, it significantly aids in training very deep networks.</li>
</ul>

<h2 id="how-does-batch-normalization-work">How does Batch Normalization work?</h2>

<p>When training models like logistic regression, it’s widely known that normalizing the input features can speed up learning. This process involves calculating the means and variances, then adjusting your data based on these metrics. Such normalization can transform the contours of your learning problem, making it more suited for gradient descent optimization.</p>

<p>For deep neural networks with multiple layers, each having its activations, the question is:</p>

<blockquote>
  <p>“Can we normalize activations from any hidden layer to optimize the training of subsequent parameters?”</p>
</blockquote>

<ul>
  <li>That’s where batch normalization enters the scene.</li>
</ul>

<h2 id="the-mechanics-of-batch-normalization">The Mechanics of Batch Normalization:</h2>

<p>Instead of normalizing the activations <code class="language-plaintext highlighter-rouge">a</code>, batch normalization typically normalizes the values <code class="language-plaintext highlighter-rouge">z</code> (before the activation function). Here’s the process:</p>

<ol>
  <li>
    <p><strong>Compute the mean:</strong>
<code class="language-plaintext highlighter-rouge">$$\mu = \frac{1}{m} \sum_{i=1}^{m} z^{[i]}$$</code></p>
  </li>
  <li>
    <p><strong>Compute the variance:</strong>
\(\sigma^2 = \frac{1}{m} \sum_{i=1}^{m} (z^{[i]} - \mu)^2\)</p>
  </li>
  <li>
    <p><strong>Normalize the values:</strong>
\(z_{norm}^{[i]} = \frac{z^{[i]} - \mu}{\sqrt{\sigma^2 + \epsilon}}\)
Where <code class="language-plaintext highlighter-rouge">epsilon</code> is added for numerical stability.</p>
  </li>
  <li>
    <p><strong>Scale and shift the normalized values:</strong>
\(z_{tilde}^{[i]} = \gamma z_{norm}^{[i]} + \beta\)
Where <code class="language-plaintext highlighter-rouge">gamma</code> and <code class="language-plaintext highlighter-rouge">beta</code> are learnable parameters.</p>
  </li>
</ol>

<ul>
  <li>
    <p>By adjusting <code class="language-plaintext highlighter-rouge">gamma</code> and <code class="language-plaintext highlighter-rouge">beta</code>, the mean and variance of the hidden units can be controlled. Setting <code class="language-plaintext highlighter-rouge">gamma</code> to the square root of \(\sigma^2 + \epsilon\) and <code class="language-plaintext highlighter-rouge">beta</code> to <code class="language-plaintext highlighter-rouge">mu</code> makes the batch normalization equivalent to the identity function. However, varying these parameters allows the hidden units to adopt other means and variances.</p>
  </li>
  <li>
    <p>Batch normalization applies this process not only to the input layer but also to deep hidden layers. This ensures that hidden units have a standardized mean and variance, controlled by <code class="language-plaintext highlighter-rouge">gamma</code> and <code class="language-plaintext highlighter-rouge">beta</code>.</p>
  </li>
</ul>

<p>You’ve provided a detailed overview of Batch Normalization (Batch Norm) and its application in deep neural networks. Here’s a succinct summary:</p>

<h3 id="batch-normalization-batchnorm-in-deep-neural-networks">Batch Normalization (BatchNorm) in Deep Neural Networks:</h3>

<ol>
  <li>
    <p><strong>Purpose:</strong> To accelerate training, improve generalization, and allow higher learning rates.</p>
  </li>
  <li><strong>Implementation:</strong>
    <ul>
      <li>For each mini-batch:
        <ul>
          <li>Compute the mean and variance of the activations (\(Z\)).</li>
          <li>Normalize the activations using these statistics.</li>
          <li>Scale and shift the normalized activations using two parameters, \(\beta\) and \(\gamma\), which are learned during training.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Parameters:</strong>
    <ul>
      <li>Traditional Neural Network: Uses weights (\(W\)) and biases (\(B\)).</li>
      <li>With Batch Norm: Still uses weights (\(W\)) but replaces the role of biases (\(B\)) with \(\beta\). Also introduces a scaling factor \(\gamma\). So, the parameters become W, \(\beta\), and \(\gamma\).</li>
    </ul>
  </li>
  <li><strong>Parameter Update:</strong>
    <ul>
      <li>Update \(W\) using standard backpropagation.</li>
      <li>Learn \(\beta\) and \(\gamma\) using derivatives (D\(\beta\) and D\(\gamma\)) derived from the cost function.</li>
      <li>These parameters can be updated using standard gradient descent or other optimization techniques like Adam or RMSprop.</li>
    </ul>
  </li>
  <li><strong>Advantages:</strong>
    <ul>
      <li>Helps handle the internal covariate shift.</li>
      <li>Normalizes the activations throughout the training process, ensuring that they don’t reach extremely high or low values, which can decelerate the learning.</li>
    </ul>
  </li>
  <li>**Elimination of bias term: **
    <ul>
      <li>Since Batch Norm centers activations around zero mean, the bias term \(B\) becomes redundant and can be removed or set to zero.</li>
    </ul>
  </li>
  <li><strong>Compatibility:</strong>
    <ul>
      <li>Batch Norm can be used with other optimization techniques, such as momentum, RMSprop, or Adam, making it versatile.</li>
    </ul>
  </li>
  <li>**Practicality: **
    <ul>
      <li>In most modern deep learning frameworks, Batch Norm is implemented, so users don’t often have to code it from scratch.</li>
      <li>It usually requires just one line of code to implement in these frameworks.</li>
    </ul>
  </li>
  <li>**How it Fits in Deep Neural Networks: **
    <ul>
      <li>After computing Z (before activation), apply Batch Norm.</li>
      <li>Then, apply the activation function to get \(A\).</li>
      <li>This process is repeated for each layer where Batch Norm is used.</li>
    </ul>
  </li>
  <li>**Note on Confusion with \(\beta\) Terminology: **
    <ul>
      <li>\(\beta\) in Batch Norm isn’t the same as \(\beta\) in optimization algorithms like momentum or Adam.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>As we’ve seen, batch normalization (batch norm) is a technique used to improve the speed, performance, and stability of neural networks.</li>
</ul>

<ol>
  <li><strong>Why Batch Norm Works:</strong>
    <ul>
      <li>Feature Scaling: Normalizing input features to have a mean of zero and variance of one can accelerate the learning process. Batch norm extends this idea by normalizing the values within the hidden layers, not just the input features.</li>
      <li>Robustness to Weights’ Changes: Deep neural networks can be sensitive to changes in weights in earlier layers, which might impact weights in subsequent layers. Batch norm makes the deeper layers less sensitive to such changes.</li>
    </ul>
  </li>
  <li><strong>Covariate Shift:</strong>
    <ul>
      <li>When the distribution of input data changes, it can impact a model’s performance. This is termed ‘covariate shift’. In the context of neural networks, as weights in early layers change, values to subsequent layers also change, leading to a sort of internal covariate shift.</li>
    </ul>
  </li>
  <li><strong>What Batch Norm Does:</strong>
    <ul>
      <li>For any given layer in the neural network, batch norm ensures that even if values from preceding layers change, their mean and variance remain consistent. This consistency provides stability, allowing each layer to learn more independently and accelerating the learning process across the entire network.</li>
    </ul>
  </li>
  <li><strong>Regularization Effect:</strong>
    <ul>
      <li>An additional benefit of batch norm is its slight regularization effect. As batch norm computes mean and variance based on mini-batches, there’s inherent noise in the values, somewhat similar to the noise added by the dropout technique. This noise prevents over-reliance on any single hidden unit, which aids in regularization. However, the main purpose of batch norm isn’t regularization; it’s to normalize activations and speed up learning.</li>
    </ul>
  </li>
  <li><strong>Handling Data during Testing:</strong>
    <ul>
      <li>Batch norm processes data in mini-batches, computing mean and variances accordingly. But when making predictions (especially for single data points), a different approach is needed since there might not be a mini-batch available.</li>
    </ul>
  </li>
  <li><strong>Training-Time Batch Norm:</strong>
    <ul>
      <li>F<strong>or each mini-batch:</strong>
        <ul>
          <li>Calculate the mean \(\mu\) and variance \(\sigma^2\) using only the examples in that mini-batch.</li>
          <li>Normalize the activations using \(\mu\) and \(\sigma^2\).</li>
          <li>Scale and shift the normalized activations using learnable parameters \(\gamma\) and \(\beta\).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Problem at Test Time:</strong>
    <ul>
      <li>During inference, you might only be processing one example at a time. Computing \(\mu\) and \(\sigma^2\) for a single example doesn’t make sense.</li>
    </ul>
  </li>
  <li><strong>Solution for Test Time:</strong>
    <ul>
      <li>During training, maintain an exponentially weighted (running) average of \(\mu\) and \(\sigma^2\) seen across mini-batches.</li>
      <li>At test time, normalize activations using these averaged values of \(\mu\) and \(\sigma^2\), and then apply the learned \(\gamma\) and \(\beta\) to scale and shift.</li>
    </ul>
  </li>
  <li><strong>Flexibility &amp; Robustness:</strong>
    <ul>
      <li>The method to estimate \(\mu\) and \(\sigma^2\) for test time is fairly robust. While an exponentially weighted average is commonly used, other reasonable methods could also be employed.</li>
      <li>Many deep learning frameworks have default methods for this, relieving the user from manually implementing the test-time adaptation.</li>
    </ul>
  </li>
  <li><strong>Takeaway:</strong>
    <ul>
      <li>Batch normalization helps in training deeper networks and accelerates training.</li>
      <li>Proper adjustments are needed when using batch norm during inference to ensure consistency and accurate predictions.</li>
    </ul>
  </li>
</ol>

<h3 id="softmax-regression-and-multi-class-classification">Softmax Regression and Multi-Class Classification</h3>

<ul>
  <li><strong>Introduction to Multi-Class Classification</strong>:
    <ul>
      <li>Until now, the focus was on binary classification (e.g., determining whether an image is a cat or not).</li>
      <li>However, there are situations where more than two classes need to be classified.</li>
      <li>Example: Classifying animals into categories such as cats (class 1), dogs (class 2), baby chicks (class 3), or none of the above (class 0).</li>
    </ul>
  </li>
  <li><strong>Softmax Regression</strong>:
    <ul>
      <li>It’s a generalization of logistic regression designed for multi-class classification tasks.</li>
      <li>Uses a symbol \(C\) to represent the total number of classes.</li>
      <li>In the given example, \(C\) is 4, representing the four animal classes.</li>
    </ul>
  </li>
  <li><strong>Neural Network Design for Softmax</strong>:
    <ul>
      <li>The top layer of the neural network has \(C\) units.</li>
      <li>Each unit represents the probability of one class.
        <ul>
          <li>E.g., The first unit might represent the probability that the input belongs to class 0 (“other”).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Softmax Output</strong>:
    <ul>
      <li>The output \(y_{hat}\) from the Softmax layer is a 4x1 vector in the given example.</li>
      <li>This vector represents the probabilities of the input belonging to each of the four classes.</li>
      <li>The sum of these probabilities should always equal 1.</li>
    </ul>
  </li>
  <li><strong>Implementing Softmax in Neural Networks</strong>:
    <ul>
      <li>The computation in the last layer of the network involves determining \(z_{L}\), which is the product of weights \(w_{L}\) and activations from the previous layer plus biases.</li>
      <li>The Softmax activation function then processes this \(z_{L}\).
        <ul>
          <li>It involves computing a temporary variable \(t\) as the exponentiation of \(z_{L}\) element-wise.</li>
          <li>The final output \(a_{L}\) is then derived by normalizing \(t\) such that the sum of its elements equals 1.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Working Example</strong>:
    <ul>
      <li>For a given \(z_{L}\) of [5, 2, -1, 3], the corresponding \(t\) values are calculated. These are then normalized to produce the final \(a_{L}\) probabilities.</li>
    </ul>
  </li>
  <li><strong>Unique Feature of Softmax Activation Function</strong>:
    <ul>
      <li>Unlike other activation functions like sigmoid, the Softmax activation function takes a vector as input and outputs another vector. This is because it has to normalize the probabilities across different classes.</li>
    </ul>
  </li>
  <li><strong>Visual Representation</strong>:
    <ul>
      <li>The neural network using Softmax can represent decision boundaries which are linear between classes.</li>
      <li>With more hidden layers, the network can represent even more complex non-linear boundaries.</li>
      <li>Softmax regression and the associated neural network design provide a way to tackle multi-class classification problems effectively.</li>
    </ul>
  </li>
</ul>

<h3 id="deepening-understanding-of-softmax-classification-and-training">Deepening Understanding of Softmax Classification and Training</h3>

<ul>
  <li><strong>Softmax Recap</strong>:
    <ul>
      <li>The <strong>Softmax activation function</strong> was introduced, transforming a <code class="language-plaintext highlighter-rouge">z[L]</code> vector into probabilities for each class.</li>
      <li>A <code class="language-plaintext highlighter-rouge">t</code> temporary variable undergoes element-wise exponentiation.</li>
      <li>The final output, <code class="language-plaintext highlighter-rouge">a[L]</code>, is derived from the <code class="language-plaintext highlighter-rouge">t</code> variable, ensuring it sums to 1.</li>
    </ul>
  </li>
  <li><strong>Understanding Softmax</strong>:
    <ul>
      <li><strong>Hardmax</strong> function: Selects the highest value in <code class="language-plaintext highlighter-rouge">z</code> and makes it 1 while setting others to 0.</li>
      <li><strong>Softmax</strong>: Soft version of hardmax. Gives probabilities for each class.</li>
    </ul>
  </li>
  <li><strong>Logistic Regression and Softmax</strong>:
    <ul>
      <li>Softmax is a generalization of logistic regression for multiple classes (<code class="language-plaintext highlighter-rouge">C &gt; 2</code>).</li>
      <li>For <code class="language-plaintext highlighter-rouge">C=2</code>, softmax effectively becomes logistic regression.</li>
    </ul>
  </li>
  <li><strong>Training with Softmax Output</strong>:
    <ul>
      <li>If a training example, represented as a vector (e.g., <code class="language-plaintext highlighter-rouge">0 1 0 0</code> for a cat), doesn’t match the neural network’s output, the model needs adjusting.</li>
    </ul>
  </li>
  <li><strong>Loss Function</strong>:
    <ul>
      <li>In softmax, the loss function is the negative sum across all classes, involving <code class="language-plaintext highlighter-rouge">y</code> (true labels) and <code class="language-plaintext highlighter-rouge">y hat</code> (predicted probabilities).</li>
      <li>This loss function ensures the probability of the correct class is maximized, following a principle similar to <strong>maximum likelihood estimation</strong>.</li>
    </ul>
  </li>
  <li><strong>Cost Function</strong>:
    <ul>
      <li>The overall cost <code class="language-plaintext highlighter-rouge">J</code> is determined by summing the loss across all training examples.</li>
    </ul>
  </li>
  <li><strong>Vectorization</strong>:
    <ul>
      <li>For efficient computation, outputs <code class="language-plaintext highlighter-rouge">y</code> and predictions <code class="language-plaintext highlighter-rouge">y hat</code> are stacked horizontally, creating matrices of dimensions <code class="language-plaintext highlighter-rouge">4 by m</code> (in a scenario with 4 classes and <code class="language-plaintext highlighter-rouge">m</code> training examples).</li>
    </ul>
  </li>
  <li><strong>Implementing Gradient Descent with Softmax</strong>:
    <ul>
      <li>The backpropagation for softmax requires calculating the derivative with respect to <code class="language-plaintext highlighter-rouge">z</code> at the output layer.</li>
      <li>This derivative, denoted <code class="language-plaintext highlighter-rouge">dz[L]</code>, can be found by subtracting the true label vector <code class="language-plaintext highlighter-rouge">y</code> from the prediction <code class="language-plaintext highlighter-rouge">y hat</code>.</li>
      <li>Modern deep learning frameworks handle the backpropagation automatically, provided the forward pass is correctly specified.</li>
      <li>Softmax allows classification into one of <code class="language-plaintext highlighter-rouge">C</code> possible classes.</li>
    </ul>
  </li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/vinija">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                        viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">vinija</span> -->
               
<!--               <a href="">-->
<!--                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"-->
<!--                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">-->
<!--                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN-->
<!--                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx-->
<!--                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa-->
<!--                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/-->
<!--                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ-->
<!--                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o-->
<!--                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT-->
<!--                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL-->
<!--                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ-->
<!--                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ-->
<!--                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu-->
<!--                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0-->
<!--                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3-->
<!--                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ-->
<!--                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47-->
<!--                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32-->
<!--                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns-->
<!--                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2-->
<!--                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66-->
<!--                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M-->
<!--                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI-->
<!--                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j-->
<!--                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP-->
<!--                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+-->
<!--                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah-->
<!--                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B-->
<!--                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k-->
<!--                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X-->
<!--                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq-->
<!--                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX-->
<!--                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO-->
<!--                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu-->
<!--                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv-->
<!--                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9-->
<!--                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX-->
<!--                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO-->
<!--                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L-->
<!--                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm-->
<!--                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx-->
<!--                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb-->
<!--                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j-->
<!--                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV-->
<!--                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei-->
<!--                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd-->
<!--                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL-->
<!--                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy-->
<!--                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX-->
<!--                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23-->
<!--                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH-->
<!--                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV-->
<!--                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX-->
<!--                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K-->
<!--                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9-->
<!--                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg-->
<!--                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8-->
<!--                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3-->
<!--                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i-->
<!--                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ-->
<!--                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo-->
<!--                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ-->
<!--                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y-->
<!--                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr-->
<!--                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD-->
<!--                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND-->
<!--                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa-->
<!--                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K-->
<!--                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG-->
<!--                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU-->
<!--                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY-->
<!--                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW-->
<!--                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP-->
<!--                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq-->
<!--                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg-->
<!--                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF-->
<!--                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW-->
<!--                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w-->
<!--                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd-->
<!--                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30-->
<!--                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q-->
<!--                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve-->
<!--                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g-->
<!--                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch-->
<!--                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG-->
<!--                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs-->
<!--                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB-->
<!--                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP-->
<!--                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im-->
<!--                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t-->
<!--                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ-->
<!--                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ-->
<!--                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5-->
<!--                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa-->
<!--                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp-->
<!--                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV-->
<!--                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11-->
<!--                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb-->
<!--                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R-->
<!--                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S-->
<!--                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY-->
<!--                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63-->
<!--                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ-->
<!--                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT-->
<!--                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2-->
<!--                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL-->
<!--                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ-->
<!--                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg-->
<!--                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI-->
<!--                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ-->
<!--                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem-->
<!--                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW-->
<!--                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje-->
<!--                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa-->
<!--                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd-->
<!--                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V-->
<!--                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA-->
<!--                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo-->
<!--                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP-->
<!--                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt-->
<!--                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y-->
<!--                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2-->
<!--                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX-->
<!--                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB-->
<!--                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt-->
<!--                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR-->
<!--                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ-->
<!--                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1-->
<!--                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H-->
<!--                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB-->
<!--                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC-->
<!--                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h-->
<!--                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO-->
<!--                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9-->
<!--                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD-->
<!--                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf-->
<!--                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp-->
<!--                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD-->
<!--                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8-->
<!--                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H-->
<!--                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h-->
<!--                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU-->
<!--                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba-->
<!--                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT-->
<!--                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr-->
<!--                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0-->
<!--                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb-->
<!--                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi-->
<!--                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy-->
<!--                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77-->
<!--                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy-->
<!--                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk-->
<!--                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe-->
<!--                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO-->
<!--                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ-->
<!--                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9-->
<!--                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR-->
<!--                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ-->
<!--                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS-->
<!--                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc-->
<!--                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS-->
<!--                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht-->
<!--                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0-->
<!--                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv-->
<!--                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+-->
<!--                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX-->
<!--                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f-->
<!--                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv-->
<!--                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew-->
<!--                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f-->
<!--                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib-->
<!--                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w-->
<!--                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3-->
<!--                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn-->
<!--                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7-->
<!--                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk-->
<!--                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a-->
<!--                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf-->
<!--                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2-->
<!--                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC-->
<!--                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN-->
<!--                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW-->
<!--                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP-->
<!--                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb-->
<!--                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+-->
<!--                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K-->
<!--                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/-->
<!--                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT-->
<!--                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ-->
<!--                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU-->
<!--                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf-->
<!--                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i-->
<!--                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX-->
<!--                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho-->
<!--                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ-->
<!--                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa-->
<!--                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p-->
<!--                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH-->
<!--                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+-->
<!--                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA-->
<!--                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T-->
<!--                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm-->
<!--                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb-->
<!--                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr-->
<!--                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2-->
<!--                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB-->
<!--                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp-->
<!--                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T-->
<!--                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj-->
<!--                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX-->
<!--                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek-->
<!--                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD-->
<!--                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ-->
<!--                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x-->
<!--                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz-->
<!--                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v-->
<!--                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N-->
<!--                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju-->
<!--                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6-->
<!--                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T-->
<!--                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE-->
<!--                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+-->
<!--                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep-->
<!--                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ-->
<!--                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc-->
<!--                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX-->
<!--                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/-->
<!--                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv-->
<!--                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z-->
<!--                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg-->
<!--                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9-->
<!--                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L-->
<!--                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af-->
<!--                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF-->
<!--                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv-->
<!--                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ-->
<!--                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0-->
<!--                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx-->
<!--                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1-->
<!--                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx-->
<!--                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm-->
<!--                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC-->
<!--                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy-->
<!--                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY-->
<!--                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC" />-->
<!--                  </svg>-->
<!--               </a>-->

               <a href="mailto:vinija@gmail.com">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>

               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="../../index.html">www.vinija.ai</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="../../js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="../../js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="../../js/mode-switcher.js"></script>
    <!-- mathjax -->
    <script type="text/javascript" src="../../../cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJaxdda6.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="../../js/clipboard.min.js"></script>
    <script src="../../js/copy.js"></script>      
    </body>

<!-- Mirrored from vinija.ai/CourseraDL/improving-deep-neural-networks/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 12 Jul 2025 14:02:13 GMT -->
</html>
