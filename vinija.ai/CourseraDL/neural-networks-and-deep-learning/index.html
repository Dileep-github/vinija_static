<!DOCTYPE html>
<html lang="en">

  
<!-- Mirrored from vinija.ai/CourseraDL/neural-networks-and-deep-learning/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 12 Jul 2025 14:02:12 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Vinija's Notes • Coursera-DL • Neural Networks and Deep Learning</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Vinija's detailed AI Notes">
  <link rel="canonical" href="index.html">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Vinija's AI Notes" href="../../feed.xml">
  
  <link href="../../favicon.html" rel="shortcut icon" />

  <!-- Google ads -->
  <script async src="../../../pagead2.googlesyndication.com/pagead/js/ff0a8.txt?client=ca-pub-5905744527956213"
     crossorigin="anonymous"></script>
</head>



    <body>

      <script src="../../../unpkg.com/vanilla-back-to-top%407.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../index-2.html">Vinija's AI Notes</a>

  <a class="site-link" href="../../index.html">Back to vinija.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="../../js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://vinija.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Coursera-DL • Neural Networks and Deep Learning</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#supervised-learning" id="markdown-toc-supervised-learning">Supervised Learning</a>    <ul>
      <li><a href="#sequential-data" id="markdown-toc-sequential-data">Sequential Data</a></li>
      <li><a href="#non-sequential-data" id="markdown-toc-non-sequential-data">Non-sequential Data</a></li>
    </ul>
  </li>
  <li><a href="#neural-network-programming-basics" id="markdown-toc-neural-network-programming-basics">Neural Network Programming Basics</a></li>
  <li><a href="#logistic-regression" id="markdown-toc-logistic-regression">Logistic Regression</a>    <ul>
      <li><a href="#gradient-descent" id="markdown-toc-gradient-descent">Gradient Descent</a></li>
    </ul>
  </li>
  <li><a href="#formalizing-neural-networks" id="markdown-toc-formalizing-neural-networks">Formalizing Neural Networks</a></li>
  <li><a href="#backpropagation" id="markdown-toc-backpropagation">Backpropagation</a></li>
  <li><a href="#single-hidden-layer-neural-networks" id="markdown-toc-single-hidden-layer-neural-networks">Single Hidden Layer Neural Networks</a>    <ul>
      <li><a href="#notations" id="markdown-toc-notations">Notations</a></li>
      <li><a href="#parameters" id="markdown-toc-parameters">Parameters</a></li>
    </ul>
  </li>
  <li><a href="#computing-nns-output" id="markdown-toc-computing-nns-output">Computing NN’s output</a></li>
  <li><a href="#activation-functions-in-neural-networks" id="markdown-toc-activation-functions-in-neural-networks">Activation Functions in Neural Networks:</a></li>
  <li><a href="#vectorization-in-python" id="markdown-toc-vectorization-in-python">Vectorization in Python</a>    <ul>
      <li><a href="#demo" id="markdown-toc-demo">Demo</a></li>
    </ul>
  </li>
  <li><a href="#matrix-multiplication" id="markdown-toc-matrix-multiplication">Matrix Multiplication</a></li>
  <li><a href="#element-wise-operations" id="markdown-toc-element-wise-operations">Element-Wise Operations</a></li>
  <li><a href="#forward-propagation" id="markdown-toc-forward-propagation">Forward Propagation</a>    <ul>
      <li><a href="#the-z-matrix" id="markdown-toc-the-z-matrix">The Z Matrix</a></li>
      <li><a href="#the-a-matrix" id="markdown-toc-the-a-matrix">The A Matrix</a></li>
    </ul>
  </li>
  <li><a href="#broadcasting-in-python" id="markdown-toc-broadcasting-in-python">Broadcasting in Python</a></li>
  <li><a href="#weight-initialization-in-neural-networks" id="markdown-toc-weight-initialization-in-neural-networks">Weight Initialization in Neural Networks</a>    <ul>
      <li><a href="#how-random-initialization-helps" id="markdown-toc-how-random-initialization-helps">How Random Initialization Helps</a></li>
      <li><a href="#practical-tips-for-weight-initialization" id="markdown-toc-practical-tips-for-weight-initialization">Practical Tips for Weight Initialization</a></li>
    </ul>
  </li>
  <li><a href="#forward-propogation" id="markdown-toc-forward-propogation">Forward Propogation</a></li>
  <li><a href="#why-deep-neural-networks-work-well" id="markdown-toc-why-deep-neural-networks-work-well">Why Deep Neural Networks Work Well</a></li>
  <li><a href="#hyperparameters-in-deep-learning" id="markdown-toc-hyperparameters-in-deep-learning">Hyperparameters in Deep Learning</a>    <ul>
      <li><a href="#parameters-1" id="markdown-toc-parameters-1">Parameters</a></li>
    </ul>
  </li>
  <li><a href="#features-vs-parameters-vs-hyperparameters" id="markdown-toc-features-vs-parameters-vs-hyperparameters">Features vs. Parameters vs. Hyperparameters</a></li>
</ul>

<h2 id="overview">Overview</h2>

<ul>
  <li>Deep Learning is a subset of Machine Learning that emphasizes the use of Neural Networks. It often involves training expansive and complex Neural Networks.</li>
  <li>Neural Networks are composed of numerous neurons. These neurons process inputs (e.g., the size of a house) to predict outcomes.
    <ul>
      <li>In the case that they use ReLU, they compute a linear function and apply a function that takes the maximum value of the result and zero, finally outputting an estimated price.</li>
      <li>The ReLU function outputs the input directly if it is positive; otherwise, it outputs zero.</li>
      <li>Mathematically, it’s defined as:
\(f(x) = \max(0, x)\)</li>
      <li>So, when an input \(x\) is passed through the ReLU function:
        <ul>
          <li>If \(\)x\(\) is positive, it returns \(\)x\(\).</li>
          <li>If \(\)x\(\) is zero or negative, it returns 0.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>In practice, a Neural Network is seldom made up of a single neuron. Instead, it’s generally a collection of numerous neurons, each receiving various inputs. These neurons collaborate to produce a more accurate and complex output prediction.</li>
  <li>Example:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Neural</span> <span class="n">Network</span><span class="p">:</span>

<span class="n">Input</span> <span class="n">Layer</span>      <span class="n">Hidden</span> <span class="n">Layer</span>      <span class="n">Output</span>
  <span class="p">.</span>  <span class="p">.</span>  <span class="p">.</span>        <span class="o">/</span>  \  <span class="o">/</span>  \       <span class="p">.</span>
  <span class="p">.</span>  <span class="p">.</span>  <span class="p">.</span>  <span class="o">----&gt;</span>  <span class="p">.</span>  <span class="p">.</span>  <span class="p">.</span>  <span class="p">.</span>  <span class="o">----&gt;</span>  <span class="p">.</span>
  <span class="p">.</span>  <span class="p">.</span>  <span class="p">.</span>        \  <span class="o">/</span>  \  <span class="o">/</span>       <span class="p">.</span>

<span class="n">Legend</span><span class="p">:</span>
<span class="p">.</span> <span class="o">=</span> <span class="n">Neuron</span>
<span class="o">/</span> \ <span class="o">=</span> <span class="n">Connections</span> <span class="n">between</span> <span class="n">neurons</span>
<span class="o">----&gt;</span> <span class="o">=</span> <span class="n">Flow</span> <span class="n">of</span> <span class="n">data</span>
</code></pre></div></div>

<ul>
  <li><strong>In the above diagram:</strong>
    <ul>
      <li>The “Input Layer” consists of individual neurons receiving various inputs.</li>
      <li>The “Hidden Layer” is a layer of neurons processing the information from the Input Layer.</li>
      <li>The “Output” is the final prediction or result produced by the network.</li>
    </ul>
  </li>
</ul>

<h2 id="supervised-learning">Supervised Learning</h2>

<ul>
  <li>At its core, supervised learning revolves around the idea of using labeled data, where we have both the input \(x\) and its corresponding desired output \(y\). The main aim is to learn a function that can establish a mapping between the two.</li>
  <li>Neural Networks, a cornerstone in the field of Deep Learning, have been widely employed for supervised learning tasks. Their adaptability and capacity for complex representations make them ideal for this purpose.</li>
  <li>Domains like Natural Language Processing (NLP) have seen neural networks being used for tasks such as sentiment analysis, machine translation, and named entity recognition.</li>
  <li>In the area of Speech Recognition, neural networks help in converting spoken words into text, understanding voice commands, and even distinguishing between different speakers.</li>
  <li>The realm of Computer Vision, which deals with allowing machines to interpret and make decisions based on visual data, heavily relies on neural networks for tasks like image recognition, object detection, and facial recognition.</li>
  <li>Beyond these, neural networks in supervised learning also find applications in healthcare for diagnostic assistance, in finance for predicting stock market movements, and even in gaming for creating more challenging and adaptive AI opponents.</li>
</ul>

<h3 id="sequential-data">Sequential Data</h3>
<ul>
  <li>Sequential data represents a series of data points indexed or listed in a specific order based on time or another sequential metric. Examples include time series data like stock prices, audio signals, and natural language sentences, where the order of data points is crucial for understanding and analysis.</li>
  <li>Here’s how neural networks handle sequential data:
    <ol>
      <li><strong>Recurrent Neural Networks (RNNs):</strong> RNNs are designed explicitly for sequential data. They possess a memory-like mechanism, allowing them to retain and process information from previous inputs in the sequence. This makes them particularly useful for tasks where context from earlier in the sequence is required to understand later parts, such as in language modeling or speech recognition.</li>
      <li><strong>Long Short-Term Memory (LSTM) networks:</strong> LSTMs are a specialized form of RNNs that address some of the traditional RNNs’ limitations, like the vanishing gradient problem. They can remember patterns over longer sequences than typical RNNs, making them better suited for tasks like machine translation or time series prediction with longer patterns.</li>
      <li><strong>Gated Recurrent Units (GRUs):</strong> GRUs are another variant of RNNs, simplifying the LSTM architecture. They also deal with the limitations of traditional RNNs and are effective for various sequential data tasks.</li>
      <li><strong>Attention Mechanisms and Transformers:</strong> These are newer architectures that, while not being recurrent, can handle sequences by attending to different parts of the input data based on their importance. Transformers, in particular, have revolutionized tasks in NLP due to their efficiency and capability to handle long-term dependencies in sequences.</li>
      <li><strong>Time-Distributed Layers:</strong> In certain neural network architectures, layers can be wrapped in a time-distributed manner, meaning they apply the same operation (like a dense layer) to every time step of a sequential input independently.</li>
      <li><strong>1D Convolutional Layers:</strong> While often associated with image processing, convolutional layers can also be applied to sequential data, capturing local patterns within sequences.</li>
    </ol>
  </li>
</ul>

<h3 id="non-sequential-data">Non-sequential Data</h3>
<ul>
  <li>Feedforward Neural Networks (FNNs) or Multi-layer Perceptrons (MLPs):
    <ul>
      <li>These are the most straightforward type of neural networks and are especially suitable for non-sequential data.</li>
      <li>They consist of an input layer, one or more hidden layers, and an output layer.</li>
      <li>Each neuron in one layer connects with every neuron in the subsequent layer, allowing for the combination and transformation of features.</li>
      <li>These networks are particularly useful for tasks like regression, classification, and even more complex tasks when combined with other techniques.</li>
    </ul>
  </li>
</ul>

<h2 id="neural-network-programming-basics">Neural Network Programming Basics</h2>
<ol>
  <li><strong>Processing Training Data:</strong> When implementing neural networks, you don’t usually process the training set using explicit loops through each training example. Instead, you process the entire set at once.</li>
  <li><strong>Computation Organization:</strong> Neural network computations can be divided into two main phases: forward propagation and backward propagation.</li>
  <li><strong>Binary Classification Example:</strong> An image can be classified as either a “cat” (output 1) or “not-cat” (output 0). Images are represented as matrices for red, green, and blue color channels.
    <ul>
      <li>Image Representation: A 64x64 image is represented by three 64x64 matrices for RGB values. These matrices are then unrolled into a feature vector. The resulting vector from a 64x64x3 image will have 12,288 dimensions.</li>
      <li><strong>Notation:</strong>
        <ul>
          <li>\(x\) represents the input feature vector.</li>
          <li>\(y\) denotes the output label (1 for cat and 0 for not-cat).</li>
          <li>A training example is denoted by \((x,y)\).</li>
          <li>\(m\) represents the number of training examples. \(m_{train}\) refers specifically to training examples, and \(m_{test}\) refers to test examples.</li>
        </ul>
      </li>
      <li>The matrix \(\)x\(\) is formed by stacking individual training examples in columns, making it an  nx \times m  dimensional matrix.</li>
      <li>The matrix \(\)y\(\) stacks the corresponding labels, resulting in a 1x m  matrix.</li>
    </ul>
  </li>
</ol>

<h2 id="logistic-regression">Logistic Regression</h2>
<ul>
  <li><strong>Logistic Regression:</strong> This model has parameters  W  and  B . The output \(\hat{y}\) is determined by the sigmoid function applied to  w  transpose \(x\) plus  b .</li>
  <li><strong>Training:</strong> You have a set of  m  training examples. The objective is to get \(\hat{y}\) close to the true labels \(y\) in the training set. The prediction for the  i^{th}  training sample,  y-hat(i) , is obtained by applying the sigmoid function to  W  transpose  X(i)  plus  B . The notation  (i)  indicates data associated with the  i^{th}  training example.</li>
  <li><strong>Loss Function:</strong> Measures the difference between the predicted output \(\hat{y}\) and the true label \(y\). Squared error might seem reasonable, but it’s not ideal for logistic regression as it leads to a non-convex optimization problem. Instead, logistic regression uses a different loss function:  \(-y \log{\hat{y}} + (1-y) \log{1-\hat{y}}\). The objective is to make this loss as small as possible.
    <ul>
      <li>If \(y\) is 1, you want \(\hat{y}\) to be close to 1.</li>
      <li>If \(y\) is 0, you want \(\hat{y}\) to be close to 0.</li>
    </ul>
  </li>
  <li><strong>Cost Function:</strong> Represents the average loss across all training examples. It’s defined as the average of the loss functions for each training example. The aim is to find parameters \(W\) and \(B\) that minimize this cost function.</li>
  <li><strong>Conclusion:</strong> Logistic regression is foundational and its setup can be viewed as a tiny neural network. The next discussion will delve into viewing logistic regression as a miniature neural network.</li>
</ul>

<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
  <li>
    <p>The logistic regression model involves parameters \(w\) and \(b\) which influence how well the model performs on a training set. The model’s performance is quantified using two functions:</p>

    <ol>
      <li><strong>Loss function:</strong> Measures the model’s performance on an individual training example.</li>
      <li><strong>Cost function \(J(w, b)\):</strong> An average of the loss function across the entire training set. It essentially evaluates how well the parameters \(w\) and \(b\) are doing on the entire set. The objective is to find \(w\) and \(b\) that minimize the cost function.</li>
    </ol>
  </li>
  <li>
    <p>The gradient descent algorithm is introduced as a tool to train or adjust the parameters \(w\) and \(b\). It starts from an initial point and takes iterative steps in the steepest downhill direction of the cost function, aiming to find its minimum value. An important feature of the cost function for logistic regression is that it’s convex, meaning it has a singular bowl-like shape and no local minima.</p>
  </li>
  <li>Some key details about gradient descent:
    <ul>
      <li>The learning rate, \(\alpha\), controls the size of each step.</li>
      <li>The derivative term, represented as \(\frac{dJ(w)}{dw}\), dictates the direction of the step based on the slope of the function. In code, this derivative term is represented with the variable <code class="language-plaintext highlighter-rouge">dw</code>.</li>
    </ul>
  </li>
  <li>The process adjusts both \(w\) and \(b\) using their respective derivatives. For functions of multiple variables, a notation involving partial derivatives is used. In code, the update quantity for \(b\) is denoted as <code class="language-plaintext highlighter-rouge">db</code>.</li>
</ul>

<h2 id="formalizing-neural-networks">Formalizing Neural Networks</h2>

<ul>
  <li>
    <p>A neural network can be visualized as multiple logistic regressions stacked together. The idea is to have multiple layers of computations. Each node in the network does two primary computations:</p>

    <ol>
      <li>Calculate a z-value</li>
      <li>Compute an a-value based on the z-value</li>
    </ol>
  </li>
  <li>
    <p>To clarify the notation used:</p>

    <ul>
      <li>Superscript square brackets (e.g., \([1]\) or \([2]\)) are used to denote different layers in the neural network. For instance, \(z[1]\) and \(a[1]\) are the computations associated with the first layer, while \(z[2]\) and \(a[2]\) correspond to the second layer.</li>
      <li>Do not confuse these with the superscript round brackets (e.g., (i)), which refer to individual training examples.</li>
      <li>The neural network functions by taking the input features \(x\), applying parameters \(w\) and \(b\), and then running through each layer’s computations. Ultimately, \(a[2]\) or \(\hat{y}\)y is the final output of the network.</li>
    </ul>
  </li>
</ul>

<h2 id="backpropagation">Backpropagation</h2>
<ol>
  <li><strong>What is Backpropagation?</strong>
    <ul>
      <li>Backpropagation is a supervised learning algorithm, for training multi-layer perceptrons (often called “neural networks”). It’s a type of optimization algorithm, specifically a gradient descent method, used to minimize the error in the network’s predictions by adjusting the weights and biases.</li>
    </ul>
  </li>
  <li>**The Basic Idea **
    <ul>
      <li>Imagine you’re trying to learn archery. You shoot an arrow, and it misses the mark. Based on how far and in which direction it missed, you adjust your aim. That’s essentially what backpropagation does, but for neural networks. The neural network makes a prediction (like shooting an arrow), calculates how far off it was from the expected result (the error), and then travels back through the network to adjust its weights and biases to reduce that error (adjusting the aim).</li>
    </ul>
  </li>
  <li>**The Process **
    <ul>
      <li>Forward Pass: Input data is passed through the network layer-by-layer, from the input layer to the output layer, producing a prediction.</li>
      <li>Compute the Loss: The difference between the prediction and the true value (the “error”) is calculated using a loss function.</li>
      <li>Backward Pass: This is where backpropagation starts. The error is passed backward through the network. Here, the partial derivatives of the error with respect to each weight and bias are computed using the chain rule from calculus. This step gives us a “gradient”, which points in the direction of the steepest ascent of the error.</li>
      <li>Update Weights and Biases: The weights and biases are then adjusted in the opposite direction of this gradient, aiming to reduce the error. This is done using an optimization algorithm, most commonly gradient descent or its variants.</li>
      <li>Iterate: The process is repeated (often many times) until the model’s predictions are satisfactory, or further training no longer reduces the error.</li>
    </ul>
  </li>
  <li>**Intuition Behind The Math **
    <ul>
      <li>Backpropagation uses the chain rule of calculus to compute gradients. Suppose you have a function <code class="language-plaintext highlighter-rouge">f(g(x))</code>. The chain rule says that the derivative of this function with respect to <code class="language-plaintext highlighter-rouge">x</code> is the product of the derivative of <code class="language-plaintext highlighter-rouge">f</code> concerning <code class="language-plaintext highlighter-rouge">g(x)</code> and the derivative of <code class="language-plaintext highlighter-rouge">g</code> concerning <code class="language-plaintext highlighter-rouge">x</code>. Now, imagine a network with many layers; to compute the gradient at the beginning (input layer), you multiply the gradients of all layers that follow it, similar to how you’d apply the chain rule repeatedly for nested functions.</li>
    </ul>
  </li>
  <li><strong>Importance of Backpropagation</strong>
    <ul>
      <li>Efficiency: While there are other methods to train neural networks, backpropagation is efficient because it calculates the gradient, which directly informs the network how to adjust its weights and biases to reduce the error.</li>
      <li>Universality: It can be applied to any differentiable loss function and network architecture.</li>
    </ul>
  </li>
  <li><strong>Challenges</strong>
    <ul>
      <li>Vanishing and Exploding Gradients: In very deep networks, gradients can become extremely small (vanish) or extremely large (explode) as they are propagated backward through the layers. This can slow down training or cause it to diverge.</li>
      <li>Local Minima: The optimization can sometimes get stuck in a local minimum (a point where all nearby values are higher, but it’s not the lowest possible value). Advanced optimization algorithms and techniques are introduced to address such issues.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>Backpropagation is like a feedback mechanism for neural networks. It determines how the model should adjust its internal weights and biases to better predict the output. It’s a powerful and central concept in deep learning.</li>
</ul>

<h2 id="single-hidden-layer-neural-networks">Single Hidden Layer Neural Networks</h2>

<ul>
  <li>We’ll start with focusing on the case of neural networks with what is called a single hidden layer.</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input Layer       Hidden Layer        Output Layer
                                  
    x1              •                     •
    x2              •                     •
    x3              •                     ŷ
</code></pre></div></div>

<ul>
  <li><strong>Input Layer:</strong> We have the input features, <code class="language-plaintext highlighter-rouge">x1</code>, <code class="language-plaintext highlighter-rouge">x2</code>, <code class="language-plaintext highlighter-rouge">x3</code> stacked up vertically. This is called the input layer of the neural network. As expected, this contains the inputs to the neural network.</li>
  <li><strong>Hidden Layer:</strong> Then there’s another layer of circles. This is called a hidden layer of the neural network. The term “hidden” refers to the fact that in the training set, the true values for these nodes in the middle are not observed. You see the inputs and the outputs, but the middle layers’ values aren’t seen in the training set. This explains the name “hidden layer”.</li>
  <li><strong>Output Layer:</strong> The final layer, in this case, is just one node. This single-node layer is called the output layer and is responsible for generating the predicted value <code class="language-plaintext highlighter-rouge">y hat</code>.</li>
</ul>

<h3 id="notations">Notations</h3>

<ul>
  <li>
    <p>The input features are also referred to as <code class="language-plaintext highlighter-rouge">A^0</code> (activations of the input layer). These values are passed on to the hidden layer.</p>
  </li>
  <li>
    <p>The hidden layer generates some set of activations, termed as <code class="language-plaintext highlighter-rouge">A^1</code>. If you have four nodes in the hidden layer, then you have an <code class="language-plaintext highlighter-rouge">A^1</code> vector with values <code class="language-plaintext highlighter-rouge">A^1_1, A^1_2,...</code>. This is a 4-dimensional vector.</p>
  </li>
  <li>
    <p>The output layer generates the value <code class="language-plaintext highlighter-rouge">A^2</code>, which is equivalent to <code class="language-plaintext highlighter-rouge">y hat</code>.</p>
  </li>
  <li>
    <p>Funny enough, this network is often called a two-layer neural network. When counting layers, the input layer is not counted, making the hidden layer layer one and the output layer layer two.</p>
  </li>
</ul>

<h3 id="parameters">Parameters</h3>
<ul>
  <li>Both hidden and output layers have associated parameters. The hidden layer will have <code class="language-plaintext highlighter-rouge">w^1</code> and <code class="language-plaintext highlighter-rouge">b^1</code> denoting parameters of layer one. Dimensions of these matrices and vectors will be explored in detail later. Similarly, the output layer has parameters <code class="language-plaintext highlighter-rouge">w^2</code> and <code class="language-plaintext highlighter-rouge">b^2</code>.</li>
</ul>

<h2 id="computing-nns-output">Computing NN’s output</h2>
<ul>
  <li>Here, we will provide a detailed explanation of how a single hidden layer neural network computes its outputs.</li>
  <li>The process is similar to logistic regression but repeated multiple times for each node in the hidden layer.</li>
</ul>

<ol>
  <li><strong>Basic Computation in a Node:</strong> For a single node in the hidden layer, the computation happens in two steps:
    <ul>
      <li>Calculate \(z = w^T\)x\(+ b\)</li>
      <li>Apply the sigmoid function to \(z\)  to get \(a\)  (activation)</li>
    </ul>

    <p>Here \(w\)  is the weight vector, \(\)x\(\)  is the input, and \(b\)  is the bias. Square brackets and subscripts denote layer and node index, respectively.</p>
  </li>
  <li>
    <p><strong>Vectorization for Efficiency:</strong> We can vectorize these operations for all nodes in the hidden layer at once. Instead of using a loop to calculate \(z\)  and \(a\)  for each node, the process can be vectorized.
 \(Z = Wx + b\) 
 \(A = \text{sigmoid}(Z)\)</p>

    <p>Here, \(W\)  is a matrix stacking all weight vectors, \(b\)  is a column vector of biases, and \(A\)  and \(Z\)  are vectors of activations and \(z\) -values, respectively.</p>
  </li>
  <li>
    <p><strong>Output Layer Computation:</strong> The output layer’s calculation is similar to logistic regression and follows the same pattern:
 \(z = w^T a + b\) 
 \(a = \text{sigmoid}(z)\)</p>
  </li>
  <li><strong>Takeaway:</strong> The output of a single hidden layer neural network can be computed with just four lines of code. This calculation can also be vectorized for multiple training examples for more efficiency.</li>
</ol>

<ul>
  <li>The overall message is that understanding the underlying computations allows for efficient implementation and helps in grasping how neural networks work.</li>
</ul>

<h2 id="activation-functions-in-neural-networks">Activation Functions in Neural Networks:</h2>

<ol>
  <li>
    <p><strong>Purpose:</strong> Activation functions introduce non-linearity to the model, enabling it to learn from the error and make adjustments, which is essential for learning complex patterns.</p>
  </li>
  <li><strong>Sigmoid Function:</strong>
    <ul>
      <li>Equation: \(a = \frac{1}{1 + e^{-z}}\)</li>
      <li>Value Range: [0, 1]</li>
      <li>Issues: Sigmoid functions result in vanishing gradient problems which can slow down the learning process.</li>
    </ul>
  </li>
  <li><strong>Hyperbolic Tangent (tanh) Function:</strong>
    <ul>
      <li>Equation: \(a = \frac{e^z - e^{-z}}{e^z + e^{-z}}\)</li>
      <li>Value Range: [-1, 1]</li>
      <li>Pros: Almost always works better than sigmoid for hidden units because values range between +1 and -1, making data mean closer to 0.</li>
    </ul>
  </li>
  <li><strong>Rectified Linear Unit (ReLU) Function:</strong>
    <ul>
      <li>Equation: \(a = max(0, z)\)</li>
      <li>Value Range: [0, ∞]</li>
      <li>Pros: Fast to compute and helps mitigate the vanishing gradient problem. Common default choice for many deep learning applications.</li>
      <li>Issues: For negative values of z, gradient is 0.</li>
    </ul>
  </li>
  <li><strong>Leaky ReLU Function:</strong>
    <ul>
      <li>Allows a small, non-zero gradient when z is less than 0, improving on the ReLU function.</li>
      <li>Value Range: \((-∞, ∞)\)</li>
      <li>Can be modified by adjusting the slope for z &lt; 0.</li>
    </ul>
  </li>
  <li><strong>General Rules:</strong>
    <ul>
      <li>For binary classification problems, the sigmoid function is useful in the output layer.</li>
      <li>For hidden layers, ReLU is often the default. Tanh can also be considered.</li>
      <li>Variants like Leaky ReLU can sometimes offer improvements.</li>
    </ul>
  </li>
  <li>
    <p><strong>Experimentation:</strong> Due to the diverse nature of problems and data, it’s advisable to experiment with different activation functions and decide based on validation set performance. It’s essential to remain adaptable and test various approaches to ascertain the best fit for a particular problem.</p>
  </li>
  <li><strong>Importance of Activation Functions:</strong> Without them, the neural network would behave like a linear regression model, failing to capture the complexities and non-linearities of the data.</li>
</ol>

<h2 id="vectorization-in-python">Vectorization in Python</h2>
<ul>
  <li>The objective of vectorization is to speed up the code significantly by processing an entire training set without using a single explicit for-loop.</li>
  <li>Here, we will discuss how to vectorize the implementation of logistic regression to significantly speed up the code.</li>
  <li>This approach can process an entire training set without using any explicit for-loops.</li>
  <li>Vectorization allows for efficient computation of all activations for all training examples simultaneously.</li>
  <li>In the deep learning realm, waiting for your code to execute can feel like an eternity. This brings us to a vital skill in today’s world: vectorization.</li>
  <li>Imagine a logistic regression problem where you have to calculate: \(Z = W^T\)x\(+ B\)
    <ul>
      <li>here, both \(W\)  and \(\)x\(\)  are \(n_x\)  dimensional vectors. If you have a plethora of features, these vectors can be quite substantial.</li>
    </ul>
  </li>
  <li>A non-vectorized method would require you to loop through every feature:</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Z = 0
For i = 1 to n_x:
   Z += W[i] * X[i]
Z += B
</code></pre></div></div>
<ul>
  <li>
    <p>This approach, as you might guess, is highly inefficient and slow.</p>
  </li>
  <li>
    <p>The vectorized alternative performs the calculation in a single step:
\(Z = np.dot(W, X) + B\)</p>
  </li>
</ul>

<h3 id="demo">Demo</h3>

<ul>
  <li>Using a Jupyter notebook, we can illustrate the time difference between vectorized and non-vectorized implementations. Here’s a brief walkthrough:</li>
</ul>

<ol>
  <li>Import necessary libraries:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">time</span>
</code></pre></div>    </div>
  </li>
  <li>Create two random arrays, <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>, each with one million elements.</li>
  <li>Time the vectorized dot product of <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Vectorized version: </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span><span class="si">}</span><span class="s">ms"</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>Time the non-vectorized dot product using an explicit for loop and compare:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
 <span class="n">c</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Non-vectorized version: </span><span class="si">{</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">)</span><span class="si">}</span><span class="s">ms"</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ol>

<ul>
  <li>The result? The vectorized version might take approximately 1.5 milliseconds, while the non-vectorized version takes around 500 milliseconds – that’s 300 times slower!</li>
  <li>Why does this matter?
    <ul>
      <li>In deep learning, especially when using algorithms at scale, a slight delay can amplify. A vectorized operation can be the difference between waiting for a minute or five hours.</li>
      <li>Many might know that scalable deep learning operations are executed on Graphics Processing Units (GPUs). However, the demonstration above was on a Central Processing Unit (CPU). Both GPU and CPU architectures can parallelize operations. They use what’s called SIMD (Single Instruction, Multiple Data) instructions. Utilizing functions like <code class="language-plaintext highlighter-rouge">np.dot</code>, instead of explicit for loops, allows Python to harness this parallelism, making computations significantly faster. While GPUs excel at SIMD calculations, CPUs aren’t too far behind.</li>
    </ul>
  </li>
  <li>Key takeaway
    <ul>
      <li>For efficient deep learning computations, vectorize wherever possible. Avoid explicit for loops to tap into the computational advantages of both CPUs and GPUs.</li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>Rule of Thumb: Whenever possible, avoid explicit for-loops.</p>
</blockquote>

<ul>
  <li>Though it might not always be feasible to completely eliminate for-loops, leveraging built-in functions or finding other efficient computational methods will usually result in a speed boost.</li>
</ul>

<h2 id="matrix-multiplication">Matrix Multiplication</h2>
<ul>
  <li>
    <p>If you’re computing a vector <code class="language-plaintext highlighter-rouge">u</code> as the product of a matrix <code class="language-plaintext highlighter-rouge">A</code> and another vector <code class="language-plaintext highlighter-rouge">v</code>, the matrix multiplication is defined as:
\(u_i = \sum_j A_{ij} v_j\)</p>
  </li>
  <li>
    <p>A non-vectorized approach would involve nested for-loops over both i and j indices. On the other hand, a vectorized approach, using Python’s NumPy library, can achieve this in one line:</p>
  </li>
</ul>

<p>\(u = np.dot(A,v)\)</p>
<ul>
  <li>This vectorized version not only simplifies the code but is also much faster.</li>
</ul>

<h2 id="element-wise-operations">Element-Wise Operations</h2>
<ul>
  <li>
    <p>Consider you have a vector <code class="language-plaintext highlighter-rouge">v</code> and want to compute the exponential of every element. A non-vectorized approach would require you to loop through every element to compute the result. However, with NumPy:
\(u = np.exp(v)\)</p>
  </li>
  <li>Here, <code class="language-plaintext highlighter-rouge">u</code> is a vector where each element is the exponential of the corresponding element in <code class="language-plaintext highlighter-rouge">v</code>. Similarly, NumPy offers a variety of vector-valued functions like:</li>
  <li><code class="language-plaintext highlighter-rouge">np.log(v)</code> for element-wise logarithm.</li>
  <li><code class="language-plaintext highlighter-rouge">np.abs(v)</code> for absolute values.</li>
  <li><code class="language-plaintext highlighter-rouge">np.maximum(v,0)</code> to compute the element-wise maximum of <code class="language-plaintext highlighter-rouge">v</code> and 0.</li>
  <li><code class="language-plaintext highlighter-rouge">v2</code> to square each element.</li>
  <li>
    <p>And many more.</p>
  </li>
  <li>In a gradient descent implementation for logistic regression, it’s common to come across multiple nested loops.
    <ul>
      <li>For instance, you might have loops iterating over features and training samples.</li>
    </ul>
  </li>
  <li>However, with clever use of vectorization, we can:</li>
  <li>Replace explicit initialization of variables like <code class="language-plaintext highlighter-rouge">dw1, dw2,...</code> with a vector <code class="language-plaintext highlighter-rouge">dw</code> of zeros.</li>
  <li>Replace nested loops with vector operations like <code class="language-plaintext highlighter-rouge">dw += xi * dz[i]</code>.</li>
  <li>
    <p>Simply use <code class="language-plaintext highlighter-rouge">dw /= m</code> instead of looping through individual components.</p>
  </li>
  <li>The result? We managed to reduce two for-loops down to one. And while this is a step in the right direction, we can push the boundaries even further.</li>
  <li>Vectorization offers a pathway to more efficient and faster computations. By eliminating or reducing the need for explicit loops, we can make our code more concise and computationally efficient.</li>
</ul>

<h2 id="forward-propagation">Forward Propagation</h2>
<p>In standard logistic regression, you may need explicit for-loops to iterate over <code class="language-plaintext highlighter-rouge">M</code> training examples to compute the predictions (<code class="language-plaintext highlighter-rouge">Z</code> values and activations <code class="language-plaintext highlighter-rouge">A</code>). However, vectorization can eliminate these for-loops.</p>

<h3 id="the-z-matrix">The Z Matrix</h3>
<ul>
  <li>Define a matrix <code class="language-plaintext highlighter-rouge">X</code> that stacks all your training inputs in columns, making it an <code class="language-plaintext highlighter-rouge">Nx</code> by <code class="language-plaintext highlighter-rouge">M</code> matrix.</li>
  <li>You can compute all Z-values (from Z1 to ZM) in one line:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">B</span>
</code></pre></div>    </div>
  </li>
  <li>Here <code class="language-plaintext highlighter-rouge">B</code> is broadcasted to match the dimensions, a feature in Python called “broadcasting.”</li>
</ul>

<h3 id="the-a-matrix">The A Matrix</h3>
<ul>
  <li>Similarly, stack all the lower-case <code class="language-plaintext highlighter-rouge">a</code> values (activations) to form a capital <code class="language-plaintext highlighter-rouge">A</code> matrix.</li>
  <li>You can calculate all activations with one efficient implementation of the sigmoid function:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="broadcasting-in-python">Broadcasting in Python</h2>
<ul>
  <li>
    <p>Broadcasting is a powerful feature in Python, especially within libraries like NumPy, which allows you to perform arithmetic operations on arrays of different shapes. This is essential when dealing with matrices and vectors in various mathematical and scientific computations. Instead of using loops to carry out operations between arrays of different shapes, broadcasting expands one or both arrays so they have the same shape, and then performs element-wise operations.</p>
  </li>
  <li>
    <p>Here’s how broadcasting works:</p>

    <ol>
      <li>
        <p><strong>Element-wise Operations:</strong> If two arrays are of exactly the same shape, then Python operations occur element-wise. This is the simplest broadcasting scenario.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>  <span class="c1"># array([2, 4, 6])
</span></code></pre></div>        </div>
      </li>
      <li>
        <p><strong>Broadcasting Rule:</strong></p>

        <ul>
          <li>If arrays have different shapes, Python first checks if they’re compatible for broadcasting. For two dimensions to be compatible:
            <ul>
              <li>They are equal, or</li>
              <li>One of them is 1</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p><strong>Broadcasting in Action:</strong></p>

        <ul>
          <li>Let’s consider a small example where we want to multiply a matrix by a scalar (a single number). Instead of multiplying each element of the matrix individually by the scalar, broadcasting will automatically apply the scalar to each element of the matrix.</li>
        </ul>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">*</span> <span class="n">scalar</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># Output:
# [[ 2  4]
#  [ 6  8]
#  [10 12]]
</span></code></pre></div>        </div>

        <ul>
          <li>Here, the scalar (2) is ‘broadcast’ to each element of the matrix.</li>
        </ul>
      </li>
      <li>
        <p><strong>Another Example:</strong></p>

        <ul>
          <li>If you have a matrix (3x3) and want to add a 1D array (1x3) to each row of the matrix, broadcasting can handle this:</li>
        </ul>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">+</span> <span class="n">array</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="c1"># Output:
# [[ 2  2  4]
#  [ 5  5  7]
#  [ 8  8 10]]
</span></code></pre></div>        </div>

        <ul>
          <li>The 1D array has been added to each row of the matrix.</li>
        </ul>
      </li>
      <li>
        <p><strong>Note on Shapes:</strong></p>

        <ul>
          <li>When using broadcasting, always be cautious of the shapes of your arrays. If Python cannot broadcast the shapes to be the same, it will raise a <code class="language-plaintext highlighter-rouge">ValueError</code>.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p>Broadcasting is a technique in Python to optimize and simplify operations on arrays.</p>
  </li>
  <li>Broadcasting Example:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="mi">56</span><span class="p">,</span> <span class="mi">104</span><span class="p">,</span> <span class="p">...],</span>    <span class="c1"># carbs, proteins, fats for food item 1 (e.g., apple)
</span>    <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mi">135</span><span class="p">,</span> <span class="p">...],</span>   <span class="c1"># carbs, proteins, fats for food item 2 (e.g., beef)
</span>    <span class="c1"># ... continue for other food items
</span><span class="p">]</span>
</code></pre></div>    </div>
    <ul>
      <li>The matrix allows for understanding caloric distributions in foods like apples and beef.</li>
      <li>The goal is to calculate the percentage of calories from carbs, proteins, and fats for each of the four foods without using explicit for-loops.</li>
    </ul>
  </li>
  <li><strong>Python Implementation:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="n">matrix</span>
<span class="n">cal</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>   <span class="c1"># Sum columns
</span><span class="n">percentages</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span> <span class="o">/</span> <span class="n">cal</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span>   <span class="c1"># Calculate percentages
</span></code></pre></div>    </div>
  </li>
  <li><strong>Python Broadcasting Explained:</strong>
    <ul>
      <li>Broadcasting allows operations on matrices of different sizes. It auto-expands matrices to make their shapes compatible for element-wise operations.</li>
      <li>
        <p>Examples:</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">+</span> <span class="mi">100</span>  <span class="c1"># Each element of the vector gets increased by 100
</span></code></pre></div>        </div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix_mn</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">matrix_1n</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">]</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">matrix_mn</span> <span class="o">+</span> <span class="n">matrix_1n</span>
</code></pre></div>        </div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix_m1</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">100</span><span class="p">],</span> <span class="p">[</span><span class="mi">200</span><span class="p">]]</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">matrix_mn</span> <span class="o">+</span> <span class="n">matrix_m1</span>
</code></pre></div>        </div>
      </li>
      <li><strong>General principles:</strong>
        <ul>
          <li>(m,n) matrix combined with (1,n) matrix: latter is copied m times.</li>
          <li>(m,n) matrix combined with (m,1) matrix: latter is copied n times.</li>
          <li>These principles apply for addition, subtraction, multiplication, and division operations.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Advanced Broadcasting:</strong>
    <ul>
      <li>For more details, check the NumPy documentation on broadcasting.</li>
    </ul>
  </li>
  <li><strong>Note for MATLAB/Octave Users:</strong>
    <ul>
      <li>In MATLAB or Octave, the <code class="language-plaintext highlighter-rouge">bsxfun</code> function performs a role similar to broadcasting in Python.</li>
    </ul>
  </li>
  <li>In summary, broadcasting automates operations over arrays of different shapes, making the code cleaner, more efficient, and more readable. However, care should be taken to ensure that the shapes are compatible according to broadcasting rules.</li>
</ul>

<h2 id="weight-initialization-in-neural-networks">Weight Initialization in Neural Networks</h2>

<ol>
  <li>
    <p><strong>Symmetry Issue:</strong> When all weights are initialized to zero, each neuron in the hidden layer will produce the same output. This is because they’re all calculating the same function. So essentially, you have multiple neurons doing the exact same thing, which defeats the purpose of having multiple neurons in the first place.</p>
  </li>
  <li>
    <p><strong>Vanishing Gradients:</strong> Initializing weights to zero makes it likely that neurons will get activated in a way that they are in the flat regions of the activation function (like sigmoid or tanh). This means the gradient will be small, and thus, the weight updates will be very small, slowing down the learning process.</p>
  </li>
</ol>

<h3 id="how-random-initialization-helps">How Random Initialization Helps</h3>

<ol>
  <li>
    <p><strong>Breaking Symmetry:</strong> Random initialization of weights ensures that each neuron computes a different function, breaking the symmetry. This allows the network to learn from the error and make individual updates to each neuron.</p>
  </li>
  <li>
    <p><strong>Accelerates Learning:</strong> Initializing weights to small random values ensures that the activation functions operate in ranges where their gradients are not extremely small, which helps speed up the learning process.</p>
  </li>
</ol>

<h3 id="practical-tips-for-weight-initialization">Practical Tips for Weight Initialization</h3>

<ol>
  <li>
    <p><strong>Small Random Values:</strong> Initializing weights with small random numbers can be effective, especially when using activation functions like tanh or sigmoid. The random values are often multiplied by a small constant like 0.01 to make sure they are not too large, avoiding the flat regions of the activation function.</p>
  </li>
  <li>
    <p><strong>Bias Terms:</strong> Initializing bias terms to zero is generally considered to be fine because the random initialization of weights is sufficient to break the symmetry.</p>
  </li>
  <li>
    <p><strong>Advanced Techniques:</strong> For deep networks, more advanced initialization techniques might be beneficial, but starting with small random values is generally a good enough approach for shallow networks.</p>
  </li>
</ol>

<ul>
  <li>Proper weight initialization is crucial for training neural networks effectively. Randomly initializing weights helps break symmetry between neurons and can accelerate the learning process.</li>
  <li>Appropriate weight initialization is crucial for efficient training of neural networks. Initializing all weights to zero leads to symmetry problems, rendering multiple neurons redundant. Instead, small random weight initialization is preferred to ensure different neurons learn different features and to prevent slow learning caused by saturated activation functions.</li>
</ul>

<h2 id="forward-propogation">Forward Propogation</h2>
<ul>
  <li>Forward propagation, commonly referred to as “forward prop”, is a fundamental concept in neural networks, both shallow and deep. It’s the process by which input data (like an image, audio clip, or text) is passed through the network to produce an output. This output can be a prediction, classification, or any other type of result that the network is designed to produce.</li>
</ul>

<ol>
  <li>
    <p><strong>Input Layer:</strong> Start by inputting data into the network. This data serves as the initial activations for the first layer of the network.</p>
  </li>
  <li>
    <p><strong>Linear Combination:</strong> For each neuron in the first hidden layer (or subsequent layers), calculate a weighted sum of the inputs (or activations from the previous layer) using the weights associated with the connections. Also add a bias term. The formula for the linear combination for a given layer \(l\) is:
\(z^{[l]} = W^{[l]}a^{[l-1]} + b^{[l]}\)
Where:
\(z^{[l]}\) is the linear combination for layer \(l\).
\(W^{[l]}\) are the weights for layer \(l\).
\(a^{[l-1]}\) are the activations from the previous layer.
\(b^{[l]}\) is the bias for layer \(l\).</p>
  </li>
  <li><strong>Activation Function:</strong> Pass the result from the linear combination through an activation function (like sigmoid, tanh, ReLU, etc.). This produces the activations</li>
  <li><strong>Forward Propagation for a Single Training Example:</strong>
    <ul>
      <li>The activations for any layer are determined using the weights and biases of that layer and the activations from the previous layer.</li>
      <li>The general formula is:
 \(z^{[l]} = w^{[l]} \times a^{[l-1]} + b^{[l]}\)
 \(a^{[l]} = g(z^{[l]})\)
Here, \(g\) is the activation function, which could be sigmoid, tanh, ReLU, etc.</li>
      <li>The input feature vector \(\)x\(\) is also considered as the activations of layer zero (\(a^{[0]}\)).</li>
    </ul>
  </li>
  <li><strong>Vectorized Version for Entire Training Set:</strong>
    <ul>
      <li>In the vectorized version, we use uppercase Z and A to represent matrices that stack individual z and a vectors column-wise for every training example.</li>
      <li>The formulas remain quite similar to the single example version, with the distinction that now they process multiple examples at once.</li>
      <li>At the end of the forward propagation, we have \(\hat{y}\), which is the neural network’s output prediction for all training examples.</li>
    </ul>
  </li>
  <li><strong>Necessity of a For Loop:</strong>
    <ul>
      <li>Despite the general practice of eliminating explicit For loops for efficiency, in this context of forward propagation through multiple layers, using a For loop from 1 through \(L\) (the number of layers) is inevitable and perfectly acceptable.</li>
    </ul>
  </li>
  <li><strong>Key Insight:</strong>
    <ul>
      <li>Implementing forward propagation in deep networks can be conceptualized as repeatedly applying the process used in a single hidden layer network across multiple layers.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>In a nutshell, forward propagation in deep networks consists of computing activations for each layer sequentially, starting from the input features and moving up to the final output. While it may seem complex, the process can be broken down into a set of repetitive steps, each associated with a single layer. The vectorized approach enhances computational efficiency by processing all training examples simultaneously.</li>
</ul>

<h2 id="why-deep-neural-networks-work-well">Why Deep Neural Networks Work Well</h2>

<ol>
  <li><strong>Hierarchical Feature Learning:</strong>
    <ul>
      <li>Deep networks can build a hierarchy of features from simple to complex.</li>
      <li>For image recognition:
        <ul>
          <li>First layer: Detects edges.</li>
          <li>Middle layers: Compose edges to detect facial parts like eyes and noses.</li>
          <li>Later layers: Combine facial parts to recognize entire faces.</li>
        </ul>
      </li>
      <li>For speech recognition:
        <ul>
          <li>First layer: Detects simple waveform features (e.g., tone direction).</li>
          <li>Middle layers: Compose basic sound units (phonemes).</li>
          <li>Later layers: Recognize words or even phrases.</li>
        </ul>
      </li>
      <li>This method resembles how we believe the human brain processes information, starting from basic features and building up to more complex ones.</li>
    </ul>
  </li>
  <li><strong>Circuit Theory Insight:</strong>
    <ul>
      <li>Some functions can be computed with fewer resources using deep networks rather than shallow ones.</li>
      <li>Example: Computing the Exclusive OR (XOR) for a set of input features.
        <ul>
          <li>With depth (multiple hidden layers), you can compute XOR using a logarithmic number of units.</li>
          <li>In a shallow network (single layer), the size of the layer needs to be exponentially large to compute the XOR.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Branding:</strong>
    <ul>
      <li>The term “deep learning” has become a popular branding term. Its evocative nature helped capture public interest, but aside from branding, deep networks have demonstrated superior performance in many applications.</li>
    </ul>
  </li>
  <li><strong>Trend:</strong>
    <ul>
      <li>While deep networks perform well, it’s essential to consider the problem’s needs. Starting with simpler models like logistic regression or networks with a few hidden layers might be effective. Over time, for some problems, extremely deep networks (with dozens of layers) have proven to be the best approach.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>In practice, it’s advisable to consider the depth of the network as a hyperparameter and adjust based on the specific problem and available data.</li>
</ul>

<h2 id="hyperparameters-in-deep-learning">Hyperparameters in Deep Learning</h2>

<ul>
  <li>Deep learning requires careful tuning and organization of both parameters (W and B) and hyperparameters. While parameters are the main aspects of the model that get optimized during training, hyperparameters act as controllers for those parameters. Here’s a breakdown:</li>
</ul>

<ol>
  <li><strong>Parameters:</strong>
    <ul>
      <li>W and B are the main trainable parameters of a neural network.</li>
    </ul>
  </li>
  <li><strong>Hyperparameters:</strong>
    <ul>
      <li>Learning Rate (α): Determines the step size during optimization. Too large, and you might overshoot the optimal solution. Too small, and it might take forever to converge or get stuck in a local minimum.</li>
      <li>Number of Iterations: How many times the optimization algorithm runs.</li>
      <li>Number of Hidden Layers (L): Affects the network’s complexity.</li>
      <li>Number of Hidden Units: Controls the size of the network.</li>
      <li>Activation Functions: ReLU, Tanh, Sigmoid, etc. They add non-linearity to the model, enabling it to learn from errors.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>
    <p>All these hyperparameters indirectly control the values of parameters (W and B). Therefore, selecting appropriate hyperparameters is crucial for the efficiency and accuracy of the model.</p>
  </li>
  <li><strong>Challenges and Tips:</strong>
    <ul>
      <li>
        <p><strong>Exploration:</strong> Deep learning today is used in a plethora of applications. Transferring knowledge from one domain to another can be tricky, as the best practices for one might not work for another. It’s often best to experiment and see what works best for your specific application.</p>
      </li>
      <li>
        <p><strong>Empirical Process:</strong> Deep learning is largely empirical. You often have to experiment with multiple settings and observe the results. Based on feedback, you adjust and iterate.</p>
      </li>
      <li>
        <p><strong>Evolving Landscape:</strong> Even if you find the best hyperparameters for a task today, they might change in the future. This can be due to advancements in technology, changes in data distribution, or various other factors.</p>
      </li>
      <li>
        <p><strong>Gaining Intuition:</strong> Over time, by experimenting and iterating, one gains an intuition about what hyperparameters might work best for a given problem.</p>
      </li>
      <li>
        <p><strong>Ongoing Research:</strong> The deep learning field is still evolving. Over time, there might be clearer guidelines on hyperparameter selection, but for now, it remains a mix of art and science.</p>
      </li>
    </ul>
  </li>
  <li>To sum up, understanding and selecting hyperparameters is an essential aspect of deep learning. It requires a lot of experimentation, patience, and continuous learning. In the end, the goal is to develop a neural network model that performs well on your specific task, and this often requires fine-tuning these hyperparameters based on feedback from your model’s performance.</li>
</ul>

<h3 id="parameters-1">Parameters</h3>

<ul>
  <li>
    <p>Parameters are the intrinsic parts of the model that are learned from the data during training. They directly influence the prediction of the model. The optimization algorithm (like gradient descent) modifies them to minimize the error.</p>
  </li>
  <li>
    <p>Examples:</p>

    <ol>
      <li>
        <p><strong>Weights and Biases in Neural Networks:</strong> In a neural network, the weights (often denoted as (W)) and biases (often denoted as (b)) are the primary parameters. They get updated during the training process to minimize the loss function.</p>
      </li>
      <li>
        <p><strong>Coefficients in Linear Regression:</strong> In a simple linear regression model (y = mx + c), the slope ((m)) and the intercept ((c)) are the parameters. The learning algorithm tries to find the best values for (m) and (c) to fit the data.</p>
      </li>
      <li>
        <p><strong>Support Vectors in SVM:</strong> In a support vector machine, the support vectors are parameters. They are the data points that are closest to the decision boundary.</p>
      </li>
    </ol>
  </li>
  <li>
    <p>In essence, while parameters are learned from the data and get updated automatically, hyperparameters are set manually and govern the overall training process.</p>
  </li>
</ul>

<h2 id="features-vs-parameters-vs-hyperparameters">Features vs. Parameters vs. Hyperparameters</h2>

<ol>
  <li><strong>Features:</strong> These are the input variables used by models to make predictions. In the house price prediction example, features would be things like the number of bedrooms, square footage, age of the house, etc. These are not parameters; they are the data that is input into the model.</li>
  <li><strong>Parameters:</strong> These are the internal variables that the model adjusts during training. For neural networks, parameters typically refer to the weights and biases in the network. They are learned from the training data and determine how the model makes predictions.</li>
  <li><strong>Hyperparameters:</strong> These are the settings or configurations that are set before training the model. They are not learned from the data. Examples include the learning rate, batch size, number of layers in a neural network, number of neurons in each layer, etc.</li>
</ol>

<ul>
  <li><strong>Summary:</strong>
    <ul>
      <li>Features are what you input into the model to get a prediction.</li>
      <li>Parameters (like weights and biases in a neural network) are what the model learns during training.</li>
      <li>Hyperparameters are settings you configure before training, determining how the training process itself operates.</li>
      <li>The distinction is important, as confusing them can lead to misunderstandings about how models work and how they are trained.</li>
    </ul>
  </li>
</ul>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/vinija">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                        viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">vinija</span> -->
               
<!--               <a href="">-->
<!--                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"-->
<!--                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">-->
<!--                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN-->
<!--                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx-->
<!--                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa-->
<!--                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/-->
<!--                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ-->
<!--                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o-->
<!--                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT-->
<!--                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL-->
<!--                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ-->
<!--                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ-->
<!--                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu-->
<!--                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0-->
<!--                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3-->
<!--                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ-->
<!--                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47-->
<!--                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32-->
<!--                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns-->
<!--                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2-->
<!--                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66-->
<!--                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M-->
<!--                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI-->
<!--                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j-->
<!--                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP-->
<!--                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+-->
<!--                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah-->
<!--                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B-->
<!--                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k-->
<!--                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X-->
<!--                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq-->
<!--                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX-->
<!--                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO-->
<!--                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu-->
<!--                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv-->
<!--                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9-->
<!--                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX-->
<!--                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO-->
<!--                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L-->
<!--                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm-->
<!--                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx-->
<!--                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb-->
<!--                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j-->
<!--                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV-->
<!--                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei-->
<!--                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd-->
<!--                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL-->
<!--                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy-->
<!--                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX-->
<!--                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23-->
<!--                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH-->
<!--                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV-->
<!--                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX-->
<!--                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K-->
<!--                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9-->
<!--                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg-->
<!--                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8-->
<!--                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3-->
<!--                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i-->
<!--                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ-->
<!--                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo-->
<!--                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ-->
<!--                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y-->
<!--                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr-->
<!--                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD-->
<!--                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND-->
<!--                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa-->
<!--                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K-->
<!--                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG-->
<!--                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU-->
<!--                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY-->
<!--                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW-->
<!--                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP-->
<!--                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq-->
<!--                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg-->
<!--                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF-->
<!--                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW-->
<!--                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w-->
<!--                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd-->
<!--                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30-->
<!--                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q-->
<!--                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve-->
<!--                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g-->
<!--                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch-->
<!--                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG-->
<!--                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs-->
<!--                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB-->
<!--                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP-->
<!--                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im-->
<!--                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t-->
<!--                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ-->
<!--                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ-->
<!--                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5-->
<!--                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa-->
<!--                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp-->
<!--                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV-->
<!--                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11-->
<!--                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb-->
<!--                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R-->
<!--                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S-->
<!--                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY-->
<!--                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63-->
<!--                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ-->
<!--                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT-->
<!--                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2-->
<!--                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL-->
<!--                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ-->
<!--                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg-->
<!--                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI-->
<!--                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ-->
<!--                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem-->
<!--                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW-->
<!--                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje-->
<!--                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa-->
<!--                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd-->
<!--                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V-->
<!--                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA-->
<!--                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo-->
<!--                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP-->
<!--                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt-->
<!--                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y-->
<!--                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2-->
<!--                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX-->
<!--                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB-->
<!--                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt-->
<!--                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR-->
<!--                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ-->
<!--                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1-->
<!--                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H-->
<!--                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB-->
<!--                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC-->
<!--                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h-->
<!--                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO-->
<!--                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9-->
<!--                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD-->
<!--                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf-->
<!--                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp-->
<!--                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD-->
<!--                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8-->
<!--                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H-->
<!--                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h-->
<!--                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU-->
<!--                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba-->
<!--                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT-->
<!--                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr-->
<!--                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0-->
<!--                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb-->
<!--                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi-->
<!--                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy-->
<!--                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77-->
<!--                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy-->
<!--                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk-->
<!--                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe-->
<!--                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO-->
<!--                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ-->
<!--                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9-->
<!--                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR-->
<!--                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ-->
<!--                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS-->
<!--                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc-->
<!--                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS-->
<!--                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht-->
<!--                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0-->
<!--                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv-->
<!--                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+-->
<!--                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX-->
<!--                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f-->
<!--                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv-->
<!--                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew-->
<!--                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f-->
<!--                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib-->
<!--                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w-->
<!--                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3-->
<!--                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn-->
<!--                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7-->
<!--                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk-->
<!--                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a-->
<!--                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf-->
<!--                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2-->
<!--                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC-->
<!--                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN-->
<!--                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW-->
<!--                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP-->
<!--                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb-->
<!--                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+-->
<!--                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K-->
<!--                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/-->
<!--                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT-->
<!--                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ-->
<!--                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU-->
<!--                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf-->
<!--                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i-->
<!--                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX-->
<!--                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho-->
<!--                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ-->
<!--                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa-->
<!--                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p-->
<!--                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH-->
<!--                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+-->
<!--                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA-->
<!--                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T-->
<!--                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm-->
<!--                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb-->
<!--                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr-->
<!--                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2-->
<!--                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB-->
<!--                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp-->
<!--                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T-->
<!--                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj-->
<!--                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX-->
<!--                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek-->
<!--                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD-->
<!--                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ-->
<!--                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x-->
<!--                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz-->
<!--                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v-->
<!--                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N-->
<!--                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju-->
<!--                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6-->
<!--                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T-->
<!--                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE-->
<!--                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+-->
<!--                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep-->
<!--                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ-->
<!--                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc-->
<!--                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX-->
<!--                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/-->
<!--                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv-->
<!--                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z-->
<!--                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg-->
<!--                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9-->
<!--                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L-->
<!--                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af-->
<!--                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF-->
<!--                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv-->
<!--                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ-->
<!--                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0-->
<!--                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx-->
<!--                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1-->
<!--                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx-->
<!--                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm-->
<!--                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC-->
<!--                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy-->
<!--                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY-->
<!--                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC" />-->
<!--                  </svg>-->
<!--               </a>-->

               <a href="mailto:vinija@gmail.com">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>

               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="../../index.html">www.vinija.ai</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="../../js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="../../js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="../../js/mode-switcher.js"></script>
    <!-- mathjax -->
    <script type="text/javascript" src="../../../cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJaxdda6.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="../../js/clipboard.min.js"></script>
    <script src="../../js/copy.js"></script>      
    </body>

<!-- Mirrored from vinija.ai/CourseraDL/neural-networks-and-deep-learning/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 12 Jul 2025 14:02:12 GMT -->
</html>
