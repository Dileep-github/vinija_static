<!DOCTYPE html>
<html lang="en">

  
<!-- Mirrored from vinija.ai/models/diffusion-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 12 Jul 2025 14:01:23 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Vinija's Notes • Primers • Diffusion Models</title>
  <meta name="viewport" content="width=device-width">
  <meta name="description" content="Vinija's detailed AI Notes">
  <link rel="canonical" href="index.html">

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../css/main.css">

  <!-- Google fonts -->
  <!-- <link href='https://fonts.googleapis.com/css?family=Roboto:400,300' rel='stylesheet' type='text/css'>-->

  <!-- RSS feed -->
  <link rel="alternate" type="application/atom+xml" title="Vinija's AI Notes" href="../../feed.xml">
  
  <link href="../../favicon.html" rel="shortcut icon" />

  <!-- Google ads -->
  <script async src="../../../pagead2.googlesyndication.com/pagead/js/ff0a8.txt?client=ca-pub-5905744527956213"
     crossorigin="anonymous"></script>
</head>



    <body>

      <script src="../../../unpkg.com/vanilla-back-to-top%407.2.1/dist/vanilla-back-to-top.min.js"></script>
      <script>addBackToTop({
        backgroundColor: '#fff',
        innerHTML: 'Back to Top',
        textColor: '#333'
      })</script>
      <style>
        #back-to-top {
          border: 1px solid #ccc;
          border-radius: 0;
          font-family: sans-serif;
          font-size: 14px;
          width: 100px;
          text-align: center;
          line-height: 30px;
          height: 30px;
        }
      </style>   

    <header class="site-header">

  <a class="site-title" href="../index-2.html">Vinija's AI Notes</a>

  <a class="site-link" href="../../index.html">Back to vinija.ai</a>

  <!-- Html Elements for Search -->
  <div id="search-container">
  <input class="site-search-box" type="text" autocomplete="off" id="search-input" placeholder="search...">
  <div id="results-container"></div>
  </div>

  <!-- Script pointing to aman-script.js -->
  <script src="../../js/aman-search.min.js" type="text/javascript"></script>

  <!-- Configuration -->
  <script>
  document.getElementById('search-input').value='';
  SimpleJekyllSearch({
    searchInput: document.getElementById('search-input'),
    resultsContainer: document.getElementById('results-container'),
    exclude: ["cs231a"],
    searchResultTemplate: '<div class="site-search-results"><a href="{url}">{title}</a></div>',
    noResultsText: '<div class="site-search-results"><p>No results found</p></div>',
    json: 'https://vinija.ai/search.json',
    limit: 5,
    fuzzy: false,
  })
  </script>    

</header>     

    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>Primers • Diffusion Models</h1>
  </header>

  <article class="post-content">
  <ul id="markdown-toc">
  <li><a href="#background" id="markdown-toc-background">Background</a></li>
  <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
  <li><a href="#advantages" id="markdown-toc-advantages">Advantages</a></li>
  <li><a href="#definitions" id="markdown-toc-definitions">Definitions</a>    <ul>
      <li><a href="#diffusion-models" id="markdown-toc-diffusion-models">Diffusion Models</a></li>
      <li><a href="#schedulers" id="markdown-toc-schedulers">Schedulers</a></li>
      <li><a href="#sampling-and-training-algorithms" id="markdown-toc-sampling-and-training-algorithms">Sampling and training algorithms</a></li>
    </ul>
  </li>
  <li><a href="#diffusion-models-the-theory" id="markdown-toc-diffusion-models-the-theory">Diffusion Models: The Theory</a></li>
  <li><a href="#diffusion-models-a-deep-dive" id="markdown-toc-diffusion-models-a-deep-dive">Diffusion models: A Deep Dive</a>    <ul>
      <li><a href="#general-overview" id="markdown-toc-general-overview">General Overview</a></li>
      <li><a href="#the-math-under-the-hood" id="markdown-toc-the-math-under-the-hood">The Math Under-the-hood</a></li>
    </ul>
  </li>
  <li><a href="#types-of-diffusion-models" id="markdown-toc-types-of-diffusion-models">Types of Diffusion Models</a>    <ul>
      <li><a href="#denoising-diffusion-probabilistic-models-ddpms--discrete-time-diffusion-models" id="markdown-toc-denoising-diffusion-probabilistic-models-ddpms--discrete-time-diffusion-models">Denoising Diffusion Probabilistic Models (DDPMs) / Discrete-Time Diffusion Models</a>        <ul>
          <li><a href="#implementation-details" id="markdown-toc-implementation-details">Implementation Details</a></li>
          <li><a href="#pros" id="markdown-toc-pros">Pros</a></li>
          <li><a href="#cons" id="markdown-toc-cons">Cons</a></li>
        </ul>
      </li>
      <li><a href="#denoising-diffusion-implicit-models-ddims" id="markdown-toc-denoising-diffusion-implicit-models-ddims">Denoising Diffusion Implicit Models (DDIMs)</a>        <ul>
          <li><a href="#implementation-details-1" id="markdown-toc-implementation-details-1">Implementation Details</a></li>
          <li><a href="#pros-1" id="markdown-toc-pros-1">Pros</a></li>
          <li><a href="#cons-1" id="markdown-toc-cons-1">Cons</a></li>
        </ul>
      </li>
      <li><a href="#score-based-generative-models-sgms--continuous-time-diffusion-models" id="markdown-toc-score-based-generative-models-sgms--continuous-time-diffusion-models">Score-Based Generative Models (SGMs) / Continuous-Time Diffusion Models</a>        <ul>
          <li><a href="#implementation-details-2" id="markdown-toc-implementation-details-2">Implementation Details</a></li>
          <li><a href="#pros-2" id="markdown-toc-pros-2">Pros</a></li>
          <li><a href="#cons-2" id="markdown-toc-cons-2">Cons</a></li>
        </ul>
      </li>
      <li><a href="#variational-diffusion-models-vdms" id="markdown-toc-variational-diffusion-models-vdms">Variational Diffusion Models (VDMs)</a>        <ul>
          <li><a href="#implementation-details-3" id="markdown-toc-implementation-details-3">Implementation Details</a></li>
          <li><a href="#pros-3" id="markdown-toc-pros-3">Pros</a></li>
          <li><a href="#cons-3" id="markdown-toc-cons-3">Cons</a></li>
        </ul>
      </li>
      <li><a href="#stochastic-differential-equation-sde-based-models" id="markdown-toc-stochastic-differential-equation-sde-based-models">Stochastic Differential Equation (SDE)-Based Models</a>        <ul>
          <li><a href="#implementation-details-4" id="markdown-toc-implementation-details-4">Implementation Details</a></li>
          <li><a href="#pros-4" id="markdown-toc-pros-4">Pros</a></li>
          <li><a href="#cons-4" id="markdown-toc-cons-4">Cons</a></li>
        </ul>
      </li>
      <li><a href="#comparative-analysis" id="markdown-toc-comparative-analysis">Comparative Analysis</a></li>
    </ul>
  </li>
  <li><a href="#training" id="markdown-toc-training">Training</a>    <ul>
      <li><a href="#recap-kl-divergence" id="markdown-toc-recap-kl-divergence">Recap: KL Divergence</a></li>
      <li><a href="#casting-l_v-l-b-in-terms-of-kl-divergences" id="markdown-toc-casting-l_v-l-b-in-terms-of-kl-divergences">Casting \(L_{v l b}\) in Terms of KL Divergences</a></li>
    </ul>
  </li>
  <li><a href="#model-choices" id="markdown-toc-model-choices">Model Choices</a>    <ul>
      <li><a href="#forward-process-and-l_t" id="markdown-toc-forward-process-and-l_t">Forward Process and \(L_{T}\)</a></li>
      <li><a href="#reverse-process-and-l_1-t-1" id="markdown-toc-reverse-process-and-l_1-t-1">Reverse Process and \(L_{1: T-1}\)</a></li>
    </ul>
  </li>
  <li><a href="#network-architecture-u-net-and-diffusion-transformer-dit" id="markdown-toc-network-architecture-u-net-and-diffusion-transformer-dit">Network Architecture: U-Net and Diffusion Transformer (DiT)</a>    <ul>
      <li><a href="#u-net-based-diffusion-models" id="markdown-toc-u-net-based-diffusion-models">U-Net-Based Diffusion Models</a></li>
      <li><a href="#diffusion-transformer-dit" id="markdown-toc-diffusion-transformer-dit">Diffusion Transformer (DiT)</a></li>
      <li><a href="#comparison" id="markdown-toc-comparison">Comparison</a>        <ul>
          <li><a href="#model-complexity-and-parameters" id="markdown-toc-model-complexity-and-parameters">Model Complexity and Parameters</a></li>
          <li><a href="#training-and-optimization" id="markdown-toc-training-and-optimization">Training and Optimization</a></li>
        </ul>
      </li>
      <li><a href="#reverse-process-of-u-net-based-diffusion-models" id="markdown-toc-reverse-process-of-u-net-based-diffusion-models">Reverse Process of U-Net-based Diffusion Models</a></li>
      <li><a href="#reverse-process-of-dit-based-diffusion-models" id="markdown-toc-reverse-process-of-dit-based-diffusion-models">Reverse Process of DiT-based Diffusion Models</a></li>
      <li><a href="#final-objective" id="markdown-toc-final-objective">Final Objective</a></li>
      <li><a href="#summary" id="markdown-toc-summary">Summary</a></li>
    </ul>
  </li>
  <li><a href="#conditional-diffusion-models" id="markdown-toc-conditional-diffusion-models">Conditional Diffusion Models</a>    <ul>
      <li><a href="#conditioning-mechanisms" id="markdown-toc-conditioning-mechanisms">Conditioning Mechanisms</a></li>
      <li><a href="#text-conditioning-in-diffusion-models" id="markdown-toc-text-conditioning-in-diffusion-models">Text Conditioning in Diffusion Models</a>        <ul>
          <li><a href="#encoding-textual-information" id="markdown-toc-encoding-textual-information">Encoding Textual Information</a></li>
          <li><a href="#concatenation-vs-cross-attention-conditioning" id="markdown-toc-concatenation-vs-cross-attention-conditioning">Concatenation vs. Cross-Attention Conditioning</a>            <ul>
              <li><a href="#cross-attention" id="markdown-toc-cross-attention">Cross-Attention</a></li>
            </ul>
          </li>
          <li><a href="#implementation-details-pytorch" id="markdown-toc-implementation-details-pytorch">Implementation Details (PyTorch)</a></li>
        </ul>
      </li>
      <li><a href="#visual-conditioning-in-diffusion-models" id="markdown-toc-visual-conditioning-in-diffusion-models">Visual Conditioning in Diffusion Models</a>        <ul>
          <li><a href="#concatenation-based-conditioning" id="markdown-toc-concatenation-based-conditioning">Concatenation-Based Conditioning</a></li>
          <li><a href="#feature-map-injection-via-cross-attention" id="markdown-toc-feature-map-injection-via-cross-attention">Feature Map Injection via Cross-Attention</a></li>
          <li><a href="#implementation-details-pytorch-1" id="markdown-toc-implementation-details-pytorch-1">Implementation Details (PyTorch)</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#classifier-free-guidance" id="markdown-toc-classifier-free-guidance">Classifier-Free Guidance</a>    <ul>
      <li><a href="#background-why-are-external-classifiers-needed-for-text-to-image-synthesis-using-diffusion-models" id="markdown-toc-background-why-are-external-classifiers-needed-for-text-to-image-synthesis-using-diffusion-models">Background: Why Are External Classifiers Needed for Text-to-Image Synthesis Using Diffusion Models?</a>        <ul>
          <li><a href="#the-need-for-external-classifiers" id="markdown-toc-the-need-for-external-classifiers">The Need for External Classifiers</a></li>
        </ul>
      </li>
      <li><a href="#key-papers-introducing-external-classifiers-for-text-to-image-synthesis-using-diffusion-models" id="markdown-toc-key-papers-introducing-external-classifiers-for-text-to-image-synthesis-using-diffusion-models">Key Papers Introducing External Classifiers for Text-to-Image Synthesis Using Diffusion Models</a></li>
      <li><a href="#how-classifier-free-guidance-works" id="markdown-toc-how-classifier-free-guidance-works">How Classifier-Free Guidance Works</a>        <ul>
          <li><a href="#dual-training-path" id="markdown-toc-dual-training-path">Dual Training Path</a></li>
          <li><a href="#equations" id="markdown-toc-equations">Equations</a></li>
        </ul>
      </li>
      <li><a href="#benefits-of-classifier-free-guidance" id="markdown-toc-benefits-of-classifier-free-guidance">Benefits of Classifier-Free Guidance</a></li>
    </ul>
  </li>
  <li><a href="#prompting-guidance" id="markdown-toc-prompting-guidance">Prompting Guidance</a>    <ul>
      <li><a href="#prompting-text-to-image-models" id="markdown-toc-prompting-text-to-image-models">Prompting Text-to-Image Models</a>        <ul>
          <li><a href="#key-prompting-guidelines" id="markdown-toc-key-prompting-guidelines">Key Prompting Guidelines</a></li>
          <li><a href="#example-prompts-for-text-to-image-models" id="markdown-toc-example-prompts-for-text-to-image-models">Example Prompts for Text-to-Image Models</a></li>
        </ul>
      </li>
      <li><a href="#prompting-text-to-video-models" id="markdown-toc-prompting-text-to-video-models">Prompting Text-to-Video Models</a>        <ul>
          <li><a href="#key-prompting-guidelines-1" id="markdown-toc-key-prompting-guidelines-1">Key Prompting Guidelines</a></li>
        </ul>
      </li>
      <li><a href="#camera-movements" id="markdown-toc-camera-movements">Camera Movements</a>        <ul>
          <li><a href="#example-prompts-for-text-to-video-models" id="markdown-toc-example-prompts-for-text-to-video-models">Example Prompts for Text-to-Video Models</a></li>
        </ul>
      </li>
      <li><a href="#summary-1" id="markdown-toc-summary-1">Summary</a>        <ul>
          <li><a href="#text-to-image" id="markdown-toc-text-to-image">Text-to-Image</a></li>
          <li><a href="#text-to-video" id="markdown-toc-text-to-video">Text-to-Video</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#diffusion-models-in-pytorch" id="markdown-toc-diffusion-models-in-pytorch">Diffusion Models in PyTorch</a>    <ul>
      <li><a href="#implementing-the-original-paper" id="markdown-toc-implementing-the-original-paper">Implementing the original paper</a>        <ul>
          <li><a href="#pre-requisites-setup-and-importing-libraries" id="markdown-toc-pre-requisites-setup-and-importing-libraries">Pre-requisites: Setup and Importing Libraries</a></li>
          <li><a href="#helper-functions" id="markdown-toc-helper-functions">Helper functions</a></li>
          <li><a href="#model-core-resnet-or-convnext" id="markdown-toc-model-core-resnet-or-convnext">Model Core: ResNet or ConvNeXT</a></li>
          <li><a href="#attention" id="markdown-toc-attention">Attention</a></li>
          <li><a href="#overall-network" id="markdown-toc-overall-network">Overall network</a></li>
          <li><a href="#forward-diffusion" id="markdown-toc-forward-diffusion">Forward diffusion</a></li>
          <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a></li>
          <li><a href="#sampling-during-training" id="markdown-toc-sampling-during-training">Sampling during training</a></li>
          <li><a href="#training-1" id="markdown-toc-training-1">Training</a></li>
          <li><a href="#creating-a-gif" id="markdown-toc-creating-a-gif">Creating a GIF</a></li>
        </ul>
      </li>
      <li><a href="#denoising-diffusion-pytorch-package" id="markdown-toc-denoising-diffusion-pytorch-package"><code class="language-plaintext highlighter-rouge">denoising-diffusion-pytorch</code> package</a></li>
      <li><a href="#minimal-example" id="markdown-toc-minimal-example">Minimal Example</a></li>
      <li><a href="#training-on-custom-data" id="markdown-toc-training-on-custom-data">Training on Custom Data</a></li>
    </ul>
  </li>
  <li><a href="#huggingface-diffusers" id="markdown-toc-huggingface-diffusers">HuggingFace Diffusers</a></li>
  <li><a href="#implementations" id="markdown-toc-implementations">Implementations</a>    <ul>
      <li><a href="#stable-diffusion" id="markdown-toc-stable-diffusion">Stable Diffusion</a></li>
      <li><a href="#dream-studio" id="markdown-toc-dream-studio">Dream Studio</a></li>
      <li><a href="#midjourney" id="markdown-toc-midjourney">Midjourney</a></li>
      <li><a href="#dall-e-2" id="markdown-toc-dall-e-2">DALL-E 2</a>        <ul>
          <li><a href="#related-clip-contrastive-language-image-pre-training" id="markdown-toc-related-clip-contrastive-language-image-pre-training">Related: CLIP (Contrastive Language-Image Pre-Training)</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#gallery" id="markdown-toc-gallery">Gallery</a></li>
  <li><a href="#faq" id="markdown-toc-faq">FAQ</a>    <ul>
      <li><a href="#at-a-high-level-how-do-diffusion-models-work-what-are-some-other-models-that-are-useful-for-image-generation-and-how-do-they-compare-to-diffusion-models" id="markdown-toc-at-a-high-level-how-do-diffusion-models-work-what-are-some-other-models-that-are-useful-for-image-generation-and-how-do-they-compare-to-diffusion-models">At a high level, how do diffusion models work? What are some other models that are useful for image generation, and how do they compare to diffusion models?</a>        <ul>
          <li><a href="#high-level-overview-of-diffusion-models" id="markdown-toc-high-level-overview-of-diffusion-models">High-Level Overview of Diffusion Models</a>            <ul>
              <li><a href="#forward-diffusion-process" id="markdown-toc-forward-diffusion-process">Forward Diffusion Process</a></li>
              <li><a href="#reverse-denoising-process" id="markdown-toc-reverse-denoising-process">Reverse Denoising Process</a></li>
            </ul>
          </li>
          <li><a href="#other-models-for-image-generation" id="markdown-toc-other-models-for-image-generation">Other Models for Image Generation</a>            <ul>
              <li><a href="#generative-adversarial-networks-gans" id="markdown-toc-generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</a></li>
              <li><a href="#variational-autoencoders-vaes" id="markdown-toc-variational-autoencoders-vaes">Variational Autoencoders (VAEs)</a></li>
              <li><a href="#autoregressive-models" id="markdown-toc-autoregressive-models">Autoregressive Models</a></li>
              <li><a href="#flow-based-models" id="markdown-toc-flow-based-models">Flow-based Models</a></li>
            </ul>
          </li>
          <li><a href="#summary-2" id="markdown-toc-summary-2">Summary</a></li>
        </ul>
      </li>
      <li><a href="#what-is-the-difference-between-ddpm-and-ddims-models" id="markdown-toc-what-is-the-difference-between-ddpm-and-ddims-models">What is the difference between DDPM and DDIMs models?</a>        <ul>
          <li><a href="#ddpm" id="markdown-toc-ddpm">DDPM</a>            <ul>
              <li><a href="#key-characteristics" id="markdown-toc-key-characteristics">Key Characteristics</a></li>
              <li><a href="#advantages-and-disadvantages" id="markdown-toc-advantages-and-disadvantages">Advantages and Disadvantages</a></li>
            </ul>
          </li>
          <li><a href="#ddims" id="markdown-toc-ddims">DDIMs</a>            <ul>
              <li><a href="#key-characteristics-1" id="markdown-toc-key-characteristics-1">Key Characteristics</a></li>
              <li><a href="#advantages-and-disadvantages-1" id="markdown-toc-advantages-and-disadvantages-1">Advantages and Disadvantages</a></li>
            </ul>
          </li>
          <li><a href="#key-differences" id="markdown-toc-key-differences">Key Differences</a></li>
        </ul>
      </li>
      <li><a href="#in-diffusion-models-there-is-a-forward-diffusion-process-and-a-reverse-diffusiondenoising-process-when-do-you-use-which-during-training-and-inference" id="markdown-toc-in-diffusion-models-there-is-a-forward-diffusion-process-and-a-reverse-diffusiondenoising-process-when-do-you-use-which-during-training-and-inference">In diffusion models, there is a forward diffusion process and a reverse diffusion/denoising process. When do you use which during training and inference?</a></li>
      <li><a href="#what-are-the-loss-functions-used-in-diffusion-models" id="markdown-toc-what-are-the-loss-functions-used-in-diffusion-models">What are the loss functions used in Diffusion Models?</a>        <ul>
          <li><a href="#mean-squared-error-mse" id="markdown-toc-mean-squared-error-mse">Mean Squared Error (MSE)</a>            <ul>
              <li><a href="#denoising-score-matching-dsm" id="markdown-toc-denoising-score-matching-dsm">Denoising Score Matching (DSM)</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#integration-with-mse" id="markdown-toc-integration-with-mse">Integration with MSE</a>        <ul>
          <li><a href="#kullback-leibler-divergence-kl-divergence" id="markdown-toc-kullback-leibler-divergence-kl-divergence">Kullback-Leibler Divergence (KL Divergence)</a></li>
          <li><a href="#negative-log-likelihood-nll" id="markdown-toc-negative-log-likelihood-nll">Negative Log-Likelihood (NLL)</a></li>
          <li><a href="#evidence-lower-bound-elbo" id="markdown-toc-evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</a></li>
          <li><a href="#hybrid-loss" id="markdown-toc-hybrid-loss">Hybrid Loss</a></li>
          <li><a href="#cross-entropy-loss" id="markdown-toc-cross-entropy-loss">Cross-Entropy Loss</a></li>
          <li><a href="#variational-bound-loss" id="markdown-toc-variational-bound-loss">Variational Bound Loss</a></li>
          <li><a href="#summary-of-common-loss-functions-in-diffusion-models" id="markdown-toc-summary-of-common-loss-functions-in-diffusion-models">Summary of Common Loss Functions in Diffusion Models</a></li>
        </ul>
      </li>
      <li><a href="#what-is-the-denoising-score-matching-loss-in-diffusion-models-provide-equation-and-intuition" id="markdown-toc-what-is-the-denoising-score-matching-loss-in-diffusion-models-provide-equation-and-intuition">What is the Denoising Score Matching Loss in Diffusion models? Provide equation and intuition.</a></li>
      <li><a href="#what-does-the-stable-in-stable-diffusion-refer-to" id="markdown-toc-what-does-the-stable-in-stable-diffusion-refer-to">What does the “stable” in stable diffusion refer to?</a></li>
      <li><a href="#how-do-you-condition-a-diffusion-model-to-the-textual-input-prompt" id="markdown-toc-how-do-you-condition-a-diffusion-model-to-the-textual-input-prompt">How do you condition a diffusion model to the textual input prompt?</a>        <ul>
          <li><a href="#text-encoding" id="markdown-toc-text-encoding">Text Encoding</a></li>
          <li><a href="#integrating-text-embeddings-into-the-diffusion-process" id="markdown-toc-integrating-text-embeddings-into-the-diffusion-process">Integrating Text Embeddings into the Diffusion Process</a></li>
          <li><a href="#reverse-diffusion-with-textual-guidance" id="markdown-toc-reverse-diffusion-with-textual-guidance">Reverse Diffusion with Textual Guidance</a></li>
          <li><a href="#sampling-and-optimization" id="markdown-toc-sampling-and-optimization">Sampling and Optimization</a></li>
          <li><a href="#evaluation-and-fine-tuning" id="markdown-toc-evaluation-and-fine-tuning">Evaluation and Fine-Tuning</a></li>
        </ul>
      </li>
      <li><a href="#in-the-context-of-diffusion-models-what-role-does-cross-attention-play-how-are-the-q-k-and-v-abstractions-modeled-for-diffusion-models" id="markdown-toc-in-the-context-of-diffusion-models-what-role-does-cross-attention-play-how-are-the-q-k-and-v-abstractions-modeled-for-diffusion-models">In the context of diffusion models, what role does cross attention play? How are the \(Q\), \(K\), and \(V\) abstractions modeled for diffusion models?</a>        <ul>
          <li><a href="#role-of-cross-attention-in-diffusion-models" id="markdown-toc-role-of-cross-attention-in-diffusion-models">Role of Cross-Attention in Diffusion Models</a></li>
          <li><a href="#modeling-q-k-and-v-in-diffusion-models" id="markdown-toc-modeling-q-k-and-v-in-diffusion-models">Modeling \(Q\), \(K\), and \(V\) in Diffusion Models</a>            <ul>
              <li><a href="#detailed-steps" id="markdown-toc-detailed-steps">Detailed Steps</a></li>
            </ul>
          </li>
          <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
        </ul>
      </li>
      <li><a href="#how-is-randomness-in-the-outputs-induced-in-a-diffusion-model" id="markdown-toc-how-is-randomness-in-the-outputs-induced-in-a-diffusion-model">How is randomness in the outputs induced in a diffusion model?</a>        <ul>
          <li><a href="#the-basic-framework-of-diffusion-models" id="markdown-toc-the-basic-framework-of-diffusion-models">The Basic Framework of Diffusion Models</a></li>
          <li><a href="#randomness-in-sampling" id="markdown-toc-randomness-in-sampling">Randomness in Sampling</a></li>
          <li><a href="#conditional-generation" id="markdown-toc-conditional-generation">Conditional Generation</a></li>
          <li><a href="#temperature-scaling" id="markdown-toc-temperature-scaling">Temperature Scaling</a></li>
          <li><a href="#conclusion-1" id="markdown-toc-conclusion-1">Conclusion</a></li>
        </ul>
      </li>
      <li><a href="#how-does-the-noise-schedule-work-in-diffusion-models-what-are-some-standard-noise-schedules" id="markdown-toc-how-does-the-noise-schedule-work-in-diffusion-models-what-are-some-standard-noise-schedules">How does the noise schedule work in diffusion models? What are some standard noise schedules?</a>        <ul>
          <li><a href="#noise-schedule-in-diffusion-models" id="markdown-toc-noise-schedule-in-diffusion-models">Noise Schedule in Diffusion Models</a>            <ul>
              <li><a href="#forward-diffusion-process-1" id="markdown-toc-forward-diffusion-process-1">Forward Diffusion Process</a></li>
              <li><a href="#reverse-denoising-process-1" id="markdown-toc-reverse-denoising-process-1">Reverse Denoising Process</a></li>
            </ul>
          </li>
          <li><a href="#standard-noise-schedules" id="markdown-toc-standard-noise-schedules">Standard Noise Schedules</a></li>
        </ul>
      </li>
      <li><a href="#choosing-a-noise-schedule" id="markdown-toc-choosing-a-noise-schedule">Choosing a Noise Schedule</a></li>
    </ul>
  </li>
  <li><a href="#recent-papers" id="markdown-toc-recent-papers">Recent Papers</a>    <ul>
      <li><a href="#high-resolution-image-synthesis-with-latent-diffusion-models" id="markdown-toc-high-resolution-image-synthesis-with-latent-diffusion-models">High-Resolution Image Synthesis with Latent Diffusion Models</a></li>
      <li><a href="#diffusion-model-alignment-using-direct-preference-optimization" id="markdown-toc-diffusion-model-alignment-using-direct-preference-optimization">Diffusion Model Alignment Using Direct Preference Optimization</a></li>
      <li><a href="#scalable-diffusion-models-with-transformers" id="markdown-toc-scalable-diffusion-models-with-transformers">Scalable Diffusion Models with Transformers</a></li>
      <li><a href="#deepfloyd-if" id="markdown-toc-deepfloyd-if">DeepFloyd IF</a></li>
      <li><a href="#pixart-alpha-fast-training-of-diffusion-transformer-for-photorealistic-text-to-image-synthesis" id="markdown-toc-pixart-alpha-fast-training-of-diffusion-transformer-for-photorealistic-text-to-image-synthesis">PIXART-\(\alpha\): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</a></li>
      <li><a href="#raphael-text-to-image-generation-via-large-mixture-of-diffusion-paths" id="markdown-toc-raphael-text-to-image-generation-via-large-mixture-of-diffusion-paths">RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</a></li>
      <li><a href="#ernie-vilg-20-improving-text-to-image-diffusion-model-with-knowledge-enhanced-mixture-of-denoising-experts" id="markdown-toc-ernie-vilg-20-improving-text-to-image-diffusion-model-with-knowledge-enhanced-mixture-of-denoising-experts">ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts</a></li>
      <li><a href="#imagen-video-high-definition-video-generation-with-diffusion-models" id="markdown-toc-imagen-video-high-definition-video-generation-with-diffusion-models">Imagen Video: High Definition Video Generation with Diffusion Models</a></li>
      <li><a href="#patch-n-pack-navit-a-vision-transformer-for-any-aspect-ratio-and-resolution" id="markdown-toc-patch-n-pack-navit-a-vision-transformer-for-any-aspect-ratio-and-resolution">Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</a></li>
      <li><a href="#sdxl-improving-latent-diffusion-models-for-high-resolution-image-synthesis" id="markdown-toc-sdxl-improving-latent-diffusion-models-for-high-resolution-image-synthesis">SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</a></li>
      <li><a href="#dreamix-video-diffusion-models-are-general-video-editors" id="markdown-toc-dreamix-video-diffusion-models-are-general-video-editors">Dreamix: Video Diffusion Models are General Video Editors</a></li>
      <li><a href="#stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets" id="markdown-toc-stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets">Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></li>
    </ul>
  </li>
  <li><a href="#fine-tuning-diffusion-models" id="markdown-toc-fine-tuning-diffusion-models">Fine-tuning Diffusion Models</a></li>
  <li><a href="#diffusion-model-alignment" id="markdown-toc-diffusion-model-alignment">Diffusion Model Alignment</a>    <ul>
      <li><a href="#diffusion-model-alignment-using-direct-preference-optimization-1" id="markdown-toc-diffusion-model-alignment-using-direct-preference-optimization-1">Diffusion Model Alignment Using Direct Preference Optimization</a></li>
    </ul>
  </li>
  <li><a href="#further-reading" id="markdown-toc-further-reading">Further Reading</a>    <ul>
      <li><a href="#the-illustrated-stable-diffusion" id="markdown-toc-the-illustrated-stable-diffusion">The Illustrated Stable Diffusion</a></li>
      <li><a href="#understanding-diffusion-models-a-unified-perspective" id="markdown-toc-understanding-diffusion-models-a-unified-perspective">Understanding Diffusion Models: A Unified Perspective</a></li>
      <li><a href="#the-annotated-diffusion-model" id="markdown-toc-the-annotated-diffusion-model">The Annotated Diffusion Model</a></li>
      <li><a href="#lilian-weng-what-are-diffusion-models" id="markdown-toc-lilian-weng-what-are-diffusion-models">Lilian Weng: What are Diffusion Models?</a></li>
      <li><a href="#stable-diffusion---what-why-how" id="markdown-toc-stable-diffusion---what-why-how">Stable Diffusion - What, Why, How?</a></li>
      <li><a href="#how-does-stable-diffusion-work--latent-diffusion-models-explained" id="markdown-toc-how-does-stable-diffusion-work--latent-diffusion-models-explained">How does Stable Diffusion work? – Latent Diffusion Models Explained</a></li>
      <li><a href="#diffusion-explainer" id="markdown-toc-diffusion-explainer">Diffusion Explainer</a></li>
      <li><a href="#jupyter-notebook-on-the-theoretical-and-implementation-aspects-of-score-based-generative-models-sgms" id="markdown-toc-jupyter-notebook-on-the-theoretical-and-implementation-aspects-of-score-based-generative-models-sgms">Jupyter notebook on the theoretical and implementation aspects of Score-based Generative Models (SGMs)</a></li>
    </ul>
  </li>
  <li><a href="#references" id="markdown-toc-references">References</a></li>
  <li><a href="#citation" id="markdown-toc-citation">Citation</a></li>
</ul>

<h2 id="background">Background</h2>

<ul>
  <li>Generative models have achieved remarkable success in creating high-quality data samples, with three prominent types being Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Flow-based models. Each of these approaches has distinct advantages and limitations. GANs, known for their adversarial training framework, often face challenges such as unstable training and reduced diversity in generated samples. VAEs rely on surrogate loss functions, which may limit the fidelity of outputs. Flow-based models require specialized reversible architectures, adding complexity to their design.</li>
  <li>Diffusion models present a compelling alternative to these traditional methods. Inspired by non-equilibrium thermodynamics, they define a Markov chain to incrementally add random noise to data in a forward process. The model then learns to reverse this diffusion process, reconstructing data samples from the noise. Unlike VAEs or Flow-based models, diffusion models follow a fixed learning procedure and utilize high-dimensional latent variables, matching the dimensionality of the original data.</li>
  <li>The concept of diffusion models for generative tasks was first introduced in <a href="https://arxiv.org/abs/1503.03585">Sohl-Dickstein et al., 2015</a>. Significant advancements came later with the contributions of <a href="https://arxiv.org/abs/1907.05600">Song et al., 2019</a> at Stanford University and <a href="https://arxiv.org/abs/2006.11239">Ho et al., 2020</a> at Google Brain, who independently improved the approach and expanded its applicability.</li>
  <li>A helpful visual overview of the different types of generative models, including diffusion models, can be found in this diagram from <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian Weng’s blog</a>:</li>
</ul>

<p><img src="../assets/diffusion-models/diff14.png" alt="" /></p>

<ul>
  <li>Diffusion models are conceptually simple yet powerful. Their state-of-the-art results, coupled with the absence of adversarial training, have propelled them to prominence. They have proven essential in cutting-edge models for conditional and unconditional generation across various modalities such as images, audio, and video. Examples include GLIDE and <a href="#dall-e-2">DALL-E 2</a> by OpenAI, Latent Diffusion by the University of Heidelberg, Imagen by Google Brain, and Stable Diffusion by <a href="https://stability.ai/">Stability.ai</a>.</li>
  <li>As a relatively new paradigm, diffusion models hold immense potential for further refinement and innovation. Their current success underscores their growing importance in the field of generative modeling.</li>
</ul>

<h2 id="overview">Overview</h2>

<ul>
  <li>The meteoric rise of diffusion models is one of the biggest developments in Machine Learning in the past several years.</li>
  <li>Diffusion models are generative models which have been gaining significant popularity in the past several years, and for good reason. A handful of seminal papers released in the 2020s alone have shown the world what Diffusion models are capable of, such as <a href="https://arxiv.org/abs/2105.05233">beating GANs</a> on image synthesis. Most recently, Diffusion models were used in <a href="https://www.assemblyai.com/blog/how-dall-e-2-actually-works/">DALL-E 2</a>, OpenAI’s image generation model (image below generated using DALL-E 2).</li>
</ul>

<p><img src="../assets/diffusion-models/dalle2.png" alt="" /></p>

<ul>
  <li>Given the recent wave of success by Diffusion models, many Machine Learning practitioners are surely interested in their inner workings.</li>
  <li>In this primer, we will examine the theoretical foundations of diffusion models, and then demonstrate how to generate images with a diffusion model in PyTorch.</li>
</ul>

<h2 id="introduction">Introduction</h2>

<ul>
  <li>Diffusion probabilistic models (also simply called diffusion models) are generative models, meaning that they are used to generate data similar to the data on which they are trained. As the name suggests, generative models are used to generate new data, for e.g., they can generate new photos of animals that look like real animals whereas discriminative models could tell apart a cat from a dog.</li>
</ul>

<p><img src="../assets/diffusion-models/1.jpg" alt="" /></p>

<ul>
  <li>Fundamentally, diffusion models work by destroying training data through the successive addition of Gaussian noise, and then learning to recover the data by reversing this noising process. In other words, diffusion models are parameterized Markov chains models trained to gradually denoise data. After training, we can use the diffusion model to generate data by simply passing randomly sampled noise through the learned denoising process. In other words, diffusion models undergo the process of transforming a random collection of numbers (the “latents tensor”) into a processed collection of numbers containing the right image information.</li>
  <li>Diffusion Models also go by Diffusion Probabilistic Models, score-based generative models or in some contexts, have been compared to <a href="https://benanne.github.io/2022/01/31/diffusion.html">denoising autoencoders</a> owing to similarity in behavior.</li>
  <li>The following diagram shows that diffusion models can be used to generate images from noise (figure modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>):</li>
</ul>

<p><img src="../assets/diffusion-models/diff3.png" alt="" /></p>

<ul>
  <li>More specifically, a diffusion model is a latent variable model which maps to the latent space using a fixed Markov chain. This chain gradually adds noise to the data in order to obtain the approximate posterior \(q\left(\mathbf{x} 1: T \mid \mathbf{x}_{0}\right)\), where \(\mathbf{x} 1, \ldots, \mathbf{x} T\) are the latent variables with the same dimensionality as \(\mathbf{x}_{0}\). In the figure below, we see such a Markov chain manifested for image data.</li>
  <li>The following diagram (figure modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>):</li>
</ul>

<p><img src="../assets/diffusion-models/diff4.png" alt="" /></p>

<ul>
  <li>Ultimately, the image is asymptotically transformed to pure Gaussian noise. The goal of training a diffusion model is to learn the reverse process - i.e. training \(p_{\theta}\left(x_{t-1} \mid x_{t}\right)\). By traversing backwards along this chain, we can generate new data, as shown below (figure modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>).</li>
</ul>

<p><img src="../assets/diffusion-models/diff5.png" alt="" /></p>

<ul>
  <li>Under-the-hood, diffusion Models define a Markov chain of diffusion steps that add random noise to the data and then learn to reverse the diffusion process in order to create the desired data output from the noise. This can be seen in the image below:
    <ul>
      <li>Recall that a Markov chain is a stochastic model that describes a sequence of possible events where the probability of each event only depends on the state of the previous event. Markov chains are used to calculate the probability of an event occurring by considering it as a state transitioning to another state or a state transitioning to the same state as before. The defining characteristic of a Markov chain is that no matter how the process arrived at its present state, the possible future states are fixed.</li>
    </ul>
  </li>
  <li><strong>Key takeaway</strong>
    <ul>
      <li>In a nutshell, diffusion models are constructed by first describing a procedure for gradually turning data into noise, and then training a neural network that learns to invert this procedure step-by-step. Each of these steps consists of taking a noisy input and making it slightly less noisy, by filling in some of the information obscured by the noise. If you start from pure noise and do this enough times, it turns out you can generate data this way! (source)</li>
    </ul>
  </li>
</ul>

<h2 id="advantages">Advantages</h2>

<ul>
  <li>Diffusion probabilistic models are latent variable models capable to synthesize high quality images. As mentioned above, research into diffusion models has exploded in recent years. Inspired by <a href="https://arxiv.org/abs/1503.03585">non-equilibrium thermodynamics</a>, diffusion models currently produce State-of-the-Art image quality, examples of which can be seen below (figure adapted from <a href="https://arxiv.org/pdf/2105.05233.pdf">source</a>):</li>
</ul>

<p><img src="../assets/diffusion-models/diff6.png" alt="" /></p>

<ul>
  <li>Beyond cutting-edge image quality, diffusion models come with a host of other benefits, including not requiring adversarial training. The difficulties of adversarial training are well-documented; and, in cases where non-adversarial alternatives exist with comparable performance and training efficiency, it is usually best to utilize them. On the topic of training efficiency, diffusion models also have the added benefits of scalability and parallelizability.</li>
  <li>While diffusion models almost seem to be producing results out of thin air, there are a lot of careful and interesting mathematical choices and details that provide the foundation for these results, and best practices are still evolving in the literature. Let’s take a look at the mathematical theory underpinning diffusion models in more detail now.</li>
  <li>Their performance is, allegedly, superior to recent state-of-the-art generative models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) in most cases.</li>
</ul>

<h2 id="definitions">Definitions</h2>

<h3 id="diffusion-models">Diffusion Models</h3>

<ul>
  <li>Diffusion models are neural models that model \(p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right)\) and are trained end-to-end to denoise a noisy input to a continuous output such as an image/audio (similar to how GANs generate continuous outputs). Examples: UNet, Conditioned UNet, 3D UNet, Transformer UNet.</li>
  <li>The following figure from the <a href="https://arxiv.org/abs/2006.11239">DDPM paper</a> shows the process of a diffusion model:</li>
</ul>

<p><img src="../assets/diffusion-models/diff1.png" alt="" /></p>

<h3 id="schedulers">Schedulers</h3>

<ul>
  <li>Algorithm class for both inference and training. The class provides functionality to compute previous image according to alpha, beta schedule as well as predict noise for training. Examples: <a href="https://arxiv.org/abs/2006.11239">DDPM</a>, <a href="https://arxiv.org/abs/2010.02502">DDIMs</a>, <a href="https://arxiv.org/abs/2202.09778">PNDM</a>, <a href="https://arxiv.org/abs/2204.13902">DEIS</a>.</li>
  <li>The figure below from the <a href="https://arxiv.org/abs/2006.11239">DDPM paper</a> shows the sampling and training algorithms:</li>
</ul>

<p><img src="../assets/diffusion-models/diff2.png" alt="" /></p>

<h3 id="sampling-and-training-algorithms">Sampling and training algorithms</h3>

<ul>
  <li>Diffusion Pipeline: End-to-end pipeline that includes multiple diffusion models, possible text encoders, super-resolution model (for high-res image generation, in case of Imagen), etc.
    <ul>
      <li>Examples: <a href="https://github.com/openai/glide-text2im">GLIDE</a>, <a href="https://github.com/CompVis/latent-diffusion">Latent-Diffusion</a>, <a href="https://imagen.research.google/">Imagen</a>, <a href="https://openai.com/dall-e-2/">DALL-E 2</a>.</li>
    </ul>
  </li>
  <li>The figure below from the <a href="https://imagen.research.google/">Imagen</a> paper shows the overall flow of the model.</li>
</ul>

<p><img src="../assets/diffusion-models/diff3.jpg" alt="" /></p>

<h2 id="diffusion-models-the-theory">Diffusion Models: The Theory</h2>

<ul>
  <li>
    <p>In this section, we explore the theoretical foundation of diffusion models:</p>

    <ol>
      <li>
        <p><strong>Markov Chain Representation</strong>: A diffusion model is parameterized as a Markov chain, implying that the latent variables \(x_1, \ldots, x_T\) depend solely on the adjacent time-step (preceding or following).</p>
      </li>
      <li>
        <p><strong>Gaussian Transition Distributions</strong>: The transitions in the Markov chain are modeled as Gaussian distributions. The forward process employs a variance schedule, while the parameters for the reverse process are learned.</p>
      </li>
      <li>
        <p><strong>Asymptotic Distribution</strong>: The diffusion process ensures that \(x_T\) converges asymptotically to an isotropic Gaussian distribution for sufficiently large \(T\).</p>
      </li>
      <li>
        <p><strong>Variance Scheduling</strong>: While the variance schedule in our implementation is fixed, it can also be learned. For fixed schedules, using a geometric progression often yields better results compared to a linear progression. Regardless of the approach, the variances typically increase over time within the series (\(\beta_i &lt; \beta_j\) for \(i &lt; j\)).</p>
      </li>
      <li>
        <p><strong>Architectural Flexibility</strong>: Diffusion models are highly versatile, accommodating any architecture with matching input and output dimensionalities. Commonly, U-Net-like architectures are utilized in implementations.</p>
      </li>
      <li>
        <p><strong>Training Objective</strong>: The training goal is to maximize the likelihood of the data, which involves optimizing model parameters to minimize the variational upper bound of the negative log likelihood.</p>
      </li>
      <li>
        <p><strong>Role of KL Divergences</strong>: Owing to the Markov assumption, most terms in the objective function can be expressed as KL divergences. These are tractable for computation because the model assumes Gaussian distributions, thereby avoiding the need for Monte Carlo approximations.</p>
      </li>
      <li>
        <p><strong>Simplified Objective</strong>: Training is most stable and effective when a simplified objective function is used to predict the noise component of a given latent variable.</p>
      </li>
      <li>
        <p><strong>Discrete Decoding</strong>: In the final step of the reverse diffusion process, a discrete decoder is employed to compute log likelihoods across pixel values.</p>
      </li>
    </ol>
  </li>
</ul>

<h2 id="diffusion-models-a-deep-dive">Diffusion models: A Deep Dive</h2>

<h3 id="general-overview">General Overview</h3>

<ul>
  <li>Diffusion Models are a latent variable model that maps the latent space using the Markov chain. They essentially are made up of neural networks that learn to gradually de-noise data.
    <ul>
      <li>Note: Latent variable models aim to model the probability distribution with latent variables. Latent variables are a transformation of the data points into a continuous lower-dimensional space. <a href="https://theaisummer.com/latent-variable-models/">(source)</a></li>
      <li>Additionally, the latent space is simply a representation of compressed data in which similar data points are closer together in space.</li>
      <li>Latent space is useful for learning data features and for finding simpler representations of data for analysis.</li>
    </ul>
  </li>
  <li>Diffusion models consist of two processes: a predefined forward diffusion and a learned reverse de-noising diffusion process.</li>
  <li>In the image below, we can see the Markov chain working towards image generation. It represents the first process of forward diffusion.</li>
  <li>The forward diffusion process \(q\) of our choosing, gradually adds Gaussian noise to an image, until you end up with pure noise.</li>
</ul>

<p><img src="../assets/diffusion-models/4.jpg" alt="" /></p>

<ul>
  <li>Next, the image is asymptotically transformed to just Gaussian noise. The goal of training a diffusion model is to learn the reverse process.</li>
  <li>The second process below is the learned reverse de-noising diffusion process \(p_\theta\). Here, a neural network is trained to gradually de-noise an image starting from pure noise, until you end up with an actual image.</li>
  <li>Thus, we traverse backwards along this chain to generate the new data as seen below:</li>
</ul>

<p><img src="../assets/diffusion-models/5.jpg" alt="" /></p>

<ul>
  <li>Both the forward and reverse process (both indexed with \(t\)) continue for a duration of finite time steps \(T\) (the DDPM authors use \(T\) =1000).</li>
  <li>You will start off with \(t = 0\) where you will sample a real image \(x_0\) from your data distribution.
    <ul>
      <li>A quick example is say you have an image of a cat from ImageNet dataset.</li>
    </ul>
  </li>
  <li>You will then continue with the forward process and sample some noise from a Gaussian distribution at each time step \(t\).
    <ul>
      <li>This will be added to the image of the previous time step.</li>
    </ul>
  </li>
  <li>Given a sufficiently large \(T\) and a continuous process of adding noise at each time step, you will end up with \(t = T\).</li>
  <li>
    <p>This is also called an isotropic Gaussian distribution.</p>
  </li>
  <li>Below is the high-level overview of how everything runs under the hood:
    <ul>
      <li>we take a random sample \(x_0\) from the real unknown and complex data distribution \(q(x_0)\)</li>
      <li>we sample a noise level \(t\) uniformly between \(1\) and \(T\) (i.e., a random time step)</li>
      <li>we sample some noise from a Gaussian distribution and corrupt the input by this noise at level \(t\) (using the nice property defined above)</li>
      <li>the neural network is trained to predict this noise based on the corrupted image \(x_t\) (i.e. noise applied on \(x_0\) based on known schedule \(\beta_t\))
In reality, all of this is done on batches of data, as one uses stochastic gradient descent to optimize neural networks.</li>
    </ul>
  </li>
</ul>

<h3 id="the-math-under-the-hood">The Math Under-the-hood</h3>

<ul>
  <li>
    <p>As mentioned above, a diffusion model consists of a forward process (or diffusion process), in which a datum (generally an image) is progressively noised, and a reverse process (or reverse diffusion process), in which noise is transformed back into a sample from the target distribution.</p>
  </li>
  <li>
    <p>The sampling chain transitions in the forward process can be set to conditional Gaussians when the noise level is sufficiently low. Combining this fact with the Markov assumption leads to a simple parameterization of the forward process:</p>

\[q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right):=\prod_{t=1}^{T} q\left(\mathbf{x}_{t} \mid \mathbf{x}_{t-1}\right):=\prod_{t=1}^{T} \mathcal{N}\left(\mathbf{x}_{t} ; \sqrt{1-\beta_{t}} \mathbf{x}_{t-1}, \beta_{t} \mathbf{I}\right)\]

    <ul>
      <li>where \(\beta_1, \ldots, \beta_T\) is a variance schedule (either learned or fixed) which, if well-behaved, ensures that \(x_T\) is nearly an isotropic Gaussian for sufficiently large \(T\).</li>
    </ul>
  </li>
  <li>
    <p>Given the Markov assumption, the joint distribution of the latent variables is the product of the Gaussian conditional chain transitions (figure modified from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>).</p>
  </li>
</ul>

<p><img src="../assets/diffusion-models/diff7.png" alt="" /></p>

<ul>
  <li>
    <p>As mentioned previously, the “magic” of diffusion models comes in the reverse process. During training, the model learns to reverse this diffusion process in order to generate new data. Starting with the pure Gaussian noise \(p(\mathbf{x} T):=\mathcal{N}\left(\mathbf{x}_{T}, \mathbf{0}, \mathbf{I}\right)\), the model learns the joint distribution \(p \theta(\mathbf{x} 0: T)\) as,</p>

\[p_{\theta}\left(\mathbf{x}_{0: T}\right):=p\left(\mathbf{x}_{T}\right) \prod_{t=1}^{T} p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right):=p\left(\mathbf{x}_{T}\right) \prod_{t=1}^{T} \mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \boldsymbol{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)\]

    <ul>
      <li>where the time-dependent parameters of the Gaussian transitions are learned. Note in particular that the Markov formulation asserts that a given reverse diffusion transition distribution depends only on the previous timestep (or following timestep, depending on how you look at it):</li>
    </ul>
  </li>
</ul>

\[p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \mathbf{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)\]

<p><img src="../assets/diffusion-models/diff8.png" alt="" /></p>

<p>Diffusion models are a class of generative models that define a forward process that gradually adds noise to data, and a reverse process that learns to denoise it step-by-step to generate new data. Below is an enhanced explanation of key types of diffusion models, including DDPM and DDIMs, with implementation details and references to the original papers where each model type was proposed.</p>

<h2 id="types-of-diffusion-models">Types of Diffusion Models</h2>

<ul>
  <li>Diffusion models are a class of generative models that define a forward process that gradually adds noise to data, and a reverse process that learns to denoise it step-by-step to generate new data. Below is an enhanced explanation of key types of diffusion models, including DDPM, DDIMs, SGMs, VDMs, and SDE-based models, with implementation details and their pros and cons.</li>
</ul>

<h3 id="denoising-diffusion-probabilistic-models-ddpms--discrete-time-diffusion-models">Denoising Diffusion Probabilistic Models (DDPMs) / Discrete-Time Diffusion Models</h3>

<ul>
  <li>Denoising Diffusion Probabilistic Models (DDPMs) were proposed in the paper “<a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a>” by Jonathan Ho, Ajay Jain, and Pieter Abbeel (2020).</li>
  <li>DDPMs are a foundational type of diffusion model introduced by Jonathan Ho, Ajay Jain, and Pieter Abbeel in 2020. They consist of two processes: a forward process (diffusion) that adds noise to data, and a reverse process (denoising) that learns to remove this noise to generate new data samples.</li>
</ul>

<h4 id="implementation-details">Implementation Details</h4>

<ul>
  <li>
    <p><strong>Forward Process:</strong></p>

    <ul>
      <li>The forward process gradually corrupts the data by adding Gaussian noise over a series of timesteps \(t = 1, \ldots, T\). Given data \(x_0\), the noisy version at timestep \(t\) is given by:</li>
    </ul>

\[q(x_t \| x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\]

    <ul>
      <li>where \(\beta_t\) are variance schedule parameters controlling the noise level at each timestep. This process ensures that as \(t\) increases, the data becomes increasingly noisy, eventually resembling pure Gaussian noise.</li>
    </ul>
  </li>
  <li>
    <p><strong>Reverse Process:</strong></p>

    <p>The reverse process aims to denoise the data by learning to reverse the forward process. The model \(p_\theta(x_{t-1} \| x_t)\) is trained to approximate:</p>

\[p_\theta(x_{t-1} \| x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \sigma_t^2 I)\]

    <ul>
      <li>where \(\mu_\theta\) is the mean predicted by a neural network, and \(\sigma_t^2\) is a fixed variance. The objective is to minimize the variational bound, typically using a simplified loss function:</li>
    </ul>

\[L = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 \right]\]

    <ul>
      <li>where \(\epsilon\) is sampled from a standard normal distribution, and \(\epsilon_\theta\) is the noise predicted by the model. This loss function encourages the model to accurately predict the noise added during the forward process, facilitating effective denoising during sampling.</li>
    </ul>
  </li>
  <li>
    <p><strong>Sampling:</strong></p>

    <ul>
      <li>The denoising process starts from pure noise \(x_T \sim \mathcal{N}(0, I)\) and gradually generates a sample \(x_0\) by reversing the noise addition steps. This iterative process involves sampling from the learned reverse distributions \(p_\theta(x_{t-1} \| x_t)\) for \(t = T, \ldots, 1\), ultimately producing a data sample that resembles the original data distribution.</li>
    </ul>
  </li>
</ul>

<h4 id="pros">Pros</h4>

<ul>
  <li>Strong theoretical foundation with a clear probabilistic framework.</li>
  <li>Capable of generating high-quality samples.</li>
</ul>

<h4 id="cons">Cons</h4>

<ul>
  <li>Sampling can be computationally intensive due to the large number of required steps.</li>
</ul>

<h3 id="denoising-diffusion-implicit-models-ddims">Denoising Diffusion Implicit Models (DDIMs)</h3>

<ul>
  <li>Denoising Diffusion Implicit Models (DDIMs) were proposed in the paper “<a href="https://arxiv.org/abs/2010.02502">Denoising Diffusion Implicit Models</a>” by Jiaming Song, Chenlin Meng, and Stefano Ermon (2020). DDIMs are an extension of DDPM that allow for deterministic sampling, which can significantly speed up the generation process.</li>
</ul>

<h4 id="implementation-details-1">Implementation Details</h4>

<ul>
  <li><strong>Forward Process:</strong>
    <ul>
      <li>Similar to DDPMs, the forward process in DDIMs involves adding noise to the data. However, the key difference lies in the parameterization of the reverse process, which allows for a deterministic mapping during sampling.</li>
    </ul>
  </li>
  <li><strong>Reverse Process:</strong>
    <ul>
      <li>DDIMs modifies the reverse process to make it deterministic. Instead of learning the reverse transition probabilities, DDIMs defines a deterministic mapping:</li>
    </ul>

\[x_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{x_t - \sqrt{1 - \alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}} \right) + \sqrt{1 - \alpha_{t-1}} \epsilon_\theta(x_t, t)\]

    <ul>
      <li>where \(\alpha_t\) is a function of the noise schedule \(\beta_t\). This deterministic approach allows for a fixed trajectory in the latent space, making the process faster and more efficient.</li>
    </ul>
  </li>
  <li><strong>Sampling:</strong>
    <ul>
      <li>The sampling process in DDIMs can be interpreted as a non-Markovian process that generates samples in fewer steps compared to DDPM, offering a trade-off between sample quality and computational efficiency. By eliminating the stochasticity in the reverse process, DDIMs enables faster sampling while maintaining high-quality sample generation.</li>
    </ul>
  </li>
</ul>

<h4 id="pros-1">Pros</h4>

<ul>
  <li>Faster sampling compared to DDPM due to deterministic processes.</li>
  <li>Offers a trade-off between sample quality and computational efficiency.</li>
</ul>

<h4 id="cons-1">Cons</h4>

<ul>
  <li>May require careful tuning to balance speed and sample quality.</li>
</ul>

<h3 id="score-based-generative-models-sgms--continuous-time-diffusion-models">Score-Based Generative Models (SGMs) / Continuous-Time Diffusion Models</h3>

<ul>
  <li>Score-Based Generative Models (SGMs) leverage the score function of the data distribution to guide the denoising process, rather than modeling the direct transition probabilities between noisy and clean samples. This approach was introduced in the paper “<a href="https://arxiv.org/abs/1907.05600">Generative Modeling by Estimating Gradients of the Data Distribution</a>” by Yang Song and Stefano Ermon (2019).</li>
</ul>

<h4 id="implementation-details-2">Implementation Details</h4>

<ol>
  <li><strong>Neural Network Architecture:</strong>
    <ul>
      <li>SGMs typically employ U-Net architectures due to their efficacy in capturing multi-scale features, which is crucial for modeling the score function across different noise levels.</li>
    </ul>
  </li>
  <li><strong>Noise Conditional Score Network (NCSN):</strong>
    <ul>
      <li>
        <p>The neural network is trained to predict the score function conditioned on the noise level, denoted as \(s_\theta(x_t, \sigma_t)\), where:</p>

\[s_\theta(x_t, \sigma_t) = \nabla_{x_t} \log p(x_t)\]
      </li>
      <li>
        <p>This conditioning allows the model to handle varying degrees of noise during the denoising process.</p>
      </li>
    </ul>
  </li>
  <li><strong>Training Objective:</strong>
    <ul>
      <li>
        <p>The model is trained using a denoising score matching objective, which minimizes:</p>

\[L = \mathbb{E}_{x_0, t, \epsilon} \left[ \lambda(t) \| s_\theta(x_t, t) - \nabla_{x_t} \log p(x_t \| x_0) \|^2 \right]\]

        <ul>
          <li>where \(\lambda(t)\) is a weighting function controlling the importance of different noise levels.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Sampling Procedure:</strong>
    <ul>
      <li>
        <p>Sampling from SGMs involves simulating the reverse diffusion process using Langevin dynamics:</p>

\[x_{t-1} = x_t + \eta s_\theta(x_t, t) + \sqrt{2\eta} z_t\]

        <ul>
          <li>where \(\eta\) is the step size and \(z_t \sim \mathcal{N}(0, I)\) is Gaussian noise.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h4 id="pros-2">Pros</h4>

<ul>
  <li>SGMs can offer flexible sampling methods, including deterministic and stochastic options.</li>
  <li>These models effectively capture complex data distributions.</li>
  <li>They generalize well across different data types.</li>
</ul>

<h4 id="cons-2">Cons</h4>

<ul>
  <li>The sampling process can be slow due to iterative Langevin steps.</li>
  <li>Training requires accurate estimation of the score function across different noise levels.</li>
</ul>

<h3 id="variational-diffusion-models-vdms">Variational Diffusion Models (VDMs)</h3>

<ul>
  <li>Variational Diffusion Models (VDMs) integrate variational inference with diffusion processes to enhance flexibility and sample quality. This approach was introduced in the paper “<a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a>” by Robin Rombach et al. (2022).</li>
</ul>

<h4 id="implementation-details-3">Implementation Details</h4>

<ol>
  <li>
    <p><strong>Latent Variable Introduction:</strong></p>

    <ul>
      <li>VDMs integrate latent variables \(z\) into the diffusion framework, combining the strengths of Variational Autoencoders (VAEs) and diffusion processes. The generative process includes:</li>
    </ul>

\[x_0 \sim p_\theta(x \| z), \quad z \sim p(z)\]
  </li>
  <li><strong>Variational Inference:</strong>
    <ul>
      <li>To approximate the intractable posterior \(p(z \| x)\), VDMs employ a variational approximation \(q_\phi(z \| x)\), and the model is trained by maximizing the Evidence Lower Bound (ELBO):</li>
    </ul>

\[\mathbb{E}_{q_\phi(z|x)} \left[ \log p_\theta(x \| z) \right] - D_{KL} (q_\phi(z|x) \| p(z))\]
  </li>
  <li><strong>Diffusion Process in Latent Space:</strong>
    <ul>
      <li>The forward diffusion process in VDMs gradually corrupts the latent variables:</li>
    </ul>

\[q(z_t \| z_{t-1}) = \mathcal{N}(z_t; \sqrt{1 - \beta_t} z_{t-1}, \beta_t I)\]

    <ul>
      <li>The reverse process then learns to denoise these latent variables, effectively modeling the data distribution in the latent space.</li>
    </ul>
  </li>
  <li>
    <p><strong>Combining VAE and Diffusion:</strong></p>

    <ul>
      <li>By integrating the VAE framework with diffusion processes, VDMs leverage the latent variable structure to guide the denoising process. This combination enhances the flexibility and expressiveness of the model, allowing it to capture complex data distributions more effectively.</li>
    </ul>
  </li>
</ol>

<h4 id="pros-3">Pros</h4>

<ul>
  <li>Latent diffusion significantly reduces computational costs compared to pixel-based diffusion.</li>
  <li>Combines the advantages of VAEs and diffusion models for improved representation learning.</li>
  <li>More efficient in generating high-resolution images.</li>
</ul>

<h4 id="cons-3">Cons</h4>

<ul>
  <li>Training requires careful balancing of the latent space structure.</li>
  <li>Requires additional encoding-decoding steps.</li>
</ul>

<h3 id="stochastic-differential-equation-sde-based-models">Stochastic Differential Equation (SDE)-Based Models</h3>

<ul>
  <li>Stochastic Differential Equation (SDE)-based diffusion models generalize the diffusion process to a continuous-time framework. This approach was introduced in the paper “<a href="https://arxiv.org/abs/2011.13456">Score-Based Generative Modeling through Stochastic Differential Equations</a>” by Yang Song et al. (2021).</li>
</ul>

<h4 id="implementation-details-4">Implementation Details</h4>

<ol>
  <li><strong>Continuous-Time Formulation:</strong>
    <ul>
      <li>SDE-based models generalize the discrete-time diffusion process to continuous time using an SDE:</li>
    </ul>

\[dx = f(x, t) dt + g(t) dW\]

    <ul>
      <li>where \(W\) is a Wiener process, and \(f(x, t)\) and \(g(t)\) are drift and diffusion coefficients, respectively.</li>
    </ul>
  </li>
  <li><strong>Forward and Reverse SDEs:</strong>
    <ul>
      <li>The forward SDE describes the noise addition:</li>
    </ul>

\[dx = -\frac{1}{2} \beta(t) x dt + \sqrt{\beta(t)} dW\]

    <ul>
      <li>The reverse-time SDE is used for sampling:</li>
    </ul>

\[dx = \left[\frac{1}{2} \beta(t) x - \beta(t) s_\theta(x, t) \right] dt + \sqrt{\beta(t)} dW\]
  </li>
  <li><strong>Training via Score Matching:</strong>
    <ul>
      <li>The score function \(s_\theta(x, t)\) is learned via score matching:</li>
    </ul>

\[\mathbb{E}_{p(x_t|x_0)} \left[ \lambda(t) \| s_\theta(x_t, t) - \nabla_{x_t} \log p(x_t \| x_0) \|^2 \right]\]
  </li>
  <li><strong>Sampling with Numerical Solvers:</strong>
    <ul>
      <li>Sampling from SDE-based models involves solving the reverse-time SDE using numerical methods such as Euler-Maruyama:</li>
    </ul>

\[x_{t - \Delta t} = x_t + \left[\frac{1}{2} \beta(t) x_t - \beta(t) s_\theta(x_t, t) \right] \Delta t + \sqrt{\beta(t) \Delta t} z_t\]

    <ul>
      <li>where \(z_t \sim \mathcal{N}(0, I)\).</li>
    </ul>
  </li>
</ol>

<h4 id="pros-4">Pros</h4>

<ul>
  <li>SDE-based models provide a continuous-time formulation, leading to more flexibility in training and sampling.</li>
  <li>They can generate high-quality samples efficiently.</li>
  <li>The framework is theoretically robust and generalizes other diffusion-based methods.</li>
</ul>

<h4 id="cons-4">Cons</h4>

<ul>
  <li>Requires specialized solvers for numerical integration.</li>
  <li>Computational cost can be high for complex models.</li>
</ul>

<h3 id="comparative-analysis">Comparative Analysis</h3>

<ul>
  <li>The table below offers a detailed comparison highlights the strengths and trade-offs among different diffusion model variants, offering insights into their practical applications and limitations.</li>
</ul>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Model Type</strong></th>
<th class="tg-hcenter-valign-first"><strong>Strengths</strong></th>
<th class="tg-hcenter-valign-first"><strong>Weaknesses</strong></th>
<th class="tg-hcenter-valign-second"><strong>Notable Features</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first"><strong>DDPM</strong></td>
<td class="tg-tleft-valign-first">Strong theoretical foundation, high-quality samples</td>
<td class="tg-tleft-valign-first">Slow sampling due to large step count</td>
<td class="tg-tleft-valign-second">Probabilistic denoising framework</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><strong>DDIMs</strong></td>
<td class="tg-tleft-valign-first">Faster sampling than DDPM, deterministic</td>
<td class="tg-tleft-valign-first">Requires careful tuning</td>
<td class="tg-tleft-valign-second">Reduces step count for efficient generation</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><strong>SGMs</strong></td>
<td class="tg-tleft-valign-first">Flexible sampling, well-suited for complex data</td>
<td class="tg-tleft-valign-first">Slow due to iterative Langevin steps</td>
<td class="tg-tleft-valign-second">Uses score function for denoising</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><strong>VDMs</strong></td>
<td class="tg-tleft-valign-first">Efficient high-resolution image generation</td>
<td class="tg-tleft-valign-first">Complexity in training and inference</td>
<td class="tg-tleft-valign-second">Integrates VAEs with diffusion</td>
</tr>
<tr>
<td class="tg-tleft-valign-first"><strong>SDE-Based</strong></td>
<td class="tg-tleft-valign-first">Continuous-time, flexible framework</td>
<td class="tg-tleft-valign-first">Requires numerical solvers, computationally expensive</td>
<td class="tg-tleft-valign-second">Extends diffusion models into continuous-time</td>
</tr>
</tbody>
</table>
</div>

<h2 id="training">Training</h2>

<ul>
  <li>A diffusion model is trained by finding the reverse Markov transitions that maximize the likelihood of the training data. In practice, training equivalently consists of minimizing the variational upper bound on the negative log likelihood.</li>
</ul>

\[\mathbb{E}\left[-\log p_{\theta}\left(\mathbf{x}_{0}\right)\right] \leq \mathbb{E}_{q}\left[-\log \frac{p_{\theta}\left(\mathbf{x}_{0: T}\right)}{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_{0}\right)}\right]=: L_{v l b}\]

<ul>
  <li>
    <p>As a notation detail, note that \(L_{v l b}\) is technically an upper bound (the negative of the ELBO) which we are trying to minimize, but we refer to it as \(L_{v l b}\) for consistency with the literature.</p>
  </li>
  <li>
    <p>We seek to rewrite the \(L_{v l b}\) in terms of Kullback-Leibler (KL) Divergences. The KL Divergence is an asymmetric statistical distance measure of how much one probability distribution \(P\) differs from a reference distribution \(Q\). We are interested in formulating \(L_{v l b}\) in terms of \(KL\) divergences because the transition distributions in our Markov chain are Gaussians, and the KL divergence between Gaussians has a closed form.</p>
  </li>
</ul>

<h3 id="recap-kl-divergence">Recap: KL Divergence</h3>

<ul>
  <li>
    <p>The mathematical form of the \(\mathrm{KL}\) divergence for continuous distributions is,</p>

\[D_{\mathrm{KL}}(P \| Q)=\int_{-\infty}^{\infty} p(x) \log \left(\frac{p(x)}{q(x)}\right) d x\]

    <ul>
      <li>Note that the double bars in the above equation indicate that the function is not symmetric with respect to its arguments.</li>
    </ul>
  </li>
  <li>
    <p>Below you can see the \(K L\) divergence of a varying distribution \(P\) (blue) from a reference distribution \(Q\) (red). The green curve indicates the function within the integral in the definition for the \(\mathrm{KL}\) divergence above, and the total area under the curve represents the value of the KL divergence of \(P\) from \(Q\) at any given moment, a value which is also displayed numerically.</p>
  </li>
</ul>

<p><img src="../assets/diffusion-models/diff9.jpg" alt="" /></p>

<h3 id="casting-l_v-l-b-in-terms-of-kl-divergences">Casting \(L_{v l b}\) in Terms of KL Divergences</h3>

<ul>
  <li>
    <p>As mentioned previously, it is possible[1] to rewrite \(L v l b\) almost completely in terms of KL divergences:</p>

\[L_{v l b}=L_{0}+L_{1}+\ldots+L_{T-1}+L_{T}\]

    <ul>
      <li>where, 
  \(\begin{gathered}
  L_{0}=-\log p_{\theta}\left(x_{0} \mid x_{1}\right) \\
  L_{t-1}=D_{K L}\left(q\left(x_{t-1} \mid x_{t}, x_{0}\right) \| p_{\theta}\left(x_{t-1} \mid x_{t}\right)\right) \\
  L_{T}=D_{K L}\left(q\left(x_{T} \mid x_{0}\right) \| p\left(x_{T}\right)\right)
  \end{gathered}\)</li>
    </ul>
  </li>
  <li>
    <p>Conditioning the forward process posterior on \(x_{0}\) in \(L_{t-1}\) results in a tractable form that leads to all KL divergences being comparisons between Gaussians. This means that the divergences can be exactly calculated with closed-form expressions rather than with <a href="https://arxiv.org/abs/2006.11239">Monte Carlo estimates</a>.</p>
  </li>
</ul>

<h2 id="model-choices">Model Choices</h2>

<ul>
  <li>With the mathematical foundation for our objective function established, we now need to make several choices regarding how our diffusion model will be implemented. For the forward process, the only choice required is defining the variance schedule, the values of which are generally increasing during the forward process.</li>
  <li>For the reverse process, we much choose the Gaussian distribution parameterization / model architecture(s). Note the high degree of flexibility that Diffusion models afford - the only requirement on our architecture is that its input and output have the same dimensionality.
We will explore the details of these choices in more detail below.</li>
</ul>

<h3 id="forward-process-and-l_t">Forward Process and \(L_{T}\)</h3>

<ul>
  <li>As noted above, regarding the forward process, we must define the variance schedule. In particular, we set them to be time-dependent constants, ignoring the fact that they can be learned. For example, per <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic models</a>, a linear schedule from \(\beta_{1}=10^{-4}\) to \(\beta_{T}=0.2\) might be used, or perhaps a geometric series.
Regardless of the particular values chosen, the fact that the variance schedule is fixed results in \(L_{T}\) becoming a constant with respect to our set of learnable parameters, allowing us to ignore it as far as training is concerned.</li>
</ul>

\[L_{T}=D_{K L}\left(q\left(x_{T}+x_{0}\right) \| p\left(x_{T}\right)\right)\]

<h3 id="reverse-process-and-l_1-t-1">Reverse Process and \(L_{1: T-1}\)</h3>

<ul>
  <li>Now we discuss the choices required in defining the reverse process. Recall from above we defined the reverse Markov transitions as a Gaussian:</li>
</ul>

\[p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \mathbf{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)\]

<ul>
  <li>We must now define the functional forms of \(\mu_{\theta}\) or \(\Sigma_{\theta}\). While there are more complicated ways to
parameterize \(\boldsymbol{\Sigma}_\theta\), we simply set,</li>
</ul>

\[\begin{gathered}
\boldsymbol{\Sigma}_{\theta}\left(x_{t}, t\right)=\sigma_{t}^{2} \mathbb{I} \\
\sigma_{t}^{2}=\beta_{t}
\end{gathered}\]

<ul>
  <li>That is, we assume that the multivariate Gaussian is a product of independent gaussians with identical variance, a variance value which can change with time. We set these variances to be equivalent to our forward process variance schedule.</li>
  <li>
    <p>Given this new formulation of \(\Sigma_{\theta_{1}}\), we have</p>

\[p_{\theta}\left(\mathbf{x}_{t-1} \mid \mathbf{x}_{t}\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \mathbf{\Sigma}_{\theta}\left(\mathbf{x}_{t}, t\right):=\mathcal{N}\left(\mathbf{x}_{t-1} ; \boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right), \sigma_{t}^{2} \mathbf{I}\right)\right.\]

    <ul>
      <li>which allows us to transform,</li>
    </ul>

\[L_{t-1}=D_{K L}\left(q\left(x_{t-1} \mid x_{t}, x_{0}\right) \| p_{\theta}\left(x_{t-1} \mid x_{t}\right)\right)\]

    <ul>
      <li>to,</li>
    </ul>

\[L_{t-1} \propto\left\|\tilde{\mu}_{t}\left(x_{t}, x_{0}\right)-\mu_{\theta}\left(x_{t}, t\right)\right\|^{2}\]

    <ul>
      <li>where the first term in the difference is a linear combination of \(x t\) and \(x_{0}\) that depends on the variance schedule \(\beta_{t}\). The exact form of this function is not relevant for our purposes, but it can be found in <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic models</a>. The significance of the above proportion is that the most straightforward parameterization of \(\mu_{\theta}\) simply predicts the diffusion posterior mean. Importantly, the authors of <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic models</a> actually found that training \(\mu \theta\) to predict the noise component at any given timestep yields better results. In particular, let</li>
    </ul>

\[\boldsymbol{\mu}_{\theta}\left(\mathbf{x}_{t}, t\right)=\frac{1}{\sqrt{\alpha_{t}}}\left(\mathbf{x}_{t}-\frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}} \boldsymbol{\epsilon}_{\theta}\left(\mathbf{x}_{t}, t\right)\right)\]

    <ul>
      <li>where, \(\alpha_{t}:=1-\beta_{t} \quad\) and \(\quad \bar{\alpha}_{t}:=\prod_{s=1}^{t} \alpha_{s}\).</li>
    </ul>
  </li>
  <li>This leads to the following alternative loss function, which the authors of <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic models</a> found to lead to more stable training and better results:</li>
</ul>

\[L_{\text {simple }}(\theta):=\mathbb{E}_{t, \mathbf{x}_{0}, \epsilon}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\theta}\left(\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \boldsymbol{\epsilon}, t\right)\right\|^{2}\right]\]

<ul>
  <li>The authors of <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic models</a> also note connections of this formulation of Diffusion models to score-matching generative models based on Langevin dynamics. Indeed, it appears that Diffusion models and ScoreBased models may be two sides of the same coin, akin to the independent and concurrent development of wave-based quantum mechanics and matrix-based quantum mechanics revealing two equivalent formulations of the <a href="https://arxiv.org/abs/1907.05600">same phenomena</a>.</li>
</ul>

<h2 id="network-architecture-u-net-and-diffusion-transformer-dit">Network Architecture: U-Net and Diffusion Transformer (DiT)</h2>

<ul>
  <li>Let’s dive deep into the network architecture of a diffusion model. Note that the only requirement for the model is that its input and output dimensionality are identical. Specifically, the neural network needs to take in a noised image at a particular time step and return the predicted noise. The predicted noise here is a tensor that has the same size and resolution as the input image. Thus, these neural networks take in tensors and return tensors of the same shape.</li>
  <li>Given this restriction, it is perhaps unsurprising that image diffusion models are commonly implemented with U-Net-like architectures. Put simply, U-Net-based diffusion models are the most prevalent type of diffusion models.</li>
  <li>There are primarily two types of architectures used in diffusion models: U-Net and Diffusion Transformer (DiT). U-Net-based architectures are highly effective for image-related tasks due to their spatially structured convolutional operations, while Diffusion Transformers leverage self-attention mechanisms to capture long-range dependencies, making them versatile for various data types.</li>
</ul>

<h3 id="u-net-based-diffusion-models">U-Net-Based Diffusion Models</h3>

<ul>
  <li>
    <p>The neural net architecture that the authors of <a href="https://arxiv.org/abs/2006.11239">DDPM</a> used was <a href="https://arxiv.org/abs/1505.04597">U-Net</a>. Here are the key aspects of the U-Net architecture:</p>

    <ul>
      <li><strong>Encoder-Decoder Structure</strong>:
        <ul>
          <li>The network consists of a “bottleneck” layer in the middle of its architecture between the encoder and decoder.</li>
          <li>The encoder first encodes an image into a smaller hidden representation called the “bottleneck”.</li>
          <li>The decoder then decodes that hidden representation back into an actual image.</li>
        </ul>
      </li>
      <li><strong>Bottleneck Layer</strong>:
        <ul>
          <li>This bottleneck ensures the network learns only the most important information.</li>
          <li>It compresses the input data to focus on essential features, facilitating effective denoising and reconstruction.</li>
        </ul>
      </li>
      <li><strong>Skip Connections</strong>:
        <ul>
          <li>U-Net architecture includes skip connections between corresponding layers of the encoder and decoder.</li>
          <li>These connections help retain fine-grained details lost during downsampling in the encoder and reintroduce them during upsampling in the decoder.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="diffusion-transformer-dit">Diffusion Transformer (DiT)</h3>

<ul>
  <li>
    <p>In contrast to U-Net-based diffusion models, Diffusion Transformers (DiT), proposed in <a href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a>, employ a transformer-based architecture, characterized by self-attention mechanisms. Here are the key features of DiT:</p>

    <ul>
      <li><strong>Transformer-Based Architecture</strong>:
        <ul>
          <li>Utilizes layers of multi-head self-attention and feed-forward networks.</li>
          <li>Captures long-range dependencies and complex interactions within the data.</li>
        </ul>
      </li>
      <li><strong>Sequential and Attention Mechanisms</strong>:
        <ul>
          <li>Handles data as sequences, leveraging self-attention to weigh the importance of different parts of the input dynamically.</li>
          <li>Well-suited for tasks requiring the understanding of relationships over long distances in the input data.</li>
          <li>Can be adapted for various data types, not just images, due to its flexibility in data representation.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="comparison">Comparison</h3>

<h4 id="model-complexity-and-parameters">Model Complexity and Parameters</h4>

<ul>
  <li><strong>DiT</strong>:
    <ul>
      <li>Generally has a higher model complexity due to the quadratic scaling of self-attention with the input size.</li>
      <li>Requires significant computational resources for handling large sequences or high-resolution data.</li>
    </ul>
  </li>
  <li><strong>U-Net</strong>:
    <ul>
      <li>Model complexity is more manageable, as convolutional operations scale linearly with input size.</li>
      <li>Easier to train and requires fewer computational resources compared to transformers, especially for high-resolution image data.</li>
    </ul>
  </li>
</ul>

<h4 id="training-and-optimization">Training and Optimization</h4>

<ul>
  <li><strong>DiT</strong>:
    <ul>
      <li>Training can be more challenging due to the complexity of the attention mechanisms and the need for large-scale data to fully leverage its capabilities.</li>
      <li>Optimization techniques like learning rate schedules, gradient clipping, and advanced initialization methods are often necessary.</li>
    </ul>
  </li>
  <li><strong>U-Net</strong>:
    <ul>
      <li>Training is generally more straightforward due to the well-understood nature of convolutional operations and spatial processing.</li>
      <li>Beneficial for applications with limited computational resources or where rapid prototyping and iteration are required.</li>
    </ul>
  </li>
</ul>

<h3 id="reverse-process-of-u-net-based-diffusion-models">Reverse Process of U-Net-based Diffusion Models</h3>

<ul>
  <li>The path along the reverse process consists of many transformations under continuous conditional Gaussian distributions. At the end of the reverse process, recall that we are trying to produce an image, which is composed of integer pixel values. Therefore, we must devise a way to obtain discrete (log) likelihoods for each possible pixel value across all pixels.</li>
  <li>
    <p>The way that this is done is by setting the last transition in the reverse diffusion chain to an independent discrete decoder. To determine the likelihood of a given image \(x_0\) given \(x_{1}\), we first impose independence between the data dimensions:</p>

\[p_{\theta}\left(x_{0} \mid x_{1}\right)=\prod_{i=1}^{D} p_{\theta}\left(x_{0}^{i} \mid x_{1}^{i}\right)\]

    <ul>
      <li>where \(D\) is the dimensionality of the data and the superscript \(i\) indicates the extraction of one coordinate. The goal now is to determine how likely each integer value is for a given pixel given the distribution across possible values for the corresponding pixel in the slightly noised image at time \(t=1\) :</li>
    </ul>

\[\mathcal{N}\left(x ; \mu_{\theta}^{i}\left(x_{1}, 1\right), \sigma_{1}^{2}\right)\]

    <ul>
      <li>where the pixel distributions for \(t=1\) are derived from the below multivariate Gaussian whose diagonal covariance matrix allows us to split the distribution into a product of univariate Gaussians, one for each dimension of the data:</li>
    </ul>

\[\mathcal{N}\left(x ; \mu_{\theta}\left(x_{1}, 1\right), \sigma_{1}^{2} \mathbb{I}\right)=\prod_{i=1}^{D} \mathcal{N}\left(x ; \mu_{\theta}^{i}\left(x_{1}, 1\right), \sigma_{1}^{2}\right)\]
  </li>
  <li>
    <p>We assume that the images consist of integers in \(0,1, \ldots, 255\) (as standard RGB images do) which have been scaled linearly to \([-1,1]\). We then break down the real line into small “buckets”, where, for a given scaled pixel value \(x\), the bucket for that range is \([x-1 / 255, x+1 / 255]\). The probability of a pixel value \(x\), given the univariate Gaussian distribution of the corresponding pixel in \(x_1\), is the area under that univariate Gaussian distribution within the bucket centered at \(x\).</p>
  </li>
  <li>The figure below shows the area for each of these buckets with their probabilities for a mean-0 Gaussian which, in this context, corresponds to a distribution with an average pixel value of \(\frac{255}{2}\) (half brightness). The red curve represents the distribution of a specific pixel in the \(t=1\) image, and the areas give the probability of the corresponding pixel value in the \(t=0\) image.</li>
</ul>

<p><img src="../assets/diffusion-models/diff10.jpg" alt="" /></p>

<ul>
  <li>
    <p>Technical Note: The first and final buckets extend out to -inf and +inf to preserve total probability.</p>
  </li>
  <li>
    <p>Given a \(t=0\) pixel value for each pixel, the value of \(p_{\theta}\left(x_{0} \mid x_{1}\right)\) is simply their product. Succinctly, this process is succinctly encapsulated by the following equation:</p>

\[p_{\theta}\left(x_{0} \mid x_{1}\right)=\prod_{i=1}^{D} p_{\theta}\left(x_{0}^{i} \mid x_{1}^{i}\right)=\prod_{i=1}^{D} \int_{\delta_{-}\left(x_{0}^{i}\right)}^{\delta_{+}\left(x_{0}^{i}\right)} \mathcal{N}\left(x ; \mu_{\theta}^{i}\left(x_{1}, 1\right), \sigma_{1}^{2}\right) d x\]

    <ul>
      <li>where,</li>
    </ul>

\[\delta_{-}(x)= \begin{cases}-\infty &amp; x=-1 \\ x-\frac{1}{255} &amp; x&gt;-1\end{cases}\]

    <ul>
      <li>and</li>
    </ul>

\[\delta_{+}(x)= \begin{cases}\infty &amp; x=1 \\ x+\frac{1}{255} &amp; x&lt;1\end{cases}\]
  </li>
  <li>
    <p>Given this equation for \(p_{\theta}\left(x_{0} \mid x_{1}\right)\), we can calculate the final term of \(L_{v l b}\) which is not formulated as a \(\mathrm{KL}\) Divergence:</p>

\[L_{0}=-\log p_{\theta}\left(x_{0} \mid x_{1}\right)\]
  </li>
</ul>

<h3 id="reverse-process-of-dit-based-diffusion-models">Reverse Process of DiT-based Diffusion Models</h3>

<ul>
  <li>
    <p>In DiT-based diffusion models, the reverse process involves a series of denoising steps utilizing transformer architectures, specifically designed to capture the long-range dependencies in the data. Unlike U-Net-based models, which primarily leverage convolutional operations, DiT-based models employ self-attention mechanisms to model the interactions between different parts of the image more effectively.</p>
  </li>
  <li>
    <p>The reverse process starts with a highly noisy image and progressively refines it through a sequence of transformer blocks. At each timestep, the model predicts the noise component that needs to be subtracted from the current noisy image to move closer to the original image distribution. This iterative process can be described as follows:</p>

    <ul>
      <li>
        <p><strong>Input Representation:</strong> The noisy image at each timestep is first encoded into a latent representation using an initial linear projection layer. This encoded representation serves as the input to the transformer blocks.</p>
      </li>
      <li>
        <p><strong>Transformer Blocks:</strong> Each transformer block consists of multi-head self-attention layers followed by feed-forward networks. The self-attention mechanism allows the model to focus on different parts of the image, capturing global context and dependencies. The feed-forward networks further process this information to refine the noise prediction.</p>
      </li>
      <li>
        <p><strong>Noise Prediction:</strong> At each timestep, the output of the transformer blocks is used to predict the noise present in the current image. This predicted noise is then subtracted from the current noisy image to obtain a less noisy version of the image.</p>
      </li>
      <li>
        <p><strong>Iterative Denoising:</strong> This process is repeated iteratively, with each step producing a progressively denoised image. The final image, after all the steps, is expected to closely match the original image distribution.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>The objective function for DiT-based models is similar to that of U-Net-based models, aiming to minimize the difference between the predicted and actual noise components. The overall loss function can be expressed as:</p>

\[L_{\text {DiT}}(\theta):=\mathbb{E}_{t, \mathbf{x}_{0}, \epsilon}\left[\left\|\epsilon-\epsilon_{\theta}\left(\mathbf{x}_{t}, t\right)\right\|^{2}\right]\]
  </li>
  <li>
    <p>Here, \(\mathbf{x}_{t}\) represents the noisy image at timestep \(t\), and \(\epsilon_{\theta}\) denotes the noise predicted by the transformer-based model. By minimizing this loss, the model learns to accurately predict the noise at each timestep, leading to high-quality image generation through the reverse diffusion process.</p>
  </li>
</ul>

<h3 id="final-objective">Final Objective</h3>

<ul>
  <li>As mentioned in the last section, the authors of <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic models</a> found that predicting the noise component of an image at a given timestep produced the best results. Ultimately, they use the following objective:</li>
</ul>

\[L_{\text {simple }}(\theta):=\mathbb{E}_{t, \mathbf{x}_{0}, \epsilon}\left[\left\|\epsilon-\epsilon \theta\left(\sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon, t\right)\right\|^{2}\right]\]

<ul>
  <li>The training and sampling algorithms for our diffusion model therefore can be succinctly captured in the below table (from <a href="https://arxiv.org/pdf/2006.11239.pdf">source</a>):</li>
</ul>

<p><img src="../assets/diffusion-models/diff11.png" alt="" /></p>

<h3 id="summary">Summary</h3>

<ul>
  <li>In summary, U-Net-based diffusion models are the most prevalent type of diffusion models, particularly effective for image-related tasks due to their spatially structured convolutional architecture. They are simpler to train and computationally more efficient. The reverse process in U-Net-based models involves many transformations under continuous conditional Gaussian distributions and concludes with an independent discrete decoder to determine pixel values.</li>
  <li>On the other hand, Diffusion Transformers (DiT) leverage the power of transformers to handle a variety of data types, capturing long-range dependencies through attention mechanisms. They utilize a series of denoising steps with transformer blocks, employing self-attention mechanisms to effectively model interactions within the data. However, DiT models are more complex and resource-intensive. The reverse process in DiT-based models involves iterative denoising steps using transformer blocks to progressively refine the noisy image.</li>
  <li>The choice between these models depends on the specific requirements of the task, the nature of the data, and the available computational resources.</li>
</ul>

<h2 id="conditional-diffusion-models">Conditional Diffusion Models</h2>

<ul>
  <li>Conditional Diffusion Models (CDMs) are an extension of diffusion probabilistic models, where the generation process is conditioned on auxiliary information. This conditioning allows for more structured and controlled synthesis, enabling models to produce outputs that adhere to specific constraints or descriptions.</li>
  <li>Conditioning in diffusion models can be applied using various inputs, such as text (e.g., CLIP embeddings, transformers) or visual data (e.g., images, segmentation maps, depth maps). These inputs influence both the theoretical underpinnings and practical implementations of the models, enhancing their ability to generate outputs aligned with user-defined specifications.</li>
  <li>Early diffusion models relied on simple concatenation techniques for conditioning. However, modern architectures have adopted more sophisticated methods like cross-attention mechanisms, which significantly improve guidance effectiveness. Additionally, techniques such as classifier-free guidance and feature modulation further refine controllability, allowing models to better interpret conditioning signals. These advancements make CDMs powerful tools for diverse tasks, including text-to-image synthesis and guided image manipulation.</li>
</ul>

<h3 id="conditioning-mechanisms">Conditioning Mechanisms</h3>

<ul>
  <li>
    <p>Diffusion models, which iteratively denoise a Gaussian noise sample to generate an image, can be conditioned by modifying either the forward diffusion process, the reverse process, or both. Below are the primary methods used for conditioning:</p>

    <ul>
      <li><strong>Concatenation</strong>: Directly concatenating conditioning information to the input (e.g., concatenating a text embedding or image feature map to the input image tensor). This was widely used in earlier models such as SR3 (<a href="https://arxiv.org/abs/2104.07636">Saharia et al., 2021</a>) and Palette (<a href="https://arxiv.org/abs/2111.05826">Saharia et al., 2022</a>).</li>
      <li><strong>Cross-Attention</strong>: Using a transformer-based cross-attention mechanism to modulate the noise prediction process. This is commonly used in modern models like Imagen (<a href="https://arxiv.org/abs/2205.11487">Saharia et al., 2022</a>) and Stable Diffusion (<a href="https://arxiv.org/abs/2112.10752">Rombach et al., 2022</a>).</li>
      <li><strong>Adaptive Normalization (AdaGN, AdaIN)</strong>: Using conditioning information to modulate the mean and variance of intermediate activations.</li>
      <li><strong>Classifier Guidance</strong>: Using an external classifier to guide the reverse diffusion process.</li>
      <li><strong>Score-Based Guidance</strong>: Modifying the score function based on conditioning information.</li>
    </ul>
  </li>
  <li>
    <p>Below, we describe how these approaches work mathematically and their implementations.</p>
  </li>
</ul>

<h3 id="text-conditioning-in-diffusion-models">Text Conditioning in Diffusion Models</h3>

<ul>
  <li>Text conditioning in diffusion models typically involves leveraging text encoders such as CLIP, T5, or BERT to obtain a text embedding, which is then integrated into the diffusion model’s denoising network.</li>
</ul>

<h4 id="encoding-textual-information">Encoding Textual Information</h4>

<ul>
  <li>
    <p>A text encoder extracts a fixed-length embedding from an input text description. Suppose the input text is denoted as \(T\), the text encoder \(E_{text}\) produces an embedding:</p>

\[z_T = E_{text}(T) \in \mathbb{R}^{d_{text}}\]

    <ul>
      <li>where \(z_T\) is the resulting embedding vector.</li>
    </ul>
  </li>
</ul>

<h4 id="concatenation-vs-cross-attention-conditioning">Concatenation vs. Cross-Attention Conditioning</h4>

<ul>
  <li>Earlier models such as SR3 (<a href="https://arxiv.org/abs/2104.07636">Saharia et al., 2021</a>) and Palette (<a href="https://arxiv.org/abs/2111.05826">Saharia et al., 2022</a>) used direct concatenation of conditioning inputs with noise latents. However, modern models like Stable Diffusion and Imagen rely on cross-attention for more expressive conditioning.</li>
</ul>

<h5 id="cross-attention">Cross-Attention</h5>

<ul>
  <li>
    <p>A common method for integrating \(z_T\) into the U-Net-based denoiser is via cross-attention. If \(f_l\) represents the feature map at layer \(l\) of the U-Net, attention-modulated features are computed as:</p>

\[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d}} \right) V,\]

    <ul>
      <li>where
  \(Q = W_Q f_l, \quad K = W_K z_T, \quad V = W_V z_T.\)</li>
    </ul>
  </li>
  <li>
    <p>This allows the model to attend to relevant text features while generating an image.</p>
  </li>
</ul>

<h4 id="implementation-details-pytorch">Implementation Details (PyTorch)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">context_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_q</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_k</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_v</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'b i d, b j d -&gt; b i j'</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">'b i j, b j d -&gt; b i d'</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<h3 id="visual-conditioning-in-diffusion-models">Visual Conditioning in Diffusion Models</h3>

<ul>
  <li>Visual conditioning can be applied using images, segmentation maps, edge maps, or depth maps as conditioning inputs.</li>
</ul>

<h4 id="concatenation-based-conditioning">Concatenation-Based Conditioning</h4>

<ul>
  <li>A simple way to condition on an image is by concatenating it with the noise input at each timestep:
  \(x_t' = \text{concat}(x_t, C)\)
    <ul>
      <li>where \(x_t\) is the noisy image and \(C\) is the conditioning image. This method was prevalent in early models like SR3.</li>
    </ul>
  </li>
</ul>

<h4 id="feature-map-injection-via-cross-attention">Feature Map Injection via Cross-Attention</h4>

<ul>
  <li>More advanced methods use feature injection via cross-attention, as seen in Stable Diffusion and Imagen. Instead of concatenation, this method extracts feature maps from a pretrained encoder \(E_{img}\):</li>
</ul>

\[z_C = E_{img}(C) \in \mathbb{R}^{h \times w \times d}\]

<ul>
  <li>and injects these features at various U-Net layers via <a href="https://arxiv.org/abs/1709.07871">FiLM (Feature-wise Linear Modulation)</a>:</li>
</ul>

\[\gamma = W_\gamma(z_C), \quad \beta = W_\beta(z_C),\]

\[f_l' = \gamma \odot f_l + \beta.\]

<h4 id="implementation-details-pytorch-1">Implementation Details (PyTorch)</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FiLM</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">conditioning_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">conditioning_dim</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">conditioning_dim</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">conditioning</span><span class="p">):</span>
        <span class="n">gamma</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gamma</span><span class="p">(</span><span class="n">conditioning</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">beta</span><span class="p">(</span><span class="n">conditioning</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">beta</span>
</code></pre></div></div>

<h2 id="classifier-free-guidance">Classifier-Free Guidance</h2>

<h3 id="background-why-are-external-classifiers-needed-for-text-to-image-synthesis-using-diffusion-models">Background: Why Are External Classifiers Needed for Text-to-Image Synthesis Using Diffusion Models?</h3>

<ul>
  <li>Diffusion models when used for text-to-image synthesis produce high-quality and coherent images from textual descriptions. However, early implementations of diffusion-based text-to-image models often struggled with aligning generated images precisely with their corresponding textual descriptions. One method to improve this alignment is through classifier guidance, where an external classifier is used to steer the diffusion process. The introduction of an external classifier provided an initial improvement in text-to-image synthesis by guiding diffusion models towards more accurate outputs.</li>
</ul>

<h4 id="the-need-for-external-classifiers">The Need for External Classifiers</h4>

<ul>
  <li><strong>Conditional Control</strong>: Early diffusion models generated images by iteratively refining a noise vector but lacked a robust mechanism to ensure strict adherence to the input text description.</li>
  <li><strong>Gradient-Based Steering</strong>: External classifiers enabled gradient-based guidance by evaluating intermediate diffusion steps and providing directional corrections to better match the conditioning input.</li>
  <li><strong>Enhancing Specificity</strong>: Without an external classifier, models sometimes produced images that, while visually plausible, did not accurately capture the semantics of the input text. The classifier provided a corrective mechanism to reinforce textual consistency.</li>
  <li><strong>Limitations of Pure Unconditional Diffusion Models</strong>: Unconditional diffusion models trained without any conditioning struggled to generate diverse yet accurate samples aligned with a given input prompt. External classifiers were introduced to bridge this gap by explicitly providing additional constraints during inference.</li>
</ul>

<h3 id="key-papers-introducing-external-classifiers-for-text-to-image-synthesis-using-diffusion-models">Key Papers Introducing External Classifiers for Text-to-Image Synthesis Using Diffusion Models</h3>

<ul>
  <li>
    <p>Several papers introduced and explored the use of external classifiers for guiding text-to-image synthesis in diffusion models:</p>

    <ul>
      <li><a href="https://arxiv.org/abs/2105.05233">Dhariwal and Nichol (2021): “Diffusion Models Beat GANs on Image Synthesis”</a>
        <ul>
          <li>This paper introduced classifier guidance as a mechanism to improve the fidelity and control of image generation in diffusion models.</li>
          <li>The approach leveraged an external classifier trained to predict image labels, which was then used to modify the sampling process by influencing the reverse diffusion steps.</li>
          <li>Mathematically, the classifier-based guidance modifies the score function as:
\(\nabla_x \log p(y \| x) \approx \frac{\partial f_y(x)}{\partial x},\)
where \(f_y(x)\) represents the classifier’s output logits for class \(y\) given an image \(x\).</li>
        </ul>
      </li>
      <li><a href="https://arxiv.org/abs/2207.12598">Ho et al. (2021): “Classifier-Free Diffusion Guidance”</a>
        <ul>
          <li>This work proposed classifier-free guidance as an alternative to classifier-based guidance, enabling the model to learn both conditioned and unconditioned paths internally without requiring an external classifier.</li>
          <li>It showed that classifier-free guidance could achieve competitive or superior results compared to classifier-based methods while reducing architectural complexity.</li>
        </ul>
      </li>
      <li><a href="https://arxiv.org/abs/2204.06125">Ramesh et al. (2022): “Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL·E 2)”</a>
        <ul>
          <li>This paper incorporated a CLIP-based approach to improve text-to-image alignment without directly using an external classifier.</li>
          <li>Instead of an explicit classifier, a pretrained CLIP model was used to guide the image generation by matching textual and visual embeddings.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="how-classifier-free-guidance-works">How Classifier-Free Guidance Works</h3>

<ul>
  <li>Compared to using external classifiers, classifier-free guidance has since emerged as a more efficient and flexible alternative, eliminating the need for additional classifiers while maintaining or exceeding the performance of classifier-based methods. Put simply, classifier-free guidance provides an alternative to external classifier-based guidance by training the model to handle both conditioned and unconditioned paths internally.</li>
  <li>By incorporating a dual-path training approach and an adjustable guidance scale, classifier-free guidance enhances fidelity, efficiency, and control in text-to-image synthesis, making it a preferred choice in modern generative models.</li>
</ul>

<h4 id="dual-training-path">Dual Training Path</h4>

<ol>
  <li><strong>Conditioned and Unconditioned Paths</strong>: During training, the model learns two distinct paths:
    <ul>
      <li>A conditioned path, where the model is trained to generate outputs aligned with a given text description.</li>
      <li>An unconditioned path, where the model generates outputs without any guidance.</li>
    </ul>
  </li>
  <li><strong>Random Conditioning Dropout</strong>: To encourage robustness, the model is trained with random conditioning dropout, where a fraction of inputs are deliberately trained without text guidance.</li>
  <li><strong>Self-Guidance Mechanism</strong>: By learning both paths simultaneously, the model can interpolate between conditioned and unconditioned generations, allowing it to effectively control guidance strength during inference.</li>
</ol>

<h4 id="equations">Equations</h4>

<ol>
  <li><strong>Training Objective</strong>:
    <ul>
      <li>The model learns two score functions:
\(\epsilon_\theta(x_t, y) \text{ and } \epsilon_\theta(x_t, \varnothing),\)
        <ul>
          <li>where:
            <ul>
              <li>\(x_t\) is the noised image at time step \(t\),</li>
              <li>\(y\) represents the conditioning input (e.g., text prompt), and</li>
              <li>\(\varnothing\) represents the unconditioned input.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Guidance During Inference</strong>:
    <ul>
      <li>Classifier-free guidance is implemented as:
\(\tilde{\epsilon}_\theta(x_t, y) = (1 + \gamma) \epsilon_\theta(x_t, y) - \gamma \epsilon_\theta(x_t, \varnothing),\)
        <ul>
          <li>where \(\gamma\) is the guidance scale controlling adherence to the conditioning input.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Effect of Guidance Scale</strong>:
    <ul>
      <li>When \(\gamma = 0\), the model behaves as an unconditional generator.</li>
      <li>When \(\gamma\) is increased, the generated output aligns more closely with the text condition.</li>
    </ul>
  </li>
</ol>

<h3 id="benefits-of-classifier-free-guidance">Benefits of Classifier-Free Guidance</h3>

<ol>
  <li><strong>Eliminates the Need for an External Classifier</strong>
    <ul>
      <li>Traditional classifier-based guidance requires a separately trained classifier, adding complexity to both training and inference.</li>
      <li>Classifier-free guidance removes this dependency, simplifying the overall architecture while maintaining strong performance.</li>
    </ul>
  </li>
  <li><strong>Improved Sample Quality</strong>
    <ul>
      <li>External classifiers introduce additional noise and potential misalignment between the classifier and the generative model.</li>
      <li>Classifier-free guidance directly integrates the conditioning within the diffusion process, leading to more natural and coherent outputs.</li>
    </ul>
  </li>
  <li><strong>Reduced Computational Cost</strong>
    <ul>
      <li>Training and utilizing an external classifier increases the computational burden.</li>
      <li>Classifier-free guidance eliminates the need for additional model components, streamlining both training and inference.</li>
    </ul>
  </li>
  <li><strong>Enhanced Generalization and Robustness</strong>
    <ul>
      <li>Classifier-based methods can be prone to adversarial vulnerabilities and overfitting to specific datasets.</li>
      <li>Classifier-free approaches allow the diffusion model to generalize better across different conditioning signals and input variations.</li>
    </ul>
  </li>
  <li><strong>Flexibility and Real-Time Control</strong>
    <ul>
      <li>Classifier-free guidance allows for dynamic adjustment of the guidance scale \(\gamma\) at inference time, providing fine-tuned control over generation quality and diversity.</li>
      <li>Users can experiment with different \(\gamma\) values without retraining the model, unlike classifier-based methods where the external classifier’s influence is fixed.</li>
    </ul>
  </li>
</ol>

<h2 id="prompting-guidance">Prompting Guidance</h2>

<ul>
  <li>Crafting effective prompts is crucial for generating high-quality and relevant outputs using diffusion models. This guide is divided into two main sections: (i) Prompting for Text-to-Image models and (ii) Prompting for Text-to-Video models.</li>
</ul>

<h3 id="prompting-text-to-image-models">Prompting Text-to-Image Models</h3>

<ul>
  <li>Text-to-image models, such as Stable Diffusion, DALL-E, and Imagen, translate textual descriptions into visual outputs. The success of a prompt depends on how well it describes the desired image in a structured, caption-like format. Below are the key considerations and techniques for crafting effective prompts for text-to-image generation.</li>
</ul>

<h4 id="key-prompting-guidelines">Key Prompting Guidelines</h4>

<ol>
  <li><strong>Phrase Your Prompt as an Image Caption</strong>:
    <ul>
      <li>Avoid conversational language or commands. Instead, describe the desired image with concise, clear details as you would in an image caption.</li>
      <li>Example: <em>“Realistic photo of a snowy mountain range under a clear blue sky, with sunlight casting long shadows.”</em></li>
    </ul>
  </li>
  <li><strong>Structure Your Prompt Using the Formula</strong>:
    <ul>
      <li><strong>[Subject] in [Environment], [Optional Pose/Position], [Optional Lighting], [Optional Camera Position/Framing], [Optional Style/Medium]</strong>.</li>
      <li>Example: <em>“A golden retriever playing in a grassy park during sunset, photorealistic, warm lighting.”</em></li>
    </ul>
  </li>
  <li><strong>Character Limit</strong>:
    <ul>
      <li>Prompts must not exceed <strong>1024 characters</strong>. Place less important details near the end.</li>
    </ul>
  </li>
  <li><strong>Avoid Negation Words</strong>:
    <ul>
      <li>Do not use words like “no,” “not,” or “without.” For example, the prompt <em>“a fruit basket with no bananas”</em> may result in bananas being included. Instead, use <strong>negative prompts</strong>:
        <ul>
          <li>Example: <code class="language-plaintext highlighter-rouge">Prompt: A fruit basket with apples and oranges.</code><br />
<code class="language-plaintext highlighter-rouge">Negative Prompt: Bananas.</code></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Refinement Techniques</strong>:
    <ul>
      <li>Use a consistent <strong>seed value</strong> to test prompt variations, iterating with small changes to understand how each affects the output.</li>
      <li>Once satisfied with a prompt, generate variations by running the same prompt with different seed values.</li>
    </ul>
  </li>
</ol>

<h4 id="example-prompts-for-text-to-image-models">Example Prompts for Text-to-Image Models</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Use Case</strong></th>
<th class="tg-hcenter-valign-first"><strong>Prompt</strong></th>
<th class="tg-hcenter-valign-second"><strong>Negative Prompt</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Stock Photo</td>
<td class="tg-tleft-valign-first"><em>"Realistic editorial photo of a teacher standing at a blackboard with a warm smile."</em></td>
<td class="tg-tleft-valign-second"><em>"Crossed arms."</em></td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Story Illustration</td>
<td class="tg-tleft-valign-first"><em>"Whimsical storybook illustration: a knight in armor kneeling before a glowing sword."</em></td>
<td class="tg-tleft-valign-second"><em>"Cartoonish style."</em></td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Cinematic Landscape</td>
<td class="tg-tleft-valign-first"><em>"Drone view of a dark river winding through a stark Icelandic landscape, cinematic quality."</em></td>
<td class="tg-tleft-valign-second">None</td>
</tr>
</tbody>
</table>
</div>

<h3 id="prompting-text-to-video-models">Prompting Text-to-Video Models</h3>

<ul>
  <li>Text-to-video models extend text-to-image capabilities to temporal domains, generating coherent sequences of frames based on textual prompts. These models use additional techniques, such as temporal embeddings, to capture motion and transitions over time.</li>
</ul>

<h4 id="key-prompting-guidelines-1">Key Prompting Guidelines</h4>

<ol>
  <li><strong>Phrase Your Prompt as a Video Summary</strong>:
    <ul>
      <li>Describe the video sequence as if summarizing its content, focusing on the subject, action, and environment.</li>
      <li>Example: <em>“A time-lapse of a sunflower blooming in a sunny garden. Vibrant colors, cinematic lighting.”</em></li>
    </ul>
  </li>
  <li><strong>Include Camera Movement for Dynamic Outputs</strong>:
    <ul>
      <li>Add camera movement descriptions (e.g., dolly shot, aerial view) at the start or end of the prompt for optimal results.</li>
      <li>Example: <em>“Arc shot of a basketball spinning on a finger in slow motion. Cinematic, sharp focus, 4K resolution.”</em></li>
    </ul>
  </li>
  <li><strong>Character Limit</strong>:
    <ul>
      <li>Like text-to-image prompts, video prompts must not exceed <strong>1024 characters</strong>.</li>
    </ul>
  </li>
  <li><strong>Avoid Negation Words</strong>:
    <ul>
      <li>Use negative prompts to exclude unwanted elements, similar to text-to-image generation.</li>
    </ul>
  </li>
  <li><strong>Refinement Techniques</strong>:
    <ul>
      <li>Experiment with different camera movements, action descriptions, or lighting effects to improve output consistency and realism.</li>
    </ul>
  </li>
</ol>

<h3 id="camera-movements">Camera Movements</h3>

<ul>
  <li>In video prompts, describing camera motion adds dynamic perspectives to the generated sequence. Below is a reference table of common camera movements and their suggested keywords:</li>
</ul>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Camera Movement</strong></th>
<th class="tg-hcenter-valign-first"><strong>Suggested Keywords</strong></th>
<th class="tg-hcenter-valign-second"><strong>Definition</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Aerial Shot</td>
<td class="tg-tleft-valign-first"><em>aerial shot, drone shot, first-person view (FPV)</em></td>
<td class="tg-tleft-valign-second">A shot taken from above, often from a drone or aircraft.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Arc Shot</td>
<td class="tg-tleft-valign-first"><em>arc shot, 360-degree shot, orbit shot</em></td>
<td class="tg-tleft-valign-second">Camera moves in a circular path around a central point/object.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Clockwise Rotation</td>
<td class="tg-tleft-valign-first"><em>camera rotates clockwise, clockwise rolling shot</em></td>
<td class="tg-tleft-valign-second">Camera rotates in a clockwise direction.</td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Dolly In</td>
<td class="tg-tleft-valign-first"><em>dolly in, camera moves forward</em></td>
<td class="tg-tleft-valign-second">Camera moves forward.</td>
</tr>
</tbody>
</table>
</div>

<h4 id="example-prompts-for-text-to-video-models">Example Prompts for Text-to-Video Models</h4>

<div align="center">
<table class="tg">
<thead>
<tr>
<th class="tg-hcenter-valign-first"><strong>Use Case</strong></th>
<th class="tg-hcenter-valign-first"><strong>Prompt</strong></th>
<th class="tg-hcenter-valign-second"><strong>Negative Prompt</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td class="tg-tleft-valign-first">Food Advertisement</td>
<td class="tg-tleft-valign-first"><em>"Cinematic dolly shot of a juicy cheeseburger with melting cheese, fries, and a cola on a diner table."</em></td>
<td class="tg-tleft-valign-second"><em>"Messy table."</em></td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Product Showcase</td>
<td class="tg-tleft-valign-first"><em>"Arc shot of a luxury wristwatch on a glass display, under studio lighting, with a blurred background."</em></td>
<td class="tg-tleft-valign-second"><em>"Low resolution."</em></td>
</tr>
<tr>
<td class="tg-tleft-valign-first">Nature Scene</td>
<td class="tg-tleft-valign-first"><em>"Aerial shot of a waterfall cascading through a dense forest. Soft lighting, 4K resolution."</em></td>
<td class="tg-tleft-valign-second">None</td>
</tr>
</tbody>
</table>
</div>

<h3 id="summary-1">Summary</h3>

<h4 id="text-to-image">Text-to-Image</h4>

<ul>
  <li>For text-to-image tasks, focus on describing the subject and its environment with optional details like lighting, style, and camera position. Use clear, concise descriptions structured like image captions.</li>
</ul>

<h4 id="text-to-video">Text-to-Video</h4>

<ul>
  <li>
    <p>For text-to-video tasks, describe the sequence as a whole, including subject actions, camera movements, and temporal transitions. Camera motion plays a critical role in adding dynamic elements to the video output.</p>
  </li>
  <li>
    <p>Both types of prompting require careful attention to phrasing and refinement to achieve optimal results. By iterating and experimenting with different seeds and negative prompts, you can generate visually stunning and contextually accurate outputs tailored to your needs.</p>
  </li>
</ul>

<h2 id="diffusion-models-in-pytorch">Diffusion Models in PyTorch</h2>

<h3 id="implementing-the-original-paper">Implementing the original paper</h3>

<ul>
  <li>Let’s go over the original <a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models (DDPMs) paper by Ho et al.,2020</a> and implement it step by step based on <a href="https://github.com/lucidrains/denoising-diffusion-pytorch">Phil Wang’s implementation</a> and <a href="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion by Hugging Face</a> which are both based off the <a href="https://github.com/hojonathanho/diffusion">original implementation</a>.</li>
</ul>

<p><img src="../assets/diffusion-models/20.jpg" alt="" /></p>

<h4 id="pre-requisites-setup-and-importing-libraries">Pre-requisites: Setup and Importing Libraries</h4>

<ul>
  <li>Let’s start with the setup and importing all the required libraries:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s">'assets/78_annotated-diffusion/ddpm_paper.png'</span><span class="p">)</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">einops</span> <span class="n">datasets</span> <span class="n">matplotlib</span> <span class="n">tqdm</span>

<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">inspect</span> <span class="kn">import</span> <span class="n">isfunction</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">einsum</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
</code></pre></div></div>

<h4 id="helper-functions">Helper functions</h4>

<ul>
  <li>Now let’s implement the neural network we have looked at earlier. First we start with a few helper functions.</li>
  <li>Most notably, we define <code class="language-plaintext highlighter-rouge">Residual</code> class which will add the input to the output of a particular function. That is, it adds a residual connection to a particular function.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">exists</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>

<span class="k">def</span> <span class="nf">default</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">val</span>
    <span class="k">return</span> <span class="n">d</span><span class="p">()</span> <span class="k">if</span> <span class="n">isfunction</span><span class="p">(</span><span class="n">d</span><span class="p">)</span> <span class="k">else</span> <span class="n">d</span>

<span class="k">class</span> <span class="nc">Residual</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">Upsample</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">ConvTranspose2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Downsample</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Note: the parameters of the neural network are shared across time (noise level).</li>
  <li>Thus, for the neural network to keep track of which time step (noise level) it is on, the authors used sinusoidal position embeddings to encode \(t\).</li>
  <li>The <code class="language-plaintext highlighter-rouge">SinusoidalPositionEmbeddings</code> class, that we have defined below, takes a tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size,1)</code> as input or the noise levels in a batch.</li>
  <li>It will then turn this input tensor into a tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, dim)</code> where <code class="language-plaintext highlighter-rouge">dim$</code> is the dimensionality of the position embeddings.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SinusoidalPositionEmbeddings</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">time</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">device</span>
        <span class="n">half_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">half_dim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">half_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="n">embeddings</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">time</span><span class="p">[:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">embeddings</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">embeddings</span><span class="p">.</span><span class="n">sin</span><span class="p">(),</span> <span class="n">embeddings</span><span class="p">.</span><span class="n">cos</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embeddings</span>
</code></pre></div></div>

<h4 id="model-core-resnet-or-convnext">Model Core: ResNet or ConvNeXT</h4>

<ul>
  <li>Now we will look at the meat or the core part of our U-Net model. The original <a href="https://arxiv.org/abs/2006.11239">DDPM</a> authors employed a Wide ResNet block via <a href="https://arxiv.org/abs/1605.07146">Zagoruyko et al., 2016</a>, however <a href="https://github.com/lucidrains/denoising-diffusion-pytorch">Phil Wang</a> has also introduced support for ConvNeXT block via <a href="https://arxiv.org/abs/2201.03545">Liu et al., 2022</a>.</li>
  <li>You are free to choose either or in your final U-Net architecture but both are provided below:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Block</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="mi">8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">SiLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">scale_shift</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="n">scale_shift</span><span class="p">):</span>
            <span class="n">scale</span><span class="p">,</span> <span class="n">shift</span> <span class="o">=</span> <span class="n">scale_shift</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="n">scale</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">shift</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span> <span class="nc">ResnetBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""https://arxiv.org/abs/1512.03385"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">SiLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">None</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">block1</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">block2</span> <span class="o">=</span> <span class="n">Block</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">res_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="n">dim_out</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">exists</span><span class="p">(</span><span class="n">time_emb</span><span class="p">):</span>
            <span class="n">time_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">time_emb</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">time_emb</span><span class="p">,</span> <span class="s">"b c -&gt; b c 1 1"</span><span class="p">)</span> <span class="o">+</span> <span class="n">h</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">block2</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">res_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="k">class</span> <span class="nc">ConvNextBlock</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""https://arxiv.org/abs/2201.03545"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="n">time_emb_dim</span><span class="p">)</span>
            <span class="k">else</span> <span class="bp">None</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">ds_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">norm</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span> <span class="o">*</span> <span class="n">mult</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim_out</span> <span class="o">*</span> <span class="n">mult</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim_out</span> <span class="o">*</span> <span class="n">mult</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">res_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">dim</span> <span class="o">!=</span> <span class="n">dim_out</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time_emb</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">ds_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">exists</span><span class="p">(</span><span class="n">time_emb</span><span class="p">):</span>
            <span class="n">condition</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">time_emb</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="s">"b c -&gt; b c 1 1"</span><span class="p">)</span>

        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">net</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">res_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="attention">Attention</h4>

<ul>
  <li>Next, we will look into defining the attention module which was added between the convolutional blocks in <a href="https://arxiv.org/abs/2006.11239">DDPM</a>.</li>
  <li><a href="https://github.com/lucidrains/denoising-diffusion-pytorch">Phil Wang</a> added two variants of attention, a normal multi-headed self-attention from the original <a href="https://arxiv.org/abs/1706.03762">Transformer paper (Vaswani et al.,2017)</a>, and linear attention variant <a href="https://arxiv.org/abs/1812.01243">(Shen et al., 2018)</a>.
    <ul>
      <li>Linear attention variant’s time and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim_head</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">dim_head</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">*</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s">"b (h c) x y -&gt; b h c (x y)"</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">heads</span><span class="p">),</span> <span class="n">qkv</span>
        <span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>

        <span class="n">sim</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span><span class="s">"b h d i, b h d j -&gt; b h i j"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">sim</span> <span class="o">-</span> <span class="n">sim</span><span class="p">.</span><span class="n">amax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">sim</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span><span class="s">"b h i j, b h d j -&gt; b h i d"</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s">"b h (x y) d -&gt; b (h d) x y"</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">LinearAttention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dim_head</span><span class="o">=</span><span class="mi">32</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">dim_head</span><span class="o">**-</span><span class="mf">0.5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">*</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">to_qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">to_out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> 
                                    <span class="n">nn</span><span class="p">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s">"b (h c) x y -&gt; b h c (x y)"</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">heads</span><span class="p">),</span> <span class="n">qkv</span>
        <span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"b h d n, b h e n -&gt; b h d e"</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">"b h d e, b h d n -&gt; b h e n"</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="s">"b h c (x y) -&gt; b (h c) x y"</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">heads</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">w</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">to_out</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><a href="https://arxiv.org/abs/2006.11239">DDPM</a> then adds group normalization to interleave the convolutional/attention layers of the U-Net architecture.</li>
  <li>Below, the <code class="language-plaintext highlighter-rouge">PreNorm</code> class will apply group normalization before the attention layer.
    <ul>
      <li>Note, there has been a <a href="https://tnq177.github.io/data/transformers_without_tears.pdf">debate</a> about whether groupnorm is better to be applied before or after attention in Transformers.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PreNorm</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fn</span> <span class="o">=</span> <span class="n">fn</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="overall-network">Overall network</h4>

<ul>
  <li>Now that we have all the building blocks of the neural network (ResNet/ConvNeXT blocks, attention, positional embeddings, group norm), lets define our entire neural network.</li>
  <li>The task of this neural network is to take in a batch of noisy images and their noise levels and then to output the noise added to the input.</li>
  <li>The network takes a batch of noisy images of shape <code class="language-plaintext highlighter-rouge">(batch_size, num_channels, height, width)</code> and a batch of noise levels of shape <code class="language-plaintext highlighter-rouge">(batch_size, 1)</code> as input, and returns a tensor of shape <code class="language-plaintext highlighter-rouge">(batch_size, num_channels, height, width)</code>.</li>
  <li>The network is built up as follows: <a href="https://huggingface.co/blog/annotated-diffusion">(source)</a>
    <ul>
      <li>first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels</li>
      <li>next, a sequence of downsampling stages are applied.
        <ul>
          <li>Each downsampling stage consists of two ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + a downsample operation</li>
        </ul>
      </li>
      <li>at the middle of the network, again ResNet or ConvNeXT blocks are applied, interleaved with attention</li>
      <li>next, a sequence of upsampling stages are applied.
        <ul>
          <li>Each upsampling stage consists of two ResNet/ConvNeXT blocks + groupnorm + attention + residual connection + an upsample operation</li>
        </ul>
      </li>
      <li>finally, a ResNet/ConvNeXT block followed by a convolutional layer is applied.</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Unet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dim</span><span class="p">,</span>
        <span class="n">init_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">out_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_mults</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
        <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">with_time_emb</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">resnet_block_groups</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">use_convnext</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">convnext_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># determine dimensions
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">channels</span> <span class="o">=</span> <span class="n">channels</span>

        <span class="n">init_dim</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">init_dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">init_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">init_dim</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

        <span class="n">dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">init_dim</span><span class="p">,</span> <span class="o">*</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">m</span><span class="p">:</span> <span class="n">dim</span> <span class="o">*</span> <span class="n">m</span><span class="p">,</span> <span class="n">dim_mults</span><span class="p">)]</span>
        <span class="n">in_out</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">dims</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
        
        <span class="k">if</span> <span class="n">use_convnext</span><span class="p">:</span>
            <span class="n">block_klass</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ConvNextBlock</span><span class="p">,</span> <span class="n">mult</span><span class="o">=</span><span class="n">convnext_mult</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">block_klass</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">ResnetBlock</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">resnet_block_groups</span><span class="p">)</span>

        <span class="c1"># time embeddings
</span>        <span class="k">if</span> <span class="n">with_time_emb</span><span class="p">:</span>
            <span class="n">time_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">4</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">SinusoidalPositionEmbeddings</span><span class="p">(</span><span class="n">dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">GELU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">time_dim</span><span class="p">,</span> <span class="n">time_dim</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">time_dim</span> <span class="o">=</span> <span class="bp">None</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">time_mlp</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="c1"># layers
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">downs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ups</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">([])</span>
        <span class="n">num_resolutions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">in_out</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">in_out</span><span class="p">):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">ind</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_resolutions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="bp">self</span><span class="p">.</span><span class="n">downs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">block_klass</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="n">block_klass</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="n">Residual</span><span class="p">(</span><span class="n">PreNorm</span><span class="p">(</span><span class="n">dim_out</span><span class="p">,</span> <span class="n">LinearAttention</span><span class="p">(</span><span class="n">dim_out</span><span class="p">))),</span>
                        <span class="n">Downsample</span><span class="p">(</span><span class="n">dim_out</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">(),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">mid_dim</span> <span class="o">=</span> <span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mid_block1</span> <span class="o">=</span> <span class="n">block_klass</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mid_attn</span> <span class="o">=</span> <span class="n">Residual</span><span class="p">(</span><span class="n">PreNorm</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">Attention</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">)))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">mid_block2</span> <span class="o">=</span> <span class="n">block_klass</span><span class="p">(</span><span class="n">mid_dim</span><span class="p">,</span> <span class="n">mid_dim</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span> <span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_out</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">in_out</span><span class="p">[</span><span class="mi">1</span><span class="p">:])):</span>
            <span class="n">is_last</span> <span class="o">=</span> <span class="n">ind</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">num_resolutions</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="bp">self</span><span class="p">.</span><span class="n">ups</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span>
                    <span class="p">[</span>
                        <span class="n">block_klass</span><span class="p">(</span><span class="n">dim_out</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="n">block_klass</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">dim_in</span><span class="p">,</span> <span class="n">time_emb_dim</span><span class="o">=</span><span class="n">time_dim</span><span class="p">),</span>
                        <span class="n">Residual</span><span class="p">(</span><span class="n">PreNorm</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span> <span class="n">LinearAttention</span><span class="p">(</span><span class="n">dim_in</span><span class="p">))),</span>
                        <span class="n">Upsample</span><span class="p">(</span><span class="n">dim_in</span><span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_last</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="n">Identity</span><span class="p">(),</span>
                    <span class="p">]</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="n">out_dim</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">out_dim</span><span class="p">,</span> <span class="n">channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">final_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">block_klass</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">time</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">init_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">time_mlp</span><span class="p">(</span><span class="n">time</span><span class="p">)</span> <span class="k">if</span> <span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">time_mlp</span><span class="p">)</span> <span class="k">else</span> <span class="bp">None</span>

        <span class="n">h</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># downsample
</span>        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">downsample</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">downs</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">h</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># bottleneck
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mid_block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mid_attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">mid_block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="c1"># upsample
</span>        <span class="k">for</span> <span class="n">block1</span><span class="p">,</span> <span class="n">block2</span><span class="p">,</span> <span class="n">attn</span><span class="p">,</span> <span class="n">upsample</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">ups</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="n">pop</span><span class="p">()),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">block1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">block2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">attn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">upsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">final_conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Note: by default, the noise predictor uses ConvNeXT blocks (as <code class="language-plaintext highlighter-rouge">use_convnext</code> is set to True) and position embeddings are added (as <code class="language-plaintext highlighter-rouge">with_time_emb</code> is set to True).</li>
</ul>

<h4 id="forward-diffusion">Forward diffusion</h4>

<ul>
  <li>Now lets take a look at the forward diffusion process. Remember forward diffusion process will gradually add noise to an image withing a number of time steps \(T\).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cosine_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.008</span><span class="p">):</span>
    <span class="s">"""
    cosine schedule as proposed in https://arxiv.org/abs/2102.09672
    """</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">timesteps</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="n">steps</span><span class="p">)</span>
    <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cos</span><span class="p">(((</span><span class="n">x</span> <span class="o">/</span> <span class="n">timesteps</span><span class="p">)</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">alphas_cumprod</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">/</span> <span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="p">,</span> <span class="n">beta_end</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">quadratic_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta_start</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta_end</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="k">def</span> <span class="nf">sigmoid_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="n">beta_start</span> <span class="o">=</span> <span class="mf">0.0001</span>
    <span class="n">beta_end</span> <span class="o">=</span> <span class="mf">0.02</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">betas</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta_end</span> <span class="o">-</span> <span class="n">beta_start</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta_start</span>
</code></pre></div></div>

<ul>
  <li>To start with, let’s use the linear schedule for \(T=200\) time steps and define the various variables from the \(\beta_t\) which we will need, such as the cumulative product of the variances \(\bar{\alpha}_t\)</li>
  <li>Each of the variables below are just 1-dimensional tensors, storing values from \(t\) to \(T\).</li>
  <li>Importantly, we also define an extract function, which will allow us to extract the appropriate \(t\) index for a batch of indices. <a href="https://huggingface.co/blog/annotated-diffusion">(source)</a></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">timesteps</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># define beta schedule
</span><span class="n">betas</span> <span class="o">=</span> <span class="n">linear_beta_schedule</span><span class="p">(</span><span class="n">timesteps</span><span class="o">=</span><span class="n">timesteps</span><span class="p">)</span>

<span class="c1"># define alphas 
</span><span class="n">alphas</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">betas</span>
<span class="n">alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">alphas_cumprod_prev</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">sqrt_recip_alphas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">alphas</span><span class="p">)</span>

<span class="c1"># calculations for diffusion q(x_t \| x_{t-1}) and others
</span><span class="n">sqrt_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alphas_cumprod</span><span class="p">)</span>
<span class="n">sqrt_one_minus_alphas_cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)</span>

<span class="c1"># calculations for posterior q(x_{t-1} \| x_t, x_0)
</span><span class="n">posterior_variance</span> <span class="o">=</span> <span class="n">betas</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod_prev</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alphas_cumprod</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_shape</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">a</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">cpu</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">*</span><span class="p">((</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))).</span><span class="n">to</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Now let’s take an image and illustrate how noise is added at each time step of the diffusion process to the PyTorch tensors:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="s">'http://images.cocodataset.org/val2017/000000039769.jpg'</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="n">raw</span><span class="p">)</span>
<span class="n">image</span>
</code></pre></div></div>

<p><img src="../assets/diffusion-models/22.jpg" alt="" /></p>

<ul>
  <li>We first normalize images by dividing by 255 (such that they are in the <code class="language-plaintext highlighter-rouge">[0,1]</code> range), and then make sure they are in the <code class="language-plaintext highlighter-rouge">[-1, 1]</code> range.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">ToTensor</span><span class="p">,</span> <span class="n">Lambda</span><span class="p">,</span> <span class="n">ToPILImage</span><span class="p">,</span> <span class="n">CenterCrop</span><span class="p">,</span> <span class="n">Resize</span>

<span class="n">image_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
    <span class="n">Resize</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
    <span class="n">CenterCrop</span><span class="p">(</span><span class="n">image_size</span><span class="p">),</span>
    <span class="n">ToTensor</span><span class="p">(),</span> <span class="c1"># turn into Numpy array of shape HWC, divide by 255
</span>    <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
    
<span class="p">])</span>

<span class="n">x_start</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_start</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Output</span><span class="p">:</span>
<span class="o">----------------------------------------------------------------------------------------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>We also define the reverse transform, which takes in a PyTorch tensor containing values in <code class="language-plaintext highlighter-rouge">[-1, 1]</code> and turn them back into a PIL image:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">reverse_transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
     <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span>
     <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span> <span class="c1"># CHW to HWC
</span>     <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span> <span class="o">*</span> <span class="mf">255.</span><span class="p">),</span>
     <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="p">.</span><span class="n">numpy</span><span class="p">().</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)),</span>
     <span class="n">ToPILImage</span><span class="p">(),</span>
<span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>Let’s run an example and see what it produces:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reverse_transform</span><span class="p">(</span><span class="n">x_start</span><span class="p">.</span><span class="n">squeeze</span><span class="p">())</span>
</code></pre></div></div>

<p><img src="../assets/diffusion-models/23.jpg" alt="" /></p>

<ul>
  <li>We can now define the forward diffusion process as in the paper:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward diffusion (using the nice property)
</span><span class="k">def</span> <span class="nf">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>

    <span class="n">sqrt_alphas_cumprod_t</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="n">sqrt_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span>
        <span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x_start</span><span class="p">.</span><span class="n">shape</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">sqrt_alphas_cumprod_t</span> <span class="o">*</span> <span class="n">x_start</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">*</span> <span class="n">noise</span>
</code></pre></div></div>

<ul>
  <li>Let’s test it on a particular time step and see the image it produces:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_noisy_image</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
  <span class="c1"># add noise
</span>  <span class="n">x_noisy</span> <span class="o">=</span> <span class="n">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>

  <span class="c1"># turn back into PIL image
</span>  <span class="n">noisy_image</span> <span class="o">=</span> <span class="n">reverse_transform</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">.</span><span class="n">squeeze</span><span class="p">())</span>

  <span class="k">return</span> <span class="n">noisy_image</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># take time step
</span><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">40</span><span class="p">])</span>

<span class="n">get_noisy_image</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../assets/diffusion-models/24.jpg" alt="" /></p>

<ul>
  <li>We can see the image is getting more noisy. Now let’s zoom out a bit and visualize this for various time steps:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># use seed for reproducability
</span><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py
</span><span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">imgs</span><span class="p">,</span> <span class="n">with_orig</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">row_title</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">imshow_kwargs</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="c1"># Make a 2d grid even if there's just 1 row
</span>        <span class="n">imgs</span> <span class="o">=</span> <span class="p">[</span><span class="n">imgs</span><span class="p">]</span>

    <span class="n">num_rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
    <span class="n">num_cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">with_orig</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span><span class="mi">200</span><span class="p">),</span> <span class="n">nrows</span><span class="o">=</span><span class="n">num_rows</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="n">num_cols</span><span class="p">,</span> <span class="n">squeeze</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">row_idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">imgs</span><span class="p">):</span>
        <span class="n">row</span> <span class="o">=</span> <span class="p">[</span><span class="n">image</span><span class="p">]</span> <span class="o">+</span> <span class="n">row</span> <span class="k">if</span> <span class="n">with_orig</span> <span class="k">else</span> <span class="n">row</span>
        <span class="k">for</span> <span class="n">col_idx</span><span class="p">,</span> <span class="n">img</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
            <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">row_idx</span><span class="p">,</span> <span class="n">col_idx</span><span class="p">]</span>
            <span class="n">ax</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">img</span><span class="p">),</span> <span class="o">**</span><span class="n">imshow_kwargs</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">xticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticklabels</span><span class="o">=</span><span class="p">[],</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>

    <span class="k">if</span> <span class="n">with_orig</span><span class="p">:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'Original image'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="n">title</span><span class="p">.</span><span class="n">set_size</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">row_title</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">row_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rows</span><span class="p">):</span>
            <span class="n">axs</span><span class="p">[</span><span class="n">row_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">].</span><span class="nb">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">row_title</span><span class="p">[</span><span class="n">row_idx</span><span class="p">])</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plot</span><span class="p">([</span><span class="n">get_noisy_image</span><span class="p">(</span><span class="n">x_start</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">t</span><span class="p">]))</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">199</span><span class="p">]])</span>
</code></pre></div></div>

<p><img src="../assets/diffusion-models/25.jpg" alt="" /></p>

<ul>
  <li>As we can see above, the image going through forward diffusion is definitely becoming more apparent.</li>
  <li>Thus, we can now move on to defining our loss function. The <code class="language-plaintext highlighter-rouge">denoise_model</code> will be our U-Net defined above. We’ll employ the Huber loss between the true and the predicted noise.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">p_losses</span><span class="p">(</span><span class="n">denoise_model</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="s">"l1"</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">)</span>

    <span class="n">x_noisy</span> <span class="o">=</span> <span class="n">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="o">=</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="n">denoise_model</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">'l1'</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">'l2'</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">loss_type</span> <span class="o">==</span> <span class="s">"huber"</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="n">predicted_noise</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<h4 id="dataset">Dataset</h4>

<ul>
  <li>Let’s now look into loading up our dataset. A quick note, our dataset needs to make sure all images are resized to the same size.</li>
  <li>Hugging Face’s <a href="https://huggingface.co/datasets/fashion_mnist/viewer/fashion_mnist/train">fashion_mnist dataset</a> which we will use in this example already does that for us with all images having a same resolution of \(28 \times 28\).</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># load dataset from the hub
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">"fashion_mnist"</span><span class="p">)</span>
<span class="n">image_size</span> <span class="o">=</span> <span class="mi">28</span>
<span class="n">channels</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
</code></pre></div></div>

<ul>
  <li>Now, we will define a function <code class="language-plaintext highlighter-rouge">transforms</code> which we’ll apply on-the-fly on the entire dataset.</li>
  <li>The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the <code class="language-plaintext highlighter-rouge">[-1,1]</code> range.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="c1"># define image transformations (e.g. using torchvision)
</span><span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
            <span class="n">transforms</span><span class="p">.</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># define function
</span><span class="k">def</span> <span class="nf">transforms</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
   <span class="n">examples</span><span class="p">[</span><span class="s">"pixel_values"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="s">"L"</span><span class="p">))</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s">"image"</span><span class="p">]]</span>
   <span class="k">del</span> <span class="n">examples</span><span class="p">[</span><span class="s">"image"</span><span class="p">]</span>

   <span class="k">return</span> <span class="n">examples</span>

<span class="n">transformed_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="n">with_transform</span><span class="p">(</span><span class="n">transforms</span><span class="p">).</span><span class="n">remove_columns</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span>

<span class="c1"># create dataloader
</span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">transformed_dataset</span><span class="p">[</span><span class="s">"train"</span><span class="p">],</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Output</span><span class="p">:</span>
<span class="o">----------------------------------------------------------------------------------------------------</span>
<span class="n">dict_keys</span><span class="p">([</span><span class="s">'pixel_values'</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="sampling-during-training">Sampling during training</h4>

<ul>
  <li>The paper also talks about sampling from the model during training in order to track progress.</li>
  <li>Ideally, generating new images from a diffusion model happens by reversing the diffusion process:
    <ul>
      <li>We start from \(T\), where we sample pure noise from a Gaussian distribution</li>
      <li>Then use our neural network to gradually de-noise it using the conditional probability it has learned, continuing until we end up at time step \(t\) = 0.</li>
      <li>We can derive a slightly less de-noised image \(x_{(t-1)}\) by plugging in the re-parametrization of the mean, using our noise predictor.</li>
      <li>Remember that the variance is known ahead of time.</li>
    </ul>
  </li>
  <li>After all of this, ideally, we end up with an image that looks like it came from the real data distribution.</li>
  <li>Lets look at the code for that below:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">t_index</span><span class="p">):</span>
    <span class="n">betas_t</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">sqrt_one_minus_alphas_cumprod_t</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span>
        <span class="n">sqrt_one_minus_alphas_cumprod</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
    <span class="p">)</span>
    <span class="n">sqrt_recip_alphas_t</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="n">sqrt_recip_alphas</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="c1"># Equation 11 in the paper
</span>    <span class="c1"># Use our model (noise predictor) to predict the mean
</span>    <span class="n">model_mean</span> <span class="o">=</span> <span class="n">sqrt_recip_alphas_t</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">x</span> <span class="o">-</span> <span class="n">betas_t</span> <span class="o">*</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="n">sqrt_one_minus_alphas_cumprod_t</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">t_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model_mean</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">posterior_variance_t</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="n">posterior_variance</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Algorithm 2 line 4:
</span>        <span class="k">return</span> <span class="n">model_mean</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">posterior_variance_t</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span> 

<span class="c1"># Algorithm 2 (including returning all images)
</span><span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()).</span><span class="n">device</span>

    <span class="n">b</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># start from pure noise (for each example in the batch)
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">imgs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">)),</span> <span class="n">desc</span><span class="o">=</span><span class="s">'sampling loop time step'</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">timesteps</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">p_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">img</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="n">b</span><span class="p">,),</span> <span class="n">i</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">),</span> <span class="n">i</span><span class="p">)</span>
        <span class="n">imgs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">imgs</span>

<span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">p_sample_loop</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">))</span>
</code></pre></div></div>

<ul>
  <li>Now, lets get to some training! We will train the model via PyTorch and occasionally save a few image samples using the <code class="language-plaintext highlighter-rouge">sample</code> function from above.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="k">def</span> <span class="nf">num_to_groups</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">divisor</span><span class="p">):</span>
    <span class="n">groups</span> <span class="o">=</span> <span class="n">num</span> <span class="o">//</span> <span class="n">divisor</span>
    <span class="n">remainder</span> <span class="o">=</span> <span class="n">num</span> <span class="o">%</span> <span class="n">divisor</span>
    <span class="n">arr</span> <span class="o">=</span> <span class="p">[</span><span class="n">divisor</span><span class="p">]</span> <span class="o">*</span> <span class="n">groups</span>
    <span class="k">if</span> <span class="n">remainder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">arr</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">remainder</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">arr</span>

<span class="n">results_folder</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">"./results"</span><span class="p">)</span>
<span class="n">results_folder</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">exist_ok</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">save_and_sample_every</span> <span class="o">=</span> <span class="mi">1000</span>
</code></pre></div></div>

<ul>
  <li>Below, we define the model, and move it to the GPU along with defining Adam, a standard optimizer.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>

<span class="n">device</span> <span class="o">=</span> <span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Unet</span><span class="p">(</span>
    <span class="n">dim</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span>
    <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">,</span>
    <span class="n">dim_mults</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,)</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

</code></pre></div></div>

<h4 id="training-1">Training</h4>

<ul>
  <li>Now lets start the training process:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchvision.utils</span> <span class="kn">import</span> <span class="n">save_image</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

      <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">"pixel_values"</span><span class="p">].</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
      <span class="n">batch</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="s">"pixel_values"</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

      <span class="c1"># Algorithm 1 line 3: sample t uniformally for every example in the batch
</span>      <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>

      <span class="n">loss</span> <span class="o">=</span> <span class="n">p_losses</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">loss_type</span><span class="o">=</span><span class="s">"huber"</span><span class="p">)</span>

      <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Loss:"</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

      <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

      <span class="c1"># save generated images
</span>      <span class="k">if</span> <span class="n">step</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">step</span> <span class="o">%</span> <span class="n">save_and_sample_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">milestone</span> <span class="o">=</span> <span class="n">step</span> <span class="o">//</span> <span class="n">save_and_sample_every</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="n">num_to_groups</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">all_images_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">n</span><span class="p">:</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">),</span> <span class="n">batches</span><span class="p">))</span>
        <span class="n">all_images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_images_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">all_images</span> <span class="o">=</span> <span class="p">(</span><span class="n">all_images</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span>
        <span class="n">save_image</span><span class="p">(</span><span class="n">all_images</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">results_folder</span> <span class="o">/</span> <span class="sa">f</span><span class="s">'sample-</span><span class="si">{</span><span class="n">milestone</span><span class="si">}</span><span class="s">.png'</span><span class="p">),</span> <span class="n">nrow</span> <span class="o">=</span> <span class="mi">6</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Output</span><span class="p">:</span>
<span class="o">----------------------------------------------------------------------------------------------------</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.46477368474006653</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.12143351882696152</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.08106148988008499</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0801810547709465</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.06122320517897606</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.06310459971427917</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.05681884288787842</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.05729678273200989</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.05497899278998375</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.04439849033951759</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.05415581166744232</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.06020551547408104</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.046830907464027405</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.051029372960329056</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0478244312107563</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.046767622232437134</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.04305662214756012</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.05216279625892639</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.04748568311333656</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.05107741802930832</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.04588869959115982</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.043014321476221085</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.046371955424547195</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.04952816292643547</span>
<span class="n">Loss</span><span class="p">:</span> <span class="mf">0.04472338408231735</span>
</code></pre></div></div>

<ul>
  <li>And finally, let’s look at our inference or sampling from the <code class="language-plaintext highlighter-rouge">sample</code> function we defined above.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># sample 64 images
</span><span class="n">samples</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">image_size</span><span class="o">=</span><span class="n">image_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">channels</span><span class="o">=</span><span class="n">channels</span><span class="p">)</span>

<span class="c1"># show a random one
</span><span class="n">random_index</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="n">random_index</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../assets/diffusion-models/27.jpg" alt="" /></p>

<ul>
  <li>Seems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).</li>
</ul>

<h4 id="creating-a-gif">Creating a GIF</h4>

<ul>
  <li>Lastly, in order to see the progression of the de-noising process, we can create a GIF:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.animation</span> <span class="k">as</span> <span class="n">animation</span>

<span class="n">random_index</span> <span class="o">=</span> <span class="mi">53</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ims</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">random_index</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="n">image_size</span><span class="p">,</span> <span class="n">image_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"gray"</span><span class="p">,</span> <span class="n">animated</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">ims</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">im</span><span class="p">])</span>

<span class="n">animate</span> <span class="o">=</span> <span class="n">animation</span><span class="p">.</span><span class="n">ArtistAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">ims</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">repeat_delay</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">animate</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'diffusion.gif'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="../assets/diffusion-models/diffusion-sweater.gif" alt="" /></p>

<ul>
  <li>Hopefully this was beneficial in clarifying the diffusion model concepts!</li>
  <li>Furthermore, it his highly recommend looking at <a href="https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb">Hugging Face’s Training with Diffusers notebook</a> to see how to leverage their Diffusion library to train a simple model.</li>
  <li>And, for inference, they also provide <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">this</a> notebook where you can see the images being generated.</li>
</ul>

<h3 id="denoising-diffusion-pytorch-package"><code class="language-plaintext highlighter-rouge">denoising-diffusion-pytorch</code> package</h3>

<ul>
  <li>While Diffusion models have not yet been democratized to the same degree as other older architectures/approaches in Machine Learning, there are still implementations available for use. The easiest way to use a diffusion model in PyTorch is to use the <code class="language-plaintext highlighter-rouge">denoising-diffusion-pytorch</code> package, which implements an image diffusion model like the one discussed in this article. To install the package, simply type the following command in the terminal:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pip</span> <span class="n">install</span> <span class="n">denoising_diffusion_pytorch</span>
</code></pre></div></div>

<h3 id="minimal-example">Minimal Example</h3>

<ul>
  <li>To train a model and generate images, we first import the necessary packages:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">denoising_diffusion_pytorch</span> <span class="kn">import</span> <span class="n">Unet</span><span class="p">,</span> <span class="n">GaussianDiffusion</span>
</code></pre></div></div>

<ul>
  <li>Next, we define our network architecture, in this case a U-Net. The <code class="language-plaintext highlighter-rouge">dim</code> parameter specifies the number of feature maps before the first down-sampling, and the <code class="language-plaintext highlighter-rouge">dim_mults</code> parameter provides multiplicands for this value and successive down-samplings:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">Unet</span><span class="p">(</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">dim_mults</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Now that our network architecture is defined, we need to define the diffusion model itself. We pass in the U-Net model that we just defined along with several parameters - the size of images to generate, the number of timesteps in the diffusion process, and a choice between the L1 and L2 norms.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">diffusion</span> <span class="o">=</span> <span class="n">GaussianDiffusion</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">image_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span>
    <span class="n">timesteps</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>   <span class="c1"># number of steps
</span>    <span class="n">loss_type</span> <span class="o">=</span> <span class="s">'l1'</span>    <span class="c1"># L1 or L2
</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>Now that the diffusion model is defined, it’s time to train. We generate random data to train on, and then train the diffusion model in the usual fashion:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">training_images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">(</span><span class="n">training_images</span><span class="p">)</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<ul>
  <li>Once the model is trained, we can finally generate images by using the <code class="language-plaintext highlighter-rouge">sample()</code> method of the <code class="language-plaintext highlighter-rouge">diffusion</code> object. Here we generate 4 images, which are only noise given that our training data was random:</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sampled_images</span> <span class="o">=</span> <span class="n">diffusion</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../assets/diffusion-models/diff12.png" alt="" /></p>

<h3 id="training-on-custom-data">Training on Custom Data</h3>

<ul>
  <li>The <code class="language-plaintext highlighter-rouge">denoising-diffusion-pytorch</code> package also allow you to train a diffusion model on a specific dataset. Simply replace the <code class="language-plaintext highlighter-rouge">'path/to/your/images'</code> string with the dataset directory path in the <code class="language-plaintext highlighter-rouge">Trainer()</code> object below, and change <code class="language-plaintext highlighter-rouge">image_size</code> to the appropriate value. After that, simply run the code to train the model, and then sample as before. Note that PyTorch must be compiled with CUDA enabled in order to use the <code class="language-plaintext highlighter-rouge">Trainer</code> class:</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer

model = Unet(
    dim = 64,
    dim_mults = (1, 2, 4, 8)
).cuda()

diffusion = GaussianDiffusion(
    model,
    image_size = 128,
    timesteps = 1000,   # number of steps
    loss_type = 'l1'    # L1 or L2
).cuda()

trainer = Trainer(
    diffusion,
    'path/to/your/images',
    train_batch_size = 32,
    train_lr = 2e-5,
    train_num_steps = 700000,         # total training steps
    gradient_accumulate_every = 2,    # gradient accumulation steps
    ema_decay = 0.995,                # exponential moving average decay
    amp = True                        # turn on mixed precision
)

trainer.train()
</code></pre></div></div>

<ul>
  <li>Below you can see progressive denoising from multivariate Gaussian noise to MNIST digits akin to reverse diffusion:</li>
</ul>

<p><img src="../assets/diffusion-models/diff13.gif" alt="" /></p>

<h2 id="huggingface-diffusers">HuggingFace Diffusers</h2>

<ul>
  <li><a href="https://github.com/huggingface/diffusers">HuggingFace diffusers</a> provides pretrained diffusion models across multiple modalities, such as vision and audio, and serves as a modular toolbox for inference and training of diffusion models.</li>
</ul>

<p><img src="../assets/diffusion-models/hf1.jpg" alt="" /></p>

<ul>
  <li>More precisely, HuggingFace Diffusers offers:
    <ul>
      <li>State-of-the-art diffusion pipelines that can be run in inference with just a couple of lines of code.</li>
      <li>Various noise schedulers that can be used interchangeably for the prefered speed vs. quality trade-off in inference.</li>
      <li>Multiple types of models, such as UNet, that can be used as building blocks in an end-to-end diffusion system.</li>
      <li>Training examples to show how to train the most popular diffusion models.</li>
    </ul>
  </li>
</ul>

<h2 id="implementations">Implementations</h2>

<h3 id="stable-diffusion"><a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">Stable Diffusion</a></h3>

<ul>
  <li><a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">Stable Diffusion</a> <a href="https://huggingface.co/blog/stable_diffusion">(blog)</a> is a state of the art text-to-image model that generates images from text. It’s makes it’s high performance models available to the public at large to use <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">here</a>.</li>
  <li>Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. It is trained on 512x512 images from a subset of the LAION-5B database which is the largest, freely accessible multi-modal dataset.</li>
  <li>Let’s now look at how it works with the illustrations below by <a href="https://twitter.com/JayAlammar/status/1572297768693006337">Jay Alammar</a>.</li>
</ul>

<p><img src="../assets/diffusion-models/7.jpg" alt="" /></p>

<ul>
  <li>Stable Diffusion is quite versatile because it can be used in a variety of ways.</li>
  <li>In the image we see above, we can see that it can take text as input and output a generated image. This is the primary use case, however, it is not the only one.</li>
</ul>

<p><img src="../assets/diffusion-models/10.jpg" alt="" /></p>

<ul>
  <li>As we can see from the image above, another use case of Stable Diffusion is with image and text as input, and it will output a generated image again. This is called img2img.</li>
  <li>It’s able to be so versatile because Stable Diffusion is not one monolith model, but a system made up of several components and models.</li>
  <li>To be specific, Stable Diffusion is made up of a:
    <ul>
      <li>1) Text Understanding component</li>
      <li>2) Image Generation component</li>
    </ul>
  </li>
</ul>

<p><img src="../assets/diffusion-models/11.png" alt="" /></p>

<ul>
  <li>The text understanding component is actually the text encoder used within <a href="https://arxiv.org/abs/2103.00020">CLIP</a>.</li>
  <li>As we can see represented in the image below, Stable Diffusion takes the input text within the Text Understander component and returns a vector representing each token in the text.</li>
  <li>This information is then passed over to the Image Generator component which internally is composed of 2 components as well.</li>
</ul>

<p><img src="../assets/diffusion-models/12.png" alt="" /></p>

<ul>
  <li>Now, referring to the image below, let’s look at the two components within the Image Generator component.
    <ul>
      <li>Image Information Creator:
        <ul>
          <li>This is the ‘secret sauce’ of Stable Diffusion as it runs for a number of steps refining the information that should go in the image that will become the model’s output.</li>
        </ul>
      </li>
      <li>Image Decoder:
        <ul>
          <li>This component takes the processed information and paints the picture.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="../assets/diffusion-models/13.png" alt="" /></p>

<ul>
  <li>Let’s zoom out for a second and look at the higher level components we have so far all working together for the image generation task:</li>
</ul>

<p><img src="../assets/diffusion-models/14.png" alt="" /></p>

<ul>
  <li>All the 3 components above are actually individual neural networks working together, specifically, they are:
    <ul>
      <li>CLIPText: Used to encode the text</li>
      <li>U-Net + scheduler: Used to gradually process image information(latent diffusion)</li>
      <li>Autoencoder Decoder: paints the final image</li>
    </ul>
  </li>
</ul>

<p><img src="../assets/diffusion-models/15.png" alt="" /></p>

<ul>
  <li>
    <p>Above we can see the steps that Stable Diffusion takes to generate its images.</p>
  </li>
  <li>
    <p>Lastly, let’s zoom into the image decoder and get a better understanding of its inner workings. Remember the image decoder is one of the two components the image generator comprises of</p>
  </li>
</ul>

<p><img src="../assets/diffusion-models/16.png" alt="" /></p>

<ul>
  <li>The random vector is considered to be random noise.</li>
  <li>Stable Diffusion is able to obtain it’s speed from the fact that the processing happens in the latent space (which needs less calculations as compared to the pixel space).</li>
</ul>

<h3 id="dream-studio"><a href="https://beta.dreamstudio.ai/home">Dream Studio</a></h3>

<ul>
  <li><a href="https://beta.dreamstudio.ai/home">Dream Studio</a> is Stable Diffusion’s AI Art Web App Tool.</li>
  <li>DreamStudio is a new suite of generative media tools engineered to grant everyone the power of limitless imagination and the effortless ease of visual expression through a combination of natural language processing and revolutionary input controls for accelerated creativity.</li>
</ul>

<h3 id="midjourney"><a href="https://www.midjourney.com/home/#about">Midjourney</a></h3>

<ul>
  <li><a href="https://www.midjourney.com/home/#about">Midjourney</a> is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species.</li>
  <li>Midjourney has not made it’s architecture details publicly available, but one has to think they still leverage diffusion models in some fashion.</li>
  <li>While Dall-E 2 creates more realistic images, MidJourney shines in adapting real art styles into creating an image of any combination of things your heart desires.</li>
</ul>

<p><img src="../assets/diffusion-models/28.jpg" alt="" /></p>

<h3 id="dall-e-2">DALL-E 2</h3>

<ul>
  <li>DALL-E 2 utilized diffusion models to create its images and was created by OpenAI.</li>
  <li>DALL-E 2 can make realistic edits to existing images from a natural language caption.
    <ul>
      <li>It can add and remove elements while taking shadows, reflections, and textures into consideration.</li>
    </ul>
  </li>
  <li>DALL-E 2 has learned the relationship between images and the text used to describe them.</li>
  <li>It uses diffusion, which starts with a pattern of random dots and gradually alters that pattern towards an image when it recognizes specific aspects of that image.</li>
  <li>OpenAI has limited the ability for DALL-E 2 to generate violent, hate, or adult images.</li>
  <li>By removing the most explicit content from the training data, OpenAI has minimized DALL-E 2’s exposure to these concepts.</li>
  <li>They have also used advanced techniques to prevent photorealistic generations of real individuals’ faces, including those of public figures.</li>
  <li>Among the most important building blocks in the DALL-E 2 architecture is CLIP to function as the main bridge between text and images.</li>
</ul>

<h4 id="related-clip-contrastive-language-image-pre-training">Related: CLIP (Contrastive Language-Image Pre-Training)</h4>

<ul>
  <li>While CLIP does not use a diffusion model, it is essential to understand DALL-E 2 so let’s do a quick recap of CLIP’s architecture.</li>
  <li>CLIP is a neural network trained on a variety of (image, text) pairs.</li>
  <li>It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3.</li>
  <li>CLIP is a multi-modal vision and language model.</li>
  <li>It can be used for image-text similarity and for zero-shot image classification.</li>
  <li>CLIP uses a ViT like transformer to get visual features and a causal language model to get the text features.</li>
  <li>Both the text and visual features are then projected to a latent space with identical dimension. The dot product between the projected image and text features is then used as a similar score.</li>
  <li>CLIP enables us to take textual phrases and understand how they map onto images</li>
</ul>

<h2 id="gallery">Gallery</h2>

<ul>
  <li>Showcasing a few images generated via Diffusion Models along with their text prompts given:
    <ul>
      <li>A Corgi puppy painted like the Mona Lisa:</li>
    </ul>

    <p><img src="../assets/diffusion-models/midjourney.jpg" alt="" /></p>

    <ul>
      <li>Beyonce sitting at a desk and coding:</li>
    </ul>

    <p><img src="../assets/diffusion-models/midjourney2.jpg" alt="" /></p>

    <ul>
      <li>Snow in Hawaii:</li>
    </ul>

    <p><img src="../assets/diffusion-models/8.jpg" alt="" /></p>

    <ul>
      <li>Sun coming in from a big window with curtains and casting a shadow on the rest of the room, artistic style:</li>
    </ul>

    <p><img src="../assets/diffusion-models/9.jpg" alt="" /></p>

    <ul>
      <li>The Taj Mahal painted in Starry Night by Vincent Van Gogh:</li>
    </ul>

    <p><img src="../assets/diffusion-models/17.jpg" alt="" /></p>
  </li>
</ul>

<h2 id="faq">FAQ</h2>

<h3 id="at-a-high-level-how-do-diffusion-models-work-what-are-some-other-models-that-are-useful-for-image-generation-and-how-do-they-compare-to-diffusion-models">At a high level, how do diffusion models work? What are some other models that are useful for image generation, and how do they compare to diffusion models?</h3>

<h4 id="high-level-overview-of-diffusion-models">High-Level Overview of Diffusion Models</h4>

<ul>
  <li>Diffusion models are a type of generative model that creates data by gradually denoising a sample from a noise distribution. The process involves two main phases: a forward diffusion process that corrupts the data by adding noise, and a reverse denoising process that learns to remove the noise step-by-step to recover the original data. Here’s a high-level breakdown:</li>
</ul>

<h5 id="forward-diffusion-process">Forward Diffusion Process</h5>

<ol>
  <li><strong>Start with a Data Sample</strong>: Begin with a real data sample, such as an image.</li>
  <li><strong>Add Noise Incrementally</strong>: Over a series of steps \(t\), progressively add Gaussian noise to the sample. The amount of noise added at each step is controlled by a noise schedule \(\beta_t\).
\(x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)\)</li>
  <li><strong>Result in Noisy Data</strong>: By the final step, the data is almost completely transformed into pure noise.</li>
</ol>

<h5 id="reverse-denoising-process">Reverse Denoising Process</h5>

<ol>
  <li><strong>Start with Noise</strong>: Begin with a sample of pure noise.</li>
  <li><strong>Learn to Remove Noise</strong>: A neural network is trained to predict and remove the added noise at each step, effectively denoising the sample.
\(p_\theta(x_{t-1} \| x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\)</li>
  <li><strong>Recover Original Data</strong>: Iteratively apply the denoising steps to transform the noise back into a data sample that resembles the original data distribution.</li>
</ol>

<h4 id="other-models-for-image-generation">Other Models for Image Generation</h4>

<ul>
  <li>Several other models are commonly used for image generation, each with unique characteristics and methodologies. Here are some notable ones:</li>
</ul>

<h5 id="generative-adversarial-networks-gans">Generative Adversarial Networks (GANs)</h5>

<ul>
  <li><strong>How They Work</strong>:</li>
  <li><strong>Two Networks</strong>: Consist of a generator and a discriminator network that compete against each other.</li>
  <li><strong>Generator</strong>: Creates fake images from random noise.</li>
  <li><strong>Discriminator</strong>: Tries to distinguish between real images and fake images produced by the generator.</li>
  <li>
    <p><strong>Adversarial Training</strong>: The generator improves to produce more realistic images as the discriminator gets better at distinguishing them.</p>
  </li>
  <li><strong>Comparison to Diffusion Models</strong>:</li>
  <li><strong>Training Stability</strong>: GANs can be harder to train and may suffer from issues like mode collapse.</li>
  <li><strong>Speed</strong>: Typically faster in generation since GANs do not require iterative denoising steps.</li>
  <li><strong>Quality</strong>: Can produce high-quality images, but may lack diversity in the generated samples compared to diffusion models.</li>
</ul>

<h5 id="variational-autoencoders-vaes">Variational Autoencoders (VAEs)</h5>

<ul>
  <li><strong>How They Work</strong>:</li>
  <li><strong>Encoder-Decoder Architecture</strong>: Consist of an encoder that maps input data to a latent space and a decoder that reconstructs the data from the latent space.</li>
  <li><strong>Latent Space Sampling</strong>: Imposes a probabilistic structure on the latent space, encouraging smooth transitions and interpolation.</li>
  <li>
    <p><strong>Variational Inference</strong>: Uses a loss function that includes a reconstruction term and a regularization term (Kullback-Leibler divergence).</p>
  </li>
  <li><strong>Comparison to Diffusion Models</strong>:</li>
  <li><strong>Latent Space Representation</strong>: VAEs provide an explicit latent space representation, which can be useful for tasks like interpolation and manipulation.</li>
  <li><strong>Sample Quality</strong>: VAEs typically produce lower-quality images compared to GANs and diffusion models.</li>
  <li><strong>Training Stability</strong>: Generally more stable and easier to train than GANs.</li>
</ul>

<h5 id="autoregressive-models">Autoregressive Models</h5>

<ul>
  <li><strong>How They Work</strong>:</li>
  <li><strong>Sequential Generation</strong>: Generate images pixel-by-pixel or patch-by-patch in a sequential manner.</li>
  <li><strong>Conditional Dependencies</strong>: Each pixel or patch is conditioned on the previously generated ones.</li>
  <li>
    <p><strong>Examples</strong>: PixelRNN, PixelCNN.</p>
  </li>
  <li><strong>Comparison to Diffusion Models</strong>:</li>
  <li><strong>Generation Time</strong>: Autoregressive models can be slow due to sequential nature.</li>
  <li><strong>Quality</strong>: Can produce high-quality images with strong dependencies between pixels.</li>
  <li><strong>Flexibility</strong>: Can naturally model complex dependencies but are computationally intensive.</li>
</ul>

<h5 id="flow-based-models">Flow-based Models</h5>

<ul>
  <li><strong>How They Work</strong>:
    <ul>
      <li><strong>Invertible Transformations</strong>: Use a series of invertible transformations to map data to a latent space and vice versa.</li>
      <li><strong>Exact Likelihood</strong>: Allow exact computation of the data likelihood, making them powerful for density estimation.</li>
      <li><strong>Examples</strong>: RealNVP, Glow.</li>
    </ul>
  </li>
  <li><strong>Comparison to Diffusion Models</strong>:
    <ul>
      <li><strong>Efficiency</strong>: Flow-based models can be efficient in both training and sampling due to invertible nature.</li>
      <li><strong>Quality</strong>: Produce high-quality images but may require more complex architectures for challenging datasets.</li>
      <li><strong>Interpretability</strong>: Provide explicit likelihood estimates and interpretable latent spaces.</li>
    </ul>
  </li>
</ul>

<h4 id="summary-2">Summary</h4>

<ul>
  <li><strong>Diffusion Models</strong>: Offer a robust and principled approach to image generation with a focus on iterative denoising. They provide high-quality samples but can be slower due to the iterative nature.</li>
  <li><strong>GANs</strong>: Known for producing very high-quality images quickly but can be challenging to train due to adversarial dynamics.</li>
  <li><strong>VAEs</strong>: Provide stable training and useful latent space representations, though often at the cost of sample quality.</li>
  <li><strong>Autoregressive Models</strong>: Capable of modeling complex dependencies with high-quality outputs, but slow due to sequential generation.</li>
  <li><strong>Flow-based Models</strong>: Efficient and interpretable with exact likelihood estimation, balancing quality and computational requirements.</li>
  <li>In summary, each model type has its strengths and weaknesses, making them suitable for different applications and preferences in the trade-off between quality, efficiency, and ease of training.</li>
</ul>

<h3 id="what-is-the-difference-between-ddpm-and-ddims-models">What is the difference between DDPM and DDIMs models?</h3>

<ul>
  <li>Denoising Diffusion Probabilistic Models (DDPM) and Denoising Diffusion Implicit Models (DDIMs) are both types of diffusion models used for generative tasks, but they differ in their approach to the reverse diffusion process, which leads to differences in their efficiency and the quality of generated samples. Here’s a detailed explanation of both models and their differences:</li>
</ul>

<h4 id="ddpm">DDPM</h4>

<ul>
  <li>DDPMs are a class of generative models that create data by reversing a Markovian diffusion process. The diffusion process gradually adds noise to the data in several steps until it becomes nearly pure Gaussian noise. The model then learns to reverse this process, step by step, to generate new data samples.</li>
</ul>

<h5 id="key-characteristics">Key Characteristics</h5>

<ol>
  <li><strong>Forward Process</strong>:
    <ul>
      <li>The forward diffusion process adds Gaussian noise to data over \(T\) timesteps.</li>
      <li>Each step is defined as:
\(q(x_t \| x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\)</li>
      <li>\(\beta_t\) is the noise schedule, typically increasing linearly or following another schedule over time.</li>
    </ul>
  </li>
  <li><strong>Reverse Process</strong>:
    <ul>
      <li>The reverse process is learned using a neural network to approximate the conditional probabilities:
\(p_\theta(x_{t-1} \| x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\)</li>
      <li>The mean \(\mu_\theta\) and variance \(\Sigma_\theta\) are predicted by the neural network.</li>
    </ul>
  </li>
  <li><strong>Training</strong>:
    <ul>
      <li>The model is trained to minimize the variational bound on the data likelihood, which involves matching the reverse process to the true posterior of the forward process.</li>
    </ul>
  </li>
  <li><strong>Sampling</strong>:
    <ul>
      <li>Sampling involves running the reverse process starting from Gaussian noise \(x_T\), iteratively refining it to produce a sample \(x_0\).</li>
    </ul>
  </li>
</ol>

<h5 id="advantages-and-disadvantages">Advantages and Disadvantages</h5>

<ul>
  <li><strong>Advantages</strong>:
    <ul>
      <li>Generates high-quality samples.</li>
      <li>Well-grounded in probabilistic principles, leading to stable training.</li>
    </ul>
  </li>
  <li><strong>Disadvantages</strong>:
    <ul>
      <li>The reverse process is slow because it involves many iterative steps.</li>
      <li>Each step requires a neural network forward pass, making the sampling process computationally expensive.</li>
    </ul>
  </li>
</ul>

<h4 id="ddims">DDIMs</h4>

<ul>
  <li>DDIMss are a variation of diffusion models that introduce a non-Markovian forward process, which allows for a more efficient reverse process. The key idea is to find a deterministic mapping that approximates the same data distribution as the original Markovian process used in DDPMs.</li>
</ul>

<h5 id="key-characteristics-1">Key Characteristics</h5>

<ol>
  <li><strong>Forward Process</strong>:
    <ul>
      <li>The forward process in DDIMss can be viewed as a non-Markovian process that achieves the same goal of perturbing data into noise.</li>
      <li>Instead of a strict Markov chain, DDIMss introduce a sequence of latent variables that allow skipping steps while preserving the ability to reverse the process.</li>
    </ul>
  </li>
  <li><strong>Reverse Process</strong>:
    <ul>
      <li>The reverse process becomes deterministic, significantly speeding up the sampling process.</li>
      <li>The reverse step is defined by a deterministic mapping, approximating the reverse diffusion without needing as many steps as DDPMs.</li>
      <li>This is achieved through a reparameterization that relates the noise-added data at different timesteps directly.</li>
    </ul>
  </li>
  <li><strong>Training</strong>:
    <ul>
      <li>Training is similar to DDPMs but leverages the deterministic nature of the reverse process for improved efficiency.</li>
    </ul>
  </li>
  <li><strong>Sampling</strong>:
    <ul>
      <li>Sampling in DDIMss can be done with fewer steps while still producing high-quality samples.</li>
      <li>The deterministic reverse process can potentially offer more control over the generation process, enabling finer adjustments to the generated data.</li>
    </ul>
  </li>
</ol>

<h5 id="advantages-and-disadvantages-1">Advantages and Disadvantages</h5>

<ul>
  <li><strong>Advantages</strong>:
    <ul>
      <li>Faster sampling compared to DDPMs due to the deterministic reverse process.</li>
      <li>Fewer sampling steps needed while maintaining or even improving sample quality.</li>
    </ul>
  </li>
  <li><strong>Disadvantages</strong>:
    <ul>
      <li>The theoretical underpinnings are less straightforward compared to the probabilistic foundations of DDPMs.</li>
      <li>Potentially less flexibility in certain applications where stochasticity in the reverse process is beneficial.</li>
    </ul>
  </li>
</ul>

<h4 id="key-differences">Key Differences</h4>

<ol>
  <li><strong>Process Type</strong>:
    <ul>
      <li><strong>DDPM</strong>: Markovian forward process with a stochastic reverse process.</li>
      <li><strong>DDIMs</strong>: Non-Markovian forward process with a deterministic reverse process.</li>
    </ul>
  </li>
  <li><strong>Sampling Efficiency</strong>:
    <ul>
      <li><strong>DDPM</strong>: Requires many reverse steps, making it computationally expensive.</li>
      <li><strong>DDIMs</strong>: Achieves faster sampling with fewer steps.</li>
    </ul>
  </li>
  <li><strong>Reverse Process</strong>:
    <ul>
      <li><strong>DDPM</strong>: Stochastic reverse process, which involves sampling from a learned Gaussian distribution at each step.</li>
      <li><strong>DDIMs</strong>: Deterministic reverse process, which directly maps noisy data to clean data without stochastic sampling.</li>
    </ul>
  </li>
  <li><strong>Complexity and Flexibility</strong>:
    <ul>
      <li><strong>DDPM</strong>: More flexible in representing complex distributions due to the stochastic nature of the reverse process.</li>
      <li><strong>DDIMs</strong>: More efficient and potentially more controllable but may be less flexible in certain scenarios.</li>
    </ul>
  </li>
</ol>

<ul>
  <li>In summary, while both DDPM and DDIMs are powerful diffusion-based generative models, DDIMss offer a more efficient sampling process by employing a deterministic reverse process, leading to faster generation of samples without compromising quality. DDPMs, on the other hand, are grounded in a robust probabilistic framework, making them more flexible but slower in practice.</li>
</ul>

<h3 id="in-diffusion-models-there-is-a-forward-diffusion-process-and-a-reverse-diffusiondenoising-process-when-do-you-use-which-during-training-and-inference">In diffusion models, there is a forward diffusion process and a reverse diffusion/denoising process. When do you use which during training and inference?</h3>

<ul>
  <li>In diffusion models, which are a class of generative models, the forward diffusion process and the denoising process play distinct roles during training and inference. Understanding when and how these processes are used is key to grasping how diffusion models work.
    <ul>
      <li><strong>Forward Diffusion Process</strong>
        <ul>
          <li><strong>During Training:</strong>
            <ul>
              <li>Noise Addition: In the forward diffusion process, noise is gradually added to the data over several steps or iterations. This process transforms the original data into a pure noise distribution through a predefined sequence of steps.</li>
              <li>Training Objective: The model is trained to predict the noise that was added at each step. Essentially, it learns to reverse the diffusion process.</li>
            </ul>
          </li>
          <li><strong>During Inference:</strong>
            <ul>
              <li>Not Directly Used: The forward diffusion process is not explicitly used during inference. However, the knowledge gained during training (about how noise is added) is implicitly used to guide the denoising process.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Denoising Process</strong>
        <ul>
          <li><strong>During Training:</strong>
            <ul>
              <li>Learning to Reverse Noise: The model learns to denoise the data, i.e., to reverse the forward diffusion process. It does this by predicting the noise that was added at each step during the forward diffusion and then subtracting this noise.</li>
              <li>Parameter Optimization: The parameters of the model are optimized to make accurate predictions of the added noise, thereby learning to gradually denoise the data back to its original form.</li>
            </ul>
          </li>
          <li><strong>During Inference:</strong>
            <ul>
              <li>Data Generation: The denoising process is the key to generating new data. Starting from pure noise, the model iteratively denoises this input, using the reverse of the forward process, to generate a sample.</li>
              <li>Iterative Refinement: At each step, the model predicts the noise to remove, effectively refining the sample from random noise into a coherent output.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li><strong>Summary</strong>
        <ul>
          <li><strong>Training Phase:</strong> Both the forward diffusion (adding noise) and the denoising (removing noise) processes are actively used. The model learns how to reverse the gradual corruption of the data (caused by adding noise) by being trained to predict and remove the noise at each step.</li>
          <li><strong>Inference Phase:</strong> Only the denoising process is used, where the model starts with noise and iteratively applies the learned denoising steps to generate a sample. The forward process is not explicitly run during inference, but its principles underpin the reverse process.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>In essence, the forward diffusion process is crucial for training the model to understand and reverse the noise addition, while the denoising process is used both in training (to learn this reversal) and in inference (to generate new data).</li>
</ul>

<h3 id="what-are-the-loss-functions-used-in-diffusion-models">What are the loss functions used in Diffusion Models?</h3>

<ul>
  <li>Here are some common loss functions used in diffusion models:</li>
</ul>

<h4 id="mean-squared-error-mse">Mean Squared Error (MSE)</h4>
<ul>
  <li>
    <p><strong>Description</strong>:  The Mean Squared Error loss function measures the average of the squares of the errors between the predicted and actual values.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>
  </li>
</ul>

\[\mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^{N} (\hat{x}_i - x_i)^2\]

<ul>
  <li><strong>Use in Diffusion Models</strong>: MSE is commonly used in the denoising score matching variant of diffusion models. Here, the model learns to predict the noise added to the original data at each diffusion step. The loss function measures how well the model can predict this noise.</li>
</ul>

<h5 id="denoising-score-matching-dsm">Denoising Score Matching (DSM)</h5>
<ul>
  <li>
    <p><strong>Description</strong>: Denoising Score Matching (DSM) is a loss function used to train models to estimate the score function, which is the gradient of the log probability density function of the data. The model learns to predict the noise added to the data, effectively denoising it.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>

\[\mathcal{L}_{\text{DSM}} = \mathbb{E}_{p_{\text{data}}(x)p_{\sigma}(\tilde{x} \| x)} \left[ \left\| s_\theta(\tilde{x}, \sigma) - \frac{\partial \log p_\sigma(\tilde{x} \| x)}{\partial \tilde{x}} \right\|^2 \right]\]

    <ul>
      <li>where, \(s_\theta(\tilde{x}, \sigma)\) is the score function parameterized by the model, \(\tilde{x}\) is the noisy version of the data \(x\), and \(p_{\sigma}(\tilde{x} \| x)\) is the noise distribution.</li>
    </ul>
  </li>
  <li>
    <p><strong>Use in Diffusion Models</strong>: DSM is particularly used in Score-Based Generative Models (SBGM), which are a type of diffusion model. The model learns to predict the score (gradient of the log probability) of the data distribution at different noise levels.</p>
  </li>
</ul>

<h3 id="integration-with-mse">Integration with MSE</h3>
<p>Denoising Score Matching can be viewed as a specific case of MSE where the target is the gradient of the log probability density function. In practice, DSM can be integrated into an MSE framework:</p>

<ul>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>

\[\mathcal{L}_{\text{MSE-DSM}} = \mathbb{E}_{p_{\text{data}}(x)p_{\sigma}(\tilde{x} \| x)} \left[ \left\| s_\theta(\tilde{x}, \sigma) - \frac{\tilde{x} - x}{\sigma^2} \right\|^2 \right]\]

    <ul>
      <li>where, the target \(\frac{\tilde{x} - x}{\sigma^2}\) represents the gradient of the log probability of the noise distribution, making this a practical implementation of DSM within an MSE framework.</li>
    </ul>
  </li>
</ul>

<h4 id="kullback-leibler-divergence-kl-divergence">Kullback-Leibler Divergence (KL Divergence)</h4>
<ul>
  <li>
    <p><strong>Description</strong>:  KL Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>

\[\mathcal{L}_{\text{KL}}(P || Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}\]
  </li>
  <li>
    <p><strong>Use in Diffusion Models</strong>: In variational diffusion models, KL Divergence is used to regularize the latent space distribution to match a prior distribution (often a standard normal distribution). This helps in ensuring that the learned latent space is structured and follows the desired distribution.</p>
  </li>
</ul>

<h4 id="negative-log-likelihood-nll">Negative Log-Likelihood (NLL)</h4>
<ul>
  <li>
    <p><strong>Description</strong>:  Negative Log-Likelihood measures the likelihood of the data under the model, with a higher likelihood indicating a better model.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>

\[\mathcal{L}_{\text{NLL}} = - \log P(x)\]
  </li>
  <li>
    <p><strong>Use in Diffusion Models</strong>: In continuous diffusion models, NLL can be used to maximize the likelihood of the data given the reverse diffusion process. This involves computing the likelihood of generating the data from the noise distribution.</p>
  </li>
</ul>

<h4 id="evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</h4>
<ul>
  <li>
    <p><strong>Description</strong>:  ELBO is used in variational inference to approximate the true posterior distribution by maximizing a lower bound on the evidence.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>

\[\mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \text{KL}(q(z|x) || p(z))\]
  </li>
  <li>
    <p><strong>Use in Diffusion Models</strong>: In variational diffusion models, ELBO is used to optimize the generative process by balancing the reconstruction accuracy and the regularization of the latent space. The first term ensures that the model can reconstruct the input data, while the second term ensures that the latent space follows the desired distribution.</p>
  </li>
</ul>

<h4 id="hybrid-loss">Hybrid Loss</h4>
<ul>
  <li>
    <p><strong>Description</strong>:  Hybrid loss combines multiple loss functions to leverage their strengths and mitigate their weaknesses.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>
  </li>
</ul>

\[\mathcal{L}_{\text{hybrid}} = \alpha \mathcal{L}_{\text{MSE}} + \beta \mathcal{L}_{\text{KL}}\]

<ul>
  <li><strong>Use in Diffusion Models</strong>: Hybrid loss functions can be designed to combine MSE for denoising with KL Divergence to regularize the latent space, allowing for both effective denoising and structured latent space learning.</li>
</ul>

<h4 id="cross-entropy-loss">Cross-Entropy Loss</h4>
<ul>
  <li>
    <p><strong>Description</strong>: Cross-Entropy Loss measures the difference between two probability distributions and is often used for classification tasks.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>
  </li>
</ul>

\[\mathcal{L}_{\text{CE}} = -\sum_{i} y_i \log(\hat{y}_i)\]

<ul>
  <li><strong>Use in Diffusion Models</strong>: In discrete diffusion models, cross-entropy loss can be used when the model is learning to predict categorical distributions at each diffusion step, such as predicting discrete tokens in a text generation task.</li>
</ul>

<h4 id="variational-bound-loss">Variational Bound Loss</h4>
<ul>
  <li>
    <p><strong>Description</strong>: This loss is a combination of reconstruction loss and a KL divergence term, ensuring that the model generates samples that are close to the true data distribution.</p>
  </li>
  <li>
    <p><strong>Mathematical Formulation</strong>:</p>
  </li>
</ul>

\[\mathcal{L}_{\text{VB}} = \sum_{t} \mathbb{E}_{q(x_{t-1} \| x_t)} [ \log p(x_{t-1} \| x_t) ] - \text{KL}(q(x_t \| x_{t-1}) || p(x_t \| x_{t-1}))\]

<ul>
  <li><strong>Use in Diffusion Models</strong>: Variational bound loss is often used in continuous-time diffusion models to learn the reverse diffusion process by approximating the true posterior distribution.</li>
</ul>

<h4 id="summary-of-common-loss-functions-in-diffusion-models">Summary of Common Loss Functions in Diffusion Models</h4>

<ol>
  <li><strong>Mean Squared Error (MSE)</strong>: Measures the average of the squares of the errors between the predicted and actual values. Practically used as Denoising Score Matching (DSM) which trains models to estimate the score function, which is the gradient of the log probability density function of the data.</li>
  <li><strong>Kullback-Leibler Divergence (KL Divergence)</strong>: Measures how one probability distribution diverges from a second, expected probability distribution.</li>
  <li><strong>Negative Log-Likelihood (NLL)</strong>: Measures the likelihood of the data under the model.</li>
  <li><strong>Evidence Lower Bound (ELBO)</strong>: Used in variational inference to approximate the true posterior distribution.</li>
  <li><strong>Hybrid Loss</strong>: Combines multiple loss functions to leverage their strengths.</li>
  <li><strong>Cross-Entropy Loss</strong>: Measures the difference between two probability distributions, used for classification tasks.</li>
  <li><strong>Variational Bound Loss</strong>: Combines reconstruction loss and a KL divergence term.</li>
</ol>

<h3 id="what-is-the-denoising-score-matching-loss-in-diffusion-models-provide-equation-and-intuition">What is the Denoising Score Matching Loss in Diffusion models? Provide equation and intuition.</h3>

<ul>
  <li>The Denoising Score Matching Loss is a critical component in the training of diffusion models, a class of generative models. This loss function is designed to train the model to effectively reverse a diffusion process, which gradually adds noise to the data over a series of steps.</li>
  <li><strong>Denoising Score Matching Loss: Equation and Intuition</strong>
    <ul>
      <li><strong>Background:</strong>
        <ul>
          <li>In diffusion models, the data is incrementally noised over a sequence of steps. The reverse process, which the model learns, involves denoising or reversing this noise addition to recreate the original data from noise.</li>
          <li><strong>Equation:</strong></li>
          <li>The denoising score matching loss at a particular timestep \(t\) can be formulated as:
  \(L(\theta)=\mathbb{E}_{x_0, \epsilon \sim \mathcal{N}(0, I), t}\left[\left\|s_\theta\left(x_t, t\right)-\nabla_{x_t} \log p_{t \mid 0}\left(x_t \mid x_0\right)\right\|^2\right]\)
            <ul>
              <li>where, \(x_0\) is the original data, \(x_t\) is the noised data at timestep \(t\), and $\epsilon$ is the added Gaussian noise.</li>
              <li>\(s_\theta\left(x_t, t\right)\) is the score (gradient of the log probability) predicted by the model with parameters \(\theta\).</li>
              <li>\(\nabla_{x_t} \log p_{t \mid 0}\left(x_t \mid x_0\right)\) is the true score, which is the gradient of the log probability of the noised data \(x_t\) conditioned on the original data \(x_0\).</li>
            </ul>
          </li>
          <li><strong>Intuition:</strong>
            <ul>
              <li>The loss function encourages the model to predict the gradient of the log probability of the noised data with respect to the data itself. Essentially, it’s training the model to estimate how to reverse the diffusion process at each step.</li>
              <li>By minimizing this loss, the model learns to approximate the reverse of the noising process, thereby learning to generate data starting from noise.</li>
              <li>This process effectively teaches the model the denoising direction at each step of the noised data, guiding it on how to gradually remove noise and reconstruct the original data.</li>
            </ul>
          </li>
          <li><strong>Importance in Training:</strong> The denoising score matching loss is crucial for training diffusion models to generate high-quality samples. It ensures that the model learns a detailed and accurate reverse mapping of the diffusion process, capturing the complex data distribution.</li>
          <li><strong>Advantages:</strong> This approach allows diffusion models to generate samples that are often of higher quality and more diverse compared to other generative models, as it carefully guides the generative process through the learned noise reversal.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>In summary, the denoising score matching loss in diffusion models is fundamental in training these models to effectively reverse the process of gradual noise addition, enabling the generation of high-quality data samples from a noise distribution. This loss function is key to the model’s ability to learn the intricate details of the data distribution and the precise dynamics of the denoising process.</li>
</ul>

<h3 id="what-does-the-stable-in-stable-diffusion-refer-to">What does the “stable” in stable diffusion refer to?</h3>

<ul>
  <li>The “stability” in stable diffusion also refers to maintaining image content in the latent space throughout the diffusion process. In diffusion models, the image is transformed from the pixel space to the “latent space” – this is a high-dimensional abstract representation of the image. Here are the differences between the two:
    <ul>
      <li><strong>Pixel Space:</strong>
        <ul>
          <li>This refers to the space in which the data (such as images) is represented in its raw form – as pixels.</li>
          <li>Each dimension corresponds to a pixel value, so an image of size 100x100 would have a pixel space of 10,000 dimensions.</li>
          <li>Pixel space representations are direct and intuitive but can be very high-dimensional and sparse for complex data like images.</li>
        </ul>
      </li>
      <li><strong>Latent Space:</strong>
        <ul>
          <li>Latent space is a lower-dimensional space where data is represented in a more compressed and abstract form.</li>
          <li>Generative models, like Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), encode high-dimensional data (from pixel space) into this lower-dimensional latent space.</li>
          <li>The latent representation captures the essential features or characteristics of the data, allowing for more efficient processing and manipulation.</li>
          <li>Operations and transformations are often performed in latent space because they can be more meaningful and computationally efficient. For example, interpolating between two points in latent space can result in a smooth transition between two images when decoded back to pixel space.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The “Stable” in Stable Diffusion refers to the fact that the forward and reverse diffusion process occur in a low-dimensional latent space vs. a high-dimensional pixel space leading to stability during diffusion.
If the latent space becomes unstable and loses image content too quickly, the generated pixel space images will be poor.</li>
  <li>Stable diffusion uses techniques to keep the latent space more stable throughout the diffusion process:
    <ul>
      <li>The denoising model tries to remove noise while preserving latent space content at each step.</li>
      <li>Regularization prevents the denoising model from changing too drastically between steps.</li>
      <li>Careful noise scheduling maintains stability in early diffusion steps.</li>
    </ul>
  </li>
  <li>This stable latent space leads to higher quality pixel generations. At the end, Stable Diffusion transforms the image from latent space back to the pixel space.</li>
</ul>

<h3 id="how-do-you-condition-a-diffusion-model-to-the-textual-input-prompt">How do you condition a diffusion model to the textual input prompt?</h3>

<ul>
  <li>Conditioning a diffusion model on a textual input prompt is a key technique in generating content that aligns closely with textual descriptions, particularly useful in applications such as text-to-image generation. This process involves several steps and components to effectively integrate text-based conditioning into the generative process of diffusion models like DALL-E, Imagen, or similar systems. Here’s a detailed explanation of how it works:</li>
</ul>

<h4 id="text-encoding">Text Encoding</h4>
<ul>
  <li><strong>Text Encoder</strong>: The first step involves encoding the textual prompt into a continuous vector representation that the model can utilize. This is typically done using a transformer-based text encoder, such as those found in language models (BERT, GPT, etc.). The encoder translates the text prompt into a high-dimensional space, capturing semantic and syntactic nuances of the input text.</li>
  <li><strong>Embeddings</strong>: The output of the text encoder is a set of embeddings or feature vectors that represent different parts or aspects of the text. These embeddings serve as the basis for conditioning the diffusion process.</li>
</ul>

<h4 id="integrating-text-embeddings-into-the-diffusion-process">Integrating Text Embeddings into the Diffusion Process</h4>
<ul>
  <li><strong>Conditioning Layer</strong>: In the architecture of the diffusion model, there are typically one or more layers specifically designed to integrate the text embeddings with the image generation process. This might be done through mechanisms such as cross-attention, where the text embeddings (queries) interact with image features (keys and values) at various stages of the diffusion process.</li>
  <li><strong>Guidance Mechanisms</strong>: Techniques like classifier-free guidance can be employed, where the model is trained to generate both conditioned (on text) and unconditioned (no text) samples. During inference, the model uses a guidance scale to adjust the strength of the conditioning, amplifying the influence of the text on the generated images.</li>
</ul>

<h4 id="reverse-diffusion-with-textual-guidance">Reverse Diffusion with Textual Guidance</h4>
<ul>
  <li><strong>Starting from Noise</strong>: The diffusion model typically starts with a sample drawn from a Gaussian distribution (i.e., a noisy image) and progressively denoises it through a series of steps.</li>
  <li><strong>Conditional Denoising Steps</strong>: During each step of the reverse diffusion process, the model consults the text embeddings to adjust the denoising trajectory. This is done by calculating how the current state of the image needs to be altered to better reflect the textual prompt, using the gradients of the loss function that measures the difference between the current image and the target condition.</li>
  <li><strong>Iterative Refinement</strong>: With each step, the model refines the image, increasingly aligning it with the conditioning text. This involves repeatedly applying the learned conditional distributions to reduce noise and enhance details that correspond to the text.</li>
</ul>

<h4 id="sampling-and-optimization">Sampling and Optimization</h4>
<ul>
  <li><strong>Dynamic Adjustments</strong>: Throughout the reverse diffusion process, parameters such as the guidance scale can be adjusted to increase or decrease the influence of the text embeddings, allowing for dynamic control over the fidelity and creativity of the generated outputs.</li>
  <li><strong>Optimization Techniques</strong>: Advanced sampling techniques like Langevin dynamics or ancestral sampling may be used to navigate the probability distributions effectively, ensuring high-quality generation that closely matches the conditioning text.</li>
</ul>

<h4 id="evaluation-and-fine-tuning">Evaluation and Fine-Tuning</h4>
<ul>
  <li><strong>Quality and Relevance Checks</strong>: The outputs are typically evaluated for both quality (visual, aesthetic) and relevance (accuracy in reflecting the text prompt). Feedback from these evaluations can be used to fine-tune the text encoder, conditioning layers, or other components of the model.</li>
  <li>
    <p><strong>User Interaction</strong>: In practical applications, users might interact with the model by tweaking the text prompt or adjusting control parameters to iteratively refine the output until it meets their requirements.</p>
  </li>
  <li>In summary, Conditioning diffusion models on textual input requires a sophisticated interplay of text encoding, model architecture adaptations, and careful management of the generative process. This complexity allows the models to produce remarkably accurate visual representations from textual descriptions, enabling a wide range of applications from art generation to functional design assistance.</li>
</ul>

<h3 id="in-the-context-of-diffusion-models-what-role-does-cross-attention-play-how-are-the-q-k-and-v-abstractions-modeled-for-diffusion-models">In the context of diffusion models, what role does cross attention play? How are the \(Q\), \(K\), and \(V\) abstractions modeled for diffusion models?</h3>

<ul>
  <li>In the context of diffusion models, particularly those that are used for generating images conditioned on text (like DALL-E 2 or Imagen), cross-attention plays a crucial role in integrating information from different modalities, typically text and images. Here’s how cross-attention is used and how the Query (\(Q\)), Key (\(K\)), and Value (\(V\)) components are modeled within such systems:</li>
</ul>

<h4 id="role-of-cross-attention-in-diffusion-models">Role of Cross-Attention in Diffusion Models</h4>

<ul>
  <li><strong>Text-to-Image Synthesis</strong>: In diffusion models designed for tasks like text-to-image generation, cross-attention mechanisms enable the model to effectively align and utilize textual information to guide the image generation process. This is critical for producing images that accurately reflect the content described by the input text.</li>
  <li><strong>Conditional Generation</strong>: Cross-attention allows the diffusion model to focus on specific aspects of the text throughout the various steps of the diffusion process. This dynamic focusing is key to iteratively refining the generated image to better match the textual description.</li>
</ul>

<h4 id="modeling-q-k-and-v-in-diffusion-models">Modeling \(Q\), \(K\), and \(V\) in Diffusion Models</h4>

<ul>
  <li><strong>Source of \(Q\), \(K\), and \(V\)</strong>: In a typical setup for a text-to-image diffusion model, the text input is encoded into a series of embeddings that are used to generate the queries (\(Q\)). The evolving image representations (as the image is gradually denoised through the reverse diffusion process) are used to produce the keys (\(K\)) and values (\(V\)).</li>
</ul>

<h5 id="detailed-steps">Detailed Steps</h5>

<ol>
  <li><strong>Text Encoding</strong>:
    <ul>
      <li>The text description is processed by a text encoder (often a Transformer-based model), which converts the input text into a series of embeddings. These embeddings serve as the queries (\(Q\)) in the cross-attention mechanism. They represent what the model needs to focus on or include in the image.</li>
    </ul>
  </li>
  <li><strong>Image Representation</strong>:
    <ul>
      <li>At each step of the reverse diffusion process, the partially denoised image is encoded to generate keys (\(K\)) and values (\(V\)). The keys help the model to understand where in the current image representation the aspects described by the text (queries) are relevant.</li>
      <li>The values carry the actual visual content that could be adjusted or enhanced based on the alignment with the text description as determined by the attention scores.</li>
    </ul>
  </li>
  <li><strong>Attention Calculation</strong>:
    <ul>
      <li>Cross-attention calculates how much each part of the image (values) should be influenced by parts of the text (queries). This is done by computing attention scores based on the similarity between queries and keys. These scores dictate how much each element of the value should be adjusted in response to the corresponding textual information.</li>
    </ul>
  </li>
  <li><strong>Iterative Refinement</strong>:
    <ul>
      <li>During the reverse diffusion process, this cross-attention guided adjustment happens iteratively. With each step, the model refines the image further, enhancing areas of the image that need more detail or correction as per the text description.</li>
    </ul>
  </li>
</ol>

<h4 id="conclusion">Conclusion</h4>

<ul>
  <li>In diffusion models, cross-attention is a powerful tool for bridging the gap between textual descriptions and visual content, ensuring that the generated images are not only high-quality but also contextually accurate. The interaction between \(Q\), \(K\), and \(V\) within the cross-attention layers effectively enables the model to “attend” to relevant textual features while translating these cues into visual modifications, thereby tailoring the image generation process to the specifics of the input text.</li>
</ul>

<h3 id="how-is-randomness-in-the-outputs-induced-in-a-diffusion-model">How is randomness in the outputs induced in a diffusion model?</h3>

<ul>
  <li>Diffusion models inherently introduce randomness in their outputs as part of the generative process, which is a key feature allowing these models to produce diverse and high-quality samples. Here’s how randomness is systematically incorporated into the operation of diffusion models:</li>
</ul>

<h4 id="the-basic-framework-of-diffusion-models">The Basic Framework of Diffusion Models</h4>

<ul>
  <li>
    <p>Diffusion models operate on a principle of gradually adding noise to the data over a series of steps (forward process) and then learning to reverse this process to generate data from noise (reverse process). This structure is inherently probabilistic and relies heavily on randomness at multiple stages:</p>

    <ul>
      <li>
        <p><strong>Forward Process (Noise Addition)</strong>: In the forward process, data is progressively corrupted by adding Gaussian noise in a sequence of steps until it becomes indistinguishable from Gaussian noise. The noise levels typically increase according to a predetermined schedule, which is crucial for the model to learn the characteristics of the data at various levels of corruption.</p>
      </li>
      <li>
        <p><strong>Reverse Process (Noise Removal/Denoising)</strong>: The reverse process is where the model generates new data by starting with pure noise and progressively denoising it. This process is guided by the learned parameters but is fundamentally random due to the stochastic nature of the process and the initial noise state.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="randomness-in-sampling">Randomness in Sampling</h4>

<ul>
  <li>
    <p>The core mechanism through which randomness influences the outputs of diffusion models is through the sampling process during the reverse diffusion:</p>

    <ul>
      <li>
        <p><strong>Stochastic Sampling</strong>: At each step of the reverse process, the model predicts the mean and variance of the conditional distribution of the denoised data given the current noisy data. A sample is then drawn from this conditional distribution, typically assumed to be Gaussian. This sampling introduces randomness because the exact point sampled from the distribution can vary, leading to different outcomes each time the process is run.</p>
      </li>
      <li>
        <p><strong>Parameterization of Noise Levels</strong>: The variance of the noise added at each step can be a critical parameter that controls the amount of randomness. By adjusting this variance, one can control the diversity of the generated samples. Higher variance typically leads to more randomness and hence more diverse outputs.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="conditional-generation">Conditional Generation</h4>

<ul>
  <li>
    <p>In conditional diffusion models, such as those conditioned on text for image generation, randomness is also introduced in how the conditioning information influences the generation:</p>

    <ul>
      <li>
        <p><strong>Conditioning Mechanism</strong>: Although the text or other conditioning data guides the generation process, the interpretation of this data by the model can introduce variations. For instance, the text “a cat sitting on a mat” could lead to images of different cats, different mats, or different settings, depending on the randomness in the sampling steps of the model.</p>
      </li>
      <li>
        <p><strong>Influence of Latent Variables</strong>: Some diffusion models integrate latent variables that capture aspects of the data not specified by the conditioning input. These latent variables add another layer of randomness, allowing for variations in features that are not explicitly controlled by the input conditions.</p>
      </li>
    </ul>
  </li>
</ul>

<h4 id="temperature-scaling">Temperature Scaling</h4>

<ul>
  <li>Temperature scaling is a technique used in many generative models to control the randomness of the outputs:
    <ul>
      <li><strong>Temperature Factor</strong>: By adjusting a temperature parameter in the noise distribution (especially in the variance), the model can be made to produce more or less random (diverse) outputs. Lower temperatures lead to less noise and often more coherent, deterministic, and conservative outputs, while higher temperatures increase randomness and diversity.</li>
    </ul>
  </li>
</ul>

<h4 id="conclusion-1">Conclusion</h4>

<ul>
  <li>Randomness in diffusion models is fundamental to their design and functionality. It allows these models to generate diverse and creative outputs from a probabilistic foundation. The control of this randomness through model design and sampling parameters is key to harnessing diffusion models for practical applications, ensuring a balance between diversity, creativity, and fidelity to any conditioning inputs.</li>
</ul>

<h3 id="how-does-the-noise-schedule-work-in-diffusion-models-what-are-some-standard-noise-schedules">How does the noise schedule work in diffusion models? What are some standard noise schedules?</h3>

<ul>
  <li>Diffusion models are a class of generative models that learn to generate data by iteratively denoising a sample, starting from pure noise. A crucial component of these models is the noise schedule, which determines how noise is added during the forward diffusion process and how it is removed during the reverse denoising process.</li>
</ul>

<h4 id="noise-schedule-in-diffusion-models">Noise Schedule in Diffusion Models</h4>

<ul>
  <li>The noise schedule in diffusion models defines the variance of the noise added at each step during the forward process. This schedule affects the quality of the generated samples and the efficiency of the learning process. The noise schedule is often described by a series of variance values \(\beta_t\) or their cumulative products \(\alpha_t\) and \(\bar{\alpha}_t\), where \(t\) denotes the time step.</li>
</ul>

<h5 id="forward-diffusion-process-1">Forward Diffusion Process</h5>

<ul>
  <li>
    <p>In the forward process, noise is added to the data at each step \(t\) according to a predefined schedule:</p>

\[q(x_t \| x_{t-1}) = \mathcal{N}(x_t; \sqrt{1 - \beta_t} x_{t-1}, \beta_t I)\]

    <ul>
      <li>Here, \(\beta_t\) represents the variance of the noise added at step \(t\). The relationship between the cumulative products and variances is given by:</li>
    </ul>
  </li>
</ul>

\[\alpha_t = 1 - \beta_t
\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\]

<ul>
  <li>
    <p>The above expressions allow us to express the noisy sample at any step \(t\) as:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]

    <ul>
      <li>where \(\epsilon \sim \mathcal{N}(0, I)\) is standard Gaussian noise, and \(x_0\) is the original data point.</li>
    </ul>
  </li>
</ul>

<h5 id="reverse-denoising-process-1">Reverse Denoising Process</h5>

<ul>
  <li>The reverse process involves learning to denoise the samples iteratively, starting from \(x_T\), which is almost pure noise, to \(x_0\). The model is trained to approximate the reverse conditional distributions \(p_\theta(x_{t-1} \mid x_t)\), typically parameterized as Gaussian distributions whose means and variances depend on the current step \(t\) and the model’s parameters \(\theta\).</li>
</ul>

\[p_\theta(x_{t-1} \| x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))\]

<h4 id="standard-noise-schedules">Standard Noise Schedules</h4>

<ul>
  <li>
    <p>Several noise schedules have been proposed and used in practice, each with different properties and trade-offs. Some standard noise schedules include:</p>

    <ol>
      <li>
        <p><strong>Linear Schedule</strong>: The variances \(\beta_t\) are increased linearly from \(\beta_1\) to \(\beta_T\). \(\beta_t = \beta_{\text{start}} + \frac{t}{T} (\beta_{\text{end}} - \beta_{\text{start}})\). This schedule is simple and often used as a baseline.</p>
      </li>
      <li>
        <p><strong>Cosine Schedule</strong>: The variances are defined using a cosine function, which provides a smooth transition and is empirically found to perform well. \(\bar{\alpha}_t = \cos\left(\frac{t / T + s}{1 + s} \cdot \frac{\pi}{2}\right)^2\), where \(s\) is a small constant to avoid zero at \(t=0\).</p>
      </li>
      <li>
        <p><strong>Quadratic Schedule</strong>: The variances \(\beta_t\) follow a quadratic function.
\(\beta_t = (\beta_{\text{start}} + (\beta_{\text{end}} - \beta_{\text{start}}) \cdot (t/T)^2)\)</p>
      </li>
      <li>
        <p><strong>Exponential Schedule</strong>: The variances increase exponentially.
\(\beta_t = \beta_{\text{start}} \cdot \left(\frac{\beta_{\text{end}}}{\beta_{\text{start}}}\right)^{t/T}\)</p>
      </li>
      <li>
        <p><strong>Constant Schedule</strong>: The variances remain constant throughout the process.
\(\beta_t = \text{constant}\)</p>
      </li>
    </ol>
  </li>
</ul>

<h3 id="choosing-a-noise-schedule">Choosing a Noise Schedule</h3>

<ul>
  <li>
    <p>The choice of noise schedule affects the stability and performance of the diffusion model. It is often a hyperparameter that needs to be tuned for the specific application. Here are some considerations:</p>

    <ul>
      <li><strong>Linearity and Simplicity</strong>: Linear schedules are straightforward and often serve as a good starting point.</li>
      <li><strong>Smoothness</strong>: Smoother schedules like the cosine schedule can result in more stable training and better sample quality.</li>
      <li><strong>Model Capacity</strong>: More complex schedules might be beneficial if the model has high capacity and can learn intricate denoising processes.</li>
      <li><strong>Empirical Performance</strong>: Often, the best schedule is determined through experimentation and empirical evaluation on the target dataset.</li>
    </ul>
  </li>
  <li>
    <p>In summary, the noise schedule is a critical component of diffusion models, dictating how noise is introduced and removed through the forward and reverse processes. Various schedules, such as linear, cosine, quadratic, and exponential, provide different ways to balance the trade-offs between model complexity, stability, and sample quality.</p>
  </li>
</ul>

<h2 id="recent-papers">Recent Papers</h2>

<h3 id="high-resolution-image-synthesis-with-latent-diffusion-models"><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html">High-Resolution Image Synthesis with Latent Diffusion Models</a></h3>

<ul>
  <li>The following paper summary has been contributed by <a href="https://www.linkedin.com/in/zhibo-darren-zhang/">Zhibo Zhang</a>.</li>
  <li>Diffusion models are known to be computationally expensive given that they require many steps of diffusion and denoising diffusion operations in possibly high-dimensional input feature spaces.</li>
  <li>This paper by Rombach et al. from Ludwig Maximilian University of Munich &amp; IWR, Heidelberg University and Runway ML in CVPR 2022 introduces diffusion models that operate on the latent space, aiming at generating high-resolution images with lower computation demands compared to those that operate directly on the pixel space.</li>
  <li>In particular, the authors adopted an autoencoder that compresses the input images into a lower dimensional latent space. The autoencoder relies on either KL regularization or VQ regularization to constrain the variance of the latent space.</li>
  <li>As shown in the illustration figure below by Rombach et al., in the latent space, the latent representation of the input image goes through a total of \(T\) diffusion operations to get the noisy representation. A U-Net is then applied on top of the noisy representation for \(T\) iterations to produce the denoised version of the representation. In addition, the authors introduced a cross attention mechanism to condition the denoising process on other types of inputs such as text and semantic maps.</li>
</ul>

<p><img src="../../images/papers/LatentDiffusionModel.html" alt="" /></p>

<ul>
  <li>In the final stage, the denoised representation will be mapped back to the pixel space using the decoder to get the synthesized image.</li>
  <li>Empirically, the best performing latent diffusion model (with a carefully chosen downsampling factor) achieved competitive FID scores in image generation when comparing with a few other state-of-the-art generative models such as variations of generative adversarial nets on a few datasets including the CelebA-HQ dataset.</li>
  <li><a href="https://github.com/CompVis/latent-diffusion">Code</a></li>
</ul>

<h3 id="diffusion-model-alignment-using-direct-preference-optimization"><a href="https://arxiv.org/abs/2311.12908">Diffusion Model Alignment Using Direct Preference Optimization</a></h3>

<ul>
  <li>This paper by Wallace et al. from Salesforce AI and Stanford University proposes a novel method for aligning diffusion models to human preferences.</li>
  <li>The paper introduces Diffusion-DPO, a method adapted from Direct Preference Optimization (DPO), for aligning text-to-image diffusion models with human preferences. This approach is a significant shift from typical language model training, emphasizing direct optimization on human comparison data.</li>
  <li>Unlike typical methods that fine-tune pre-trained models using curated images and captions, Diffusion-DPO directly optimizes a policy that best satisfies human preferences under a classification objective. It re-formulates DPO to account for a diffusion model notion of likelihood using the evidence lower bound, deriving a differentiable objective.</li>
  <li>The authors utilized the Pick-a-Pic dataset, comprising 851K crowdsourced pairwise preferences, to fine-tune the base model of the Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. The fine-tuned model showed significant improvements over both the base SDXL-1.0 and its larger variant in terms of visual appeal and prompt alignment, as evaluated by human preferences.</li>
  <li>The paper also explores a variant of the method that uses AI feedback, showing comparable performance to training on human preferences. This opens up possibilities for scaling diffusion model alignment methods.</li>
  <li>The figure below from paper illustrates: (Top) DPO-SDXL significantly outperforms SDXL in human evaluation. (L) PartiPrompts and (R) HPSv2 benchmark results across three evaluation questions, majority vote of 5 labelers. (Bottom) Qualitative comparisons between SDXL and DPO-SDXL. DPOSDXL demonstrates superior prompt following and realism. DPO-SDXL outputs are better aligned with human aesthetic preferences, favoring high contrast, vivid colors, fine detail, and focused composition. They also capture fine-grained textual details more faithfully.</li>
</ul>

<p><img src="../../images/papers/Diffusion-DPO.jpg" alt="" /></p>

<ul>
  <li>Experiments demonstrate the effectiveness of Diffusion-DPO in various scenarios, including image-to-image editing and learning from AI feedback. The method significantly outperforms existing models in human evaluations for general preference, visual appeal, and prompt alignment.</li>
  <li>The paper’s findings indicate that Diffusion-DPO can effectively increase measured human appeal across an open vocabulary with stable training, without increased inference time, and improves generic text-image alignment.</li>
  <li>The authors note ethical considerations and risks associated with text-to-image generation, emphasizing the importance of diverse and representative sets of labelers and the potential biases inherent in the pre-trained models and labeling process.</li>
  <li>In summary, the paper presents a groundbreaking approach to align diffusion models with human preferences, demonstrating notable improvements in visual appeal and prompt alignment. It highlights the potential of direct preference optimization in the realm of text-to-image diffusion models and opens avenues for further research and application in this field.</li>
</ul>

<h3 id="scalable-diffusion-models-with-transformers"><a href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a></h3>

<ul>
  <li>This paper by Peebles and Xie from UC Berkeley and New York University introduces a new class of diffusion models that leverage the Transformer architecture for generating images. This innovative approach replaces the traditional convolutional U-Net backbone in latent diffusion models (LDMs) with a transformer operating on latent patches.</li>
  <li>Traditional diffusion models in image-level generative tasks predominantly use a convolutional U-Net architecture. However, the dominance of transformers in various domains like natural language processing and vision prompts this exploration of their use as a backbone for diffusion models.</li>
  <li>The paper proposes Diffusion Transformers (DiTs), which adhere closely to the standard Vision Transformer (ViT) model but with some vital tweaks. DiTs are designed to be faithful to standard transformer architecture, particularly the Vision Transformer (ViT) model, and are trained as latent diffusion models of images.</li>
  <li><strong>Transformer Blocks and Design Space</strong>:
    <ul>
      <li>DiTs process input tokens transformed from spatial representations of images (“patchify” process) through a sequence of transformer blocks.</li>
      <li>Four types of transformer blocks are explored: in-context conditioning, cross-attention block, adaptive layer norm (adaLN) block, and adaLN-Zero block. Each block processes additional conditional information like noise timesteps or class labels.</li>
      <li>The adaLN-Zero block, which initializes each DiT block as an identity function and modulates the activations immediately prior to any residual connections within the block, demonstrates the most efficient performance, achieving lower Frechet Inception Distance (FID) values than the other block types.</li>
    </ul>
  </li>
  <li>The figure below from the paper shows the Diffusion Transformer (DiT) architecture. Left: We train conditional latent DiT models. The input latent is decomposed into patches and processed by several DiT blocks. Right: Details of our DiT blocks. We experiment with variants of standard transformer blocks that incorporate conditioning via adaptive layer norm, cross-attention and extra input tokens. Adaptive layer norm works best.</li>
</ul>

<p><img src="../../images/papers/DiT.html" alt="" /></p>

<ul>
  <li><strong>Model Scaling and Performance</strong>:
    <ul>
      <li>DiTs are scalable in terms of forward pass complexity, measured in GFLOPs. Different configurations (DiT-S, DiT-B, DiT-L, DiT-XL) cover a range of model sizes and computational complexities.</li>
      <li>Increasing model size and decreasing patch size significantly improves performance. FID scores improve as the transformer becomes deeper and wider, indicating that scaling model size (GFLOPs) is key to improved performance.</li>
      <li>The largest DiT-XL/2 models outperform all prior diffusion models, achieving a state-of-the-art FID of 2.27 on class-conditional ImageNet benchmarks at resolutions of 256 \(\times\) 256 and 512 \(\times\) 512.</li>
    </ul>
  </li>
  <li><strong>Implementation and Results</strong>: The models are trained using the AdamW optimizer. The DiT-XL/2 model, trained for 7 million steps, demonstrates high compute efficiency compared to both latent and pixel space U-Net models.</li>
  <li><strong>Visual Quality</strong>: The paper highlights notable improvements in the visual quality of generated images with scaling in both model size and the number of tokens processed.</li>
  <li>Overall, the paper showcases the potential of transformer-based architectures in diffusion models, emphasizing scalability and compute efficiency, which contributes significantly to the field of generative models for images.</li>
  <li><a href="https://www.wpeebles.com/DiT">Project page</a></li>
</ul>

<h3 id="deepfloyd-if"><a href="https://github.com/deep-floyd/IF">DeepFloyd IF</a></h3>

<ul>
  <li>DeepFloyd, a part of Stability AI, has introduced DeepFloyd IF, a cutting-edge text-to-image cascaded pixel diffusion model known for its high photorealism and language understanding capabilities. This model is an open-source project and represents a significant advancement in text-to-image synthesis technology.</li>
  <li>DeepFloyd IF is built with multiple neural modules (independent neural networks that tackle specific tasks), joining forces within a single architecture to produce a synergistic effect.</li>
  <li>DeepFloyd IF generates high-resolution images in a cascading manner: the action kicks off with a base model that produces low-resolution samples, which are then boosted by a series of upscale models to create stunning high-resolution images, as shown in the figure <a href="https://www.deepfloyd.ai/deepfloyd-if">(source)</a> below.</li>
</ul>

<p><img src="../../images/papers/DeepFloydIF2.html" alt="" /></p>

<ul>
  <li>DeepFloyd IF’s base and super-resolution models adopt diffusion models, making use of Markov chain steps to introduce random noise into the data, before reversing the process to generate new data samples from the noise.</li>
  <li>DeepFloyd IF operates within the pixel space, as opposed to latent diffusion (e.g. Stable Diffusion) that depends on latent image representations.</li>
  <li>The unique structure of DeepFloyd IF consists of a frozen text encoder and three cascaded pixel diffusion modules. The process begins with a base model generating a 64 \(\times\) 64 pixel image from a text prompt. This is followed by two super-resolution models, each escalating the resolution to 256 \(\times\) 256 pixels and then to 1024 \(\times\) 1024 pixels. All stages utilize a frozen text encoder based on the T5 transformer architecture, which extracts text embeddings. These embeddings are then input into a UNet architecture, which is enhanced with cross-attention and attention pooling features.</li>
  <li>The figure below from the paper shows the model architecture of DeepFloyd IF.</li>
</ul>

<p><img src="../../images/papers/DeepFloydIF.html" alt="" /></p>

<ul>
  <li>The efficiency and effectiveness of DeepFloyd IF are evident in its performance, where it achieved a zero-shot FID score of 6.66 on the COCO dataset. This score is a testament to its state-of-the-art capabilities, outperforming other models in the domain. The success of DeepFloyd IF underscores the potential of larger UNet architectures in the initial stages of cascaded diffusion models and opens new avenues for future advancements in text-to-image synthesis.</li>
  <li><a href="https://github.com/deep-floyd/IF">Code</a>; <a href="https://www.deepfloyd.ai/deepfloyd-if">Project page</a>.</li>
</ul>

<h3 id="pixart-alpha-fast-training-of-diffusion-transformer-for-photorealistic-text-to-image-synthesis"><a href="https://arxiv.org/abs/2310.00426">PIXART-\(\alpha\): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis</a></h3>

<ul>
  <li>The paper “PIXART-\(\alpha\): Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis” by Chen et al. introduces PIXART-\(\alpha\), a transformer-based latent diffusion model for text-to-image (T2I) synthesis. This model competes with leading image generators such as SDXL, Imagen, and DALL-E 2 in quality, while significantly reducing training costs and CO2 emissions. Notably, it is also OPEN-RAIL licensed.</li>
  <li><strong>Key Innovations</strong>:
    <ul>
      <li><strong>Efficient Architecture</strong>: PIXART-\(\alpha\) employs a Diffusion Transformer (DiT) with cross-attention modules, focusing on efficiency. This includes a streamlined class-condition branch and reparameterization for efficient training.</li>
      <li><strong>Training Strategy Decomposition</strong>: The training is divided into three stages: learning pixel distributions, text-image alignment, and aesthetic enhancement.</li>
      <li><strong>High-Informative Data</strong>: Utilizes an auto-labeling pipeline with LLaVA to create a dense, precise text-image dataset, improving the speed of text-image alignment learning.</li>
    </ul>
  </li>
  <li><strong>Technical Implementation</strong>:
    <ul>
      <li><strong>Text Encoding</strong>: Uses the T5-XXL model for advanced text encoding, enabling better handling of complex prompts.</li>
      <li><strong>Pre-training and Stages</strong>: Incorporates pre-training on ImageNet, learning stages for pixel distribution, alignment, and aesthetics.</li>
      <li><strong>Hardware Requirements</strong>: Initially requires 23GB GPU VRAM, but with diffusers, it can run under 7GB.</li>
    </ul>
  </li>
  <li>The figure below from the paper shows the model architecture of PIXART-\(\alpha\). A cross-attention module is integrated into each block to inject textual conditions. To optimize efficiency, all blocks share the same adaLN-single parameters for time conditions.</li>
</ul>

<p><img src="../../images/papers/PIXART.html" alt="" /></p>

<ul>
  <li><strong>Performance and Efficiency</strong>:
    <ul>
      <li><strong>Quality and Control</strong>: Delivers high-quality image synthesis with superior semantic control.</li>
      <li><strong>Resource Efficiency</strong>: Achieves near state-of-the-art quality with only 2% of the training cost of other models, reducing CO2 emissions by 90%.</li>
      <li><strong>Optimization Techniques</strong>: Implements shared normalization parameters (adaLN-single) and uses AdamW optimizer to enhance efficiency.</li>
    </ul>
  </li>
  <li><strong>Applications and Extensions</strong>: Showcases versatility through methods like DreamBooth and ControlNet, further expanding its practical applications.</li>
  <li>PIXART-\(\alpha\) represents a major advancement in T2I generation, offering a high-quality, efficient, and environmentally friendly solution. Its unique architecture and training strategy make it an innovative addition to the field of photorealistic T2I synthesis.</li>
  <li><a href="https://github.com/PixArt-alpha/PixArt-alpha">Code</a>; <a href="https://huggingface.co/docs/diffusers/main/en/api/pipelines/pixart">Hugging Face</a>; <a href="https://pixart-alpha.github.io/">Project page</a></li>
</ul>

<h3 id="raphael-text-to-image-generation-via-large-mixture-of-diffusion-paths"><a href="https://arxiv.org/abs/2305.18295">RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths</a></h3>

<ul>
  <li>This technical report by Xue et al. from the University of Hong Kong and SenseTime Research, the authors introduce RAPHAEL, a novel text-to-image diffusion model that generates highly artistic images closely aligned with textual prompts.</li>
  <li>RAPHAEL uniquely combines tens of mixture-of-experts (MoEs) layers, including space-MoE and time-MoE layers, allowing billions of diffusion paths. Each path intuitively functions as a “painter” for depicting specific textual concepts onto designated image regions at certain diffusion timesteps. This mechanism substantially enhances the precision in aligning text and image content.</li>
  <li>The authors report that RAPHAEL outperforms recent models like Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2 in terms of image quality and aesthetic appeal. This is evidenced by superior performance in diverse styles (e.g., Japanese comics, realism, cyberpunk) and a state-of-the-art zero-shot FID score of 6.61 on the COCO dataset.</li>
  <li>An edge-supervised learning module is introduced to further refine image quality, focusing on maintaining intricate boundary details in various styles.
RAPHAEL is implemented using a U-Net architecture with 16 transformer blocks, each containing a self-attention layer, a cross-attention layer, space-MoE, and time-MoE layers. The model, with three billion parameters, was trained on 1,000 A100 GPUs for two months.</li>
  <li>Framework of RAPHAEL. (a) Each block contains four primary components including a selfattention layer, a cross-attention layer, a space-MoE layer, and a time-MoE layer. The space-MoE is responsible for depicting different text concepts in specific image regions, while the time-MoE handles different diffusion timesteps. Each block uses edge-supervised cross-attention learning to further improve image quality. (b) shows details of space-MoE. For example, given a prompt “a furry bear under sky”, each text token and its corresponding image region (given by a binary mask) are directed through distinct space experts, i.e., each expert learns particular visual features at a region. By stacking several space-MoEs, we can easily learn to depict thousands of text concepts.</li>
</ul>

<p><img src="../../images/papers/RAPHAEL.html" alt="" /></p>

<ul>
  <li>The authors conducted extensive experiments, including a user study using the ViLG-300 benchmark, demonstrating RAPHAEL’s robustness and superiority in generating images that closely conform to the textual prompts. The study also showcases RAPHAEL’s flexibility in generating images of diverse styles and high resolutions up to 4096 \(\times\) 6144 when combined with a tailor-made SR-GAN model.</li>
  <li>RAPHAEL’s potential applications extend to various domains, with implications for both academic research and industry. The model’s limitations include the potential misuse for creating misleading or false information, a challenge common to powerful text-to-image generators.</li>
  <li><a href="https://raphael-painter.github.io/">Project page</a></li>
</ul>

<h3 id="ernie-vilg-20-improving-text-to-image-diffusion-model-with-knowledge-enhanced-mixture-of-denoising-experts"><a href="https://arxiv.org/abs/2210.15257">ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts</a></h3>

<ul>
  <li>This paper by Feng et al. from Baidu Inc. and Wuhan University of Science and Technology in CVPR 2023 focuses on enhancing text-to-image generation using diffusion models.</li>
  <li>They introduce ERNIE-ViLG 2.0, a large-scale Chinese text-to-image generation model, employing a diffusion-based approach with a 24B parameter scale. The model aims to significantly upgrade image quality and text relevancy.</li>
  <li>The model incorporates fine-grained textual and visual knowledge to improve semantic control and resolve object-attribute mismatching in image generation. This is achieved by using a text parser and an object detector to identify key elements in the text-image pair and aligning them in the learning process.</li>
  <li>Introduction of the Mixture-of-Denoising-Experts (MoDE) mechanism, which uses multiple specialized expert networks for different stages of the denoising process, allowing for more efficient handling of various denoising requirements at different steps.</li>
  <li>The figure below from the paper shows the architecture of ERNIE-ViLG 2.0, which incorporates fine-grained textual and visual knowledge of key elements in the scene and utilizes different denoising experts at different denoising stages.</li>
</ul>

<p><img src="../../images/papers/ERNIE-ViLG2.html" alt="" /></p>

<ul>
  <li>ERNIE-ViLG 2.0 demonstrates state-of-the-art performance on MS-COCO with a zero-shot FID-30k score of 6.75. It also outperforms models like DALL-E 2 and Stable Diffusion in human evaluations using a bilingual prompt set, ViLG-300, for a fair comparison between English and Chinese text-to-image models.</li>
  <li>The model’s implementation involves a transformer-based text encoder with 1.3B parameters, 10 denoising U-Net experts with 2.2B parameters each, and training on 320 Tesla A100 GPUs for 18 days. The dataset comprises 170M image-text pairs, including English datasets translated into Chinese.</li>
  <li>Ablation studies and qualitative showcases confirm the effectiveness of the proposed knowledge enhancement strategies and the MoDE mechanism. The model shows improved handling of complex prompts, better sharpness, and texture in generated images.</li>
  <li>Future work includes enriching external image-text alignment knowledge and expanding the usage of multiple experts to advance generation capabilities. The paper also discusses potential risks and limitations related to data bias and model misuse in text-to-image generation.</li>
  <li><a href="https://wenxin.baidu.com/ernie-vilg">Project page</a></li>
</ul>

<h3 id="imagen-video-high-definition-video-generation-with-diffusion-models"><a href="https://arxiv.org/abs/2210.02303">Imagen Video: High Definition Video Generation with Diffusion Models</a></h3>

<ul>
  <li>This paper by Ho et al. from Google Research, Brain Team, introduces Imagen Video, a text-conditional video generation system leveraging a cascade of video diffusion models. Imagen Video generates high-definition videos from text prompts using a base video generation model and a sequence of interleaved spatial and temporal super-resolution models.</li>
  <li>The core contributions and methodology of this work include the following technical details:
    <ul>
      <li><strong>Architecture and Components:</strong> Imagen Video utilizes a frozen T5 text encoder to process the text prompts, followed by a base video diffusion model and multiple spatial and temporal super-resolution (SSR and TSR) models. Specifically, the system comprises seven sub-models: one base video generation model, three SSR models, and three TSR models. This cascade structure allows the system to generate 1280x768 resolution videos at 24 frames per second, with a total of 128 frames (approximately 5.3 seconds).</li>
      <li><strong>Diffusion Models:</strong> The diffusion models in Imagen Video are based on continuous-time formulations, with a forward process defined as a Gaussian process. The training objective is to denoise the latent variables through a noise-prediction loss. The v-parameterization is employed to predict noise, which ensures numerical stability and avoids color-shifting artifacts.</li>
      <li><strong>Text Conditioning and Cascading:</strong> Text conditioning is achieved by injecting contextual embeddings from the T5-XXL text encoder into all models, ensuring alignment between the generated video and the text prompt. The cascading approach involves generating a low-resolution video first, which is then progressively enhanced through spatial and temporal super-resolution models. This method allows for high-resolution outputs without overly complex individual models.</li>
    </ul>
  </li>
  <li>The following figure from the paper shows the cascaded sampling pipeline starting from a text prompt input to generating a 5.3-second, 1280 \(\times\) 768 video at 24fps. “SSR” and “TSR” denote spatial and temporal super-resolution respectively, and videos are labeled as frames \(\times\) width \(\times\) height. In practice, the text embeddings are injected into all models, not just the base model.</li>
</ul>

<p><img src="../../images/papers/ImagenVideo.html" alt="" /></p>

<ul>
  <li><strong>Implementation Details:</strong>
    <ul>
      <li><strong>v-parameterization:</strong> Used for numerical stability and to avoid artifacts in high-resolution video generation.</li>
      <li><strong>Conditioning Augmentation:</strong> Gaussian noise augmentation is applied to the conditioning inputs during training to reduce domain gaps and facilitate parallel training of different models in the cascade.</li>
      <li><strong>Joint Training on Images and Videos:</strong> The models are trained on a mix of video-text pairs and image-text pairs, treating individual images as single-frame videos. This approach allows the model to leverage larger and more diverse image-text datasets.</li>
      <li><strong>Classifier-Free Guidance:</strong> This method enhances sample fidelity and ensures that the generated video closely follows the text prompt by adjusting the denoising prediction.</li>
      <li><strong>Progressive Distillation:</strong> This technique is used to speed up the sampling process. It involves distilling a trained DDIMs sampler into a model requiring fewer steps, thus significantly reducing computation time while maintaining sample quality.</li>
    </ul>
  </li>
  <li><strong>Experiments and Findings:</strong>
    <ul>
      <li>The model shows high fidelity in video generation and can produce diverse content, including 3D object understanding and various artistic styles.</li>
      <li>Scaling the parameter count of the video U-Net leads to improved performance, indicating that video modeling benefits significantly from larger models.</li>
      <li>The v-parameterization outperforms ε-parameterization, especially at higher resolutions, due to faster convergence and reduced color inconsistencies.</li>
      <li>Distillation reduces sampling time by 18x, making the model more efficient without sacrificing perceptual quality.</li>
    </ul>
  </li>
  <li><strong>Conclusion:</strong> Imagen Video extends text-to-image diffusion techniques to video generation, achieving high-quality, temporally consistent videos. The integration of various advanced methodologies from image generation, such as v-parameterization, conditioning augmentation, and classifier-free guidance, demonstrates their effectiveness in the video domain. The work also highlights the potential for further improvements in video generation capabilities through continued research and development.</li>
  <li><a href="https://imagen.research.google/video/">Project page</a></li>
</ul>

<h3 id="patch-n-pack-navit-a-vision-transformer-for-any-aspect-ratio-and-resolution"><a href="https://arxiv.org/abs/2307.06304">Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</a></h3>

<ul>
  <li>This paper by Dehghani et al. from Google DeepMind introduces NaViT (Native Resolution ViT), a vision transformer designed to process images of arbitrary resolutions and aspect ratios without resizing them to a fixed resolution, which is common but suboptimal.</li>
  <li>NaViT leverages sequence packing during training, a technique inspired by natural language processing where multiple examples are packed into a single sequence, allowing efficient training on variable length inputs. This is termed Patch n’ Pack.</li>
  <li><strong>Architectural Changes</strong>: NaViT builds on the Vision Transformer (ViT) but introduces masked self-attention and masked pooling to prevent different examples from attending to each other. It also uses factorized and fractional positional embeddings to handle arbitrary resolutions and aspect ratios. These embeddings are decomposed into separate embeddings for x and y coordinates and summed together, allowing for easy extrapolation to unseen resolutions.</li>
  <li><strong>Training Enhancements</strong>: NaViT employs continuous token dropping, varying the token dropping rate per image, and resolution sampling, allowing mixed-resolution training by sampling from a distribution of image sizes while preserving aspect ratios. This enhances throughput and exposes the model to high-resolution images during training, yielding substantial performance improvements over equivalent ViTs.</li>
  <li><strong>Efficiency</strong>: NaViT demonstrates significant computational efficiency, processing five times more images during training than ViT within the same compute budget. The O(n^2) cost of attention, a concern when packing multiple images into longer sequences, diminishes with model scale, making the attention cost a smaller proportion of the overall computation.</li>
  <li>The following figure from the paper shows an example packing enables variable resolution images with preserved aspect ratio, reducing training time, improving
performance and increasing flexibility. We show here the aspects of the data preprocessing and modelling that need to be modified to support Patch n’ Pack.
The position-wise operations in the network, such as MLPs, residual connections, and layer normalisations, do not need to be altered.</li>
</ul>

<p><img src="../../images/papers/NaViT.html" alt="" /></p>

<ul>
  <li><strong>Implementation</strong>: The authors implemented NaViT using a greedy packing approach to fix the final sequence lengths containing multiple examples. They addressed padding issues and example-level loss computation by modifying pooling heads to account for packing and using chunked contrastive loss to manage memory and time constraints.</li>
  <li><strong>Performance</strong>: NaViT consistently outperforms ViT across various tasks, including image and video classification, object detection, and semantic segmentation. It shows improved results on robustness and fairness benchmarks, achieving better performance with lower computational costs and providing flexibility in handling different resolutions during inference.</li>
  <li><strong>Evaluation</strong>: NaViT’s training and adaptation efficiency were evaluated through empirical studies on datasets like ImageNet, LVIS, WebLI, and ADE20k. The model demonstrated superior performance in terms of accuracy and computational efficiency, highlighting the benefits of preserving aspect ratios and using mixed-resolution training.</li>
  <li>NaViT represents a significant departure from the traditional convolutional neural network (CNN)-designed pipelines, offering a promising direction for Vision Transformers by enabling flexible and efficient processing of images at their native resolutions.</li>
</ul>

<h3 id="sdxl-improving-latent-diffusion-models-for-high-resolution-image-synthesis"><a href="https://arxiv.org/abs/2307.01952">SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis</a></h3>

<ul>
  <li>This paper by Podell et al. from Stability AI Applied Research details significant advancements in the field of text-to-image synthesis using latent diffusion models (LDMs).</li>
  <li>The paper introduces SDXL, a latent diffusion model that significantly improves upon previous versions of Stable Diffusion for text-to-image synthesis.</li>
  <li>SDXL incorporates a UNet architecture three times larger than its predecessors, primarily due to an increased number of attention blocks and a larger cross-attention context. This is achieved by using a second text encoder, significantly enhancing the model’s capabilities.</li>
  <li>Novel conditioning schemes are introduced, such as conditioning on original image resolution and cropping parameters. This conditioning is achieved through Fourier feature encoding and significantly improves the model’s performance and flexibility.</li>
  <li>SDXL is trained on multiple aspect ratios, a notable departure from standard square image outputs. This training approach allows the model to better handle images with varied aspect ratios, reflecting real-world data more accurately.</li>
  <li>An improved autoencoder is used, enhancing the fidelity of generated images, particularly in high-frequency details.</li>
  <li>The paper also discusses a refinement model used as a post-hoc image-to-image technique to further improve the visual quality of samples generated by SDXL.
SDXL demonstrates superior performance compared to earlier versions of Stable Diffusion and rivals state-of-the-art black-box image generators. The model’s performance was validated through user studies and quantitative metrics.</li>
  <li>The figure below from the illustrates: (Left) Comparing user preferences between SDXL and Stable Diffusion 1.5 &amp; 2.1. While SDXL already clearly outperforms Stable Diffusion 1.5 &amp; 2.1, adding the additional refinement stage boosts performance. (Right) Visualization of the two-stage pipeline: They generate initial latents of size 128 \(\times\) 128 using SDXL. Afterwards, they utilize a specialized high-resolution refinement model and apply SDEdit on the latents generated in the first step, using the same prompt. SDXL and the refinement model use the same autoencoder.</li>
</ul>

<p><img src="../../images/papers/SDXL.html" alt="" /></p>

<ul>
  <li>The authors emphasize the open nature of SDXL, highlighting its potential to foster transparency in large model training and evaluation, which is crucial for responsible and ethical deployment of such technologies.</li>
  <li>The paper represents a significant step forward in generative modeling for high-resolution image synthesis, showcasing the potential of latent diffusion models in creating detailed and realistic images from textual descriptions.</li>
</ul>

<h3 id="dreamix-video-diffusion-models-are-general-video-editors"><a href="https://arxiv.org/abs/2302.01329">Dreamix: Video Diffusion Models are General Video Editors</a></h3>

<ul>
  <li>Text-driven image and video diffusion models have recently achieved unprecedented generation realism. While diffusion models have been successfully applied for image editing, very few works have done so for video editing.</li>
  <li>This paper by Molad et al. from Google Research and The Hebrew University of Jerusalem presents the first diffusion-based method that is able to perform text-based motion and appearance editing of general videos. Our approach uses a video diffusion model to combine, at inference time, the low-resolution spatio-temporal information from the original video with new, high resolution information that it synthesized to align with the guiding text prompt.</li>
  <li>The following figure from the paper shows the video editing use-case with Dreamix: Frames from a video conditioned on the text prompt “A bear dancing and jumping to upbeat music, moving his whole body“. Dreamix transforms the eating monkey (top row) into a dancing bear, affecting appearance and motion (bottom row). It maintains fidelity to color, posture, object size and camera pose, resulting in a temporally consistent video.</li>
</ul>

<p><img src="../../images/papers/Dreamix1.html" alt="" /></p>

<ul>
  <li>As obtaining high-fidelity to the original video requires retaining some of its high-resolution information, we add a preliminary stage of finetuning the model on the original video, significantly boosting fidelity.</li>
  <li>They propose to improve motion editability by a new, mixed objective that jointly finetunes with full temporal attention and with temporal attention masking.</li>
  <li>They further introduce a new framework for image animation. They first transform the image into a coarse video by simple image processing operations such as replication and perspective geometric projections, and then use their general video editor to animate it.</li>
  <li>As a further application, Dreamix can be used for subject-driven video generation. Extensive qualitative and numerical experiments showcase the remarkable editing ability of Dreamix and establish its superior performance compared to baseline methods.</li>
  <li>The following figure from the paper illustrates the process of inference. Dreamix supports multiple applications by application dependent pre-processing (left), converting the input content into a uniform video format. For image-to-video, the input image is duplicated and transformed using perspective transformations, synthesizing a coarse video with some camera motion. For subject-driven video generation, the input is omitted - finetuning alone takes care of the fidelity. This coarse video is then edited using their general “Dreamix Video Editor“ (right): we first corrupt the video by downsampling followed by adding noise. We then apply the finetuned text-guided VDM, which upscales the video to the final spatio-temporal resolution.</li>
</ul>

<p><img src="../../images/papers/Dreamix2.html" alt="" /></p>

<ul>
  <li><a href="https://dreamix-video-editing.github.io/">Code</a></li>
</ul>

<h3 id="stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets"><a href="https://static1.squarespace.com/static/6213c340453c3f502425776e/t/655ce779b9d47d342a93c890/1700587395994/stable_video_diffusion.pdf">Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></h3>

<ul>
  <li>This paper by Blattmann et al. from Stability AI introduces Stable Video Diffusion (SVD), a latent video diffusion model designed for high-resolution text-to-video and image-to-video generation. They address the challenge of lacking a unified strategy for curating video data and propose a methodical curation process for training successful video LDMs, which includes three stages:
    <ul>
      <li>Stage I: Text-to-image (or simply, image pretraining), i.e., a 2D text-to-image diffusion model.</li>
      <li>Stage II: video pretraining, which trains on large amounts of videos.</li>
      <li>Stage III: video finetuning, which refines the model on a small subset of high-quality videos at higher resolution.</li>
    </ul>
  </li>
  <li>In the initial stage, leveraging insights from large-scale image model training, the authors curated an extensive pretraining dataset named LVD, consisting of approximately 580 million annotated video clip pairs. This dataset underwent rigorous processing, including cut detection and annotation using several methods such as image captioning and optical flow analysis, to filter out low-quality content. Specifically, to avoid the samples in the dataset that can be expected to degrade the performance of the final video model, such as clips with less motion, excessive text presence, or generally low aesthetic value, they therefore additionally annotate the dataset with dense optical flow calculated at 2 FPS, with which static scenes are filtered out by removing any videos whose average optical flow magnitude is below a certain threshold.</li>
  <li>The following figure from the paper shows that the initial dataset contains many static scenes and cuts which hurts training of generative video models. Left: Average number of clips per video before and after our processing, revealing that our pipeline detects lots of additional cuts. Right: The distribution of average optical flow score for one of these subsets before processing, which contains many static clips.</li>
</ul>

<p><img src="../../images/papers/SVD.html" alt="" /></p>

<ul>
  <li>The paper outlines the importance of each training stage and demonstrates that systematic data curation significantly boosts model performance. Notably, they emphasize the necessity of pretraining on a well-curated dataset for generating high-quality videos, showing that models pretrained in this manner outperform others when finetuned on smaller, high-quality datasets.</li>
  <li>Leveraging the curated dataset, the authors trained a base model that provides a comprehensive motion representation. This base model was further finetuned for several applications, including text-to-video and image-to-video generation, demonstrating state-of-the-art performance. The model also supports controlled camera motion through LoRA modules and has been shown to serve as a robust multi-view 3D prior, capable of generating multiple consistent views of an object in a feedforward manner.</li>
  <li>The SVD model stands out for its ability to efficiently generate high-fidelity videos from both text and images, offering a substantial advancement over existing methods in terms of visual quality and consistency. The authors released the code and model weights, contributing a valuable resource to the research community for further exploration and development in video generation technology.</li>
  <li><a href="https://stability.ai/news/stable-video-diffusion-open-ai-video-model">Blog</a>; <a href="https://github.com/Stability-AI/generative-models">Code</a>; <a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt">Hugging Face</a></li>
</ul>

<h2 id="fine-tuning-diffusion-models">Fine-tuning Diffusion Models</h2>

<ul>
  <li>Per <a href="https://huggingface.co/blog/lora">Using LoRA for Efficient Stable Diffusion Fine-Tuning</a>, Low-Rank Adaptation (LoRA) can be used for efficient fine-tuning of large language models, originally introduced by Microsoft researchers.</li>
  <li>LoRA involves freezing pre-trained model weights and adding trainable layers to reduce the number of parameters and GPU memory requirements. It has been applied to Stable Diffusion fine-tuning, particularly in cross-attention layers.</li>
  <li>The technique enables quicker and less computationally intensive training, resulting in much smaller trained weights. The <a href="https://huggingface.co/blog/lora">article</a> also covers the use of LoRA in diffusers for Dreambooth and full fine-tuning methods, highlighting the reduced training time and lower computational requirements.</li>
  <li>Additionally, it introduces methods like Textual Inversion and Pivotal Tuning, which are complementary to LoRA. The page includes code snippets for using LoRA in Stable Diffusion fine-tuning and Dreambooth training.</li>
</ul>

<h2 id="diffusion-model-alignment">Diffusion Model Alignment</h2>

<h3 id="diffusion-model-alignment-using-direct-preference-optimization-1"><a href="https://arxiv.org/abs/2311.12908">Diffusion Model Alignment Using Direct Preference Optimization</a></h3>

<ul>
  <li>This paper by Wallace et al. from Salesforce AI and Stanford University proposes a novel method for aligning diffusion models to human preferences.</li>
  <li>The paper introduces Diffusion-DPO, a method adapted from DPO, for aligning text-to-image diffusion models with human preferences. This approach is a significant shift from typical language model training, emphasizing direct optimization on human comparison data.</li>
  <li>Unlike typical methods that fine-tune pre-trained models using curated images and captions, Diffusion-DPO directly optimizes a policy that best satisfies human preferences under a classification objective. It re-formulates DPO to account for a diffusion model notion of likelihood using the evidence lower bound, deriving a differentiable objective.</li>
  <li>The authors utilized the Pick-a-Pic dataset, comprising 851K crowdsourced pairwise preferences, to fine-tune the base model of the Stable Diffusion XL (SDXL)-1.0 model with Diffusion-DPO. The fine-tuned model showed significant improvements over both the base SDXL-1.0 and its larger variant in terms of visual appeal and prompt alignment, as evaluated by human preferences.</li>
  <li>The paper also explores a variant of the method that uses AI feedback, showing comparable performance to training on human preferences. This opens up possibilities for scaling diffusion model alignment methods.</li>
  <li>The figure below from paper illustrates: (Top) DPO-SDXL significantly outperforms SDXL in human evaluation. (L) PartiPrompts and (R) HPSv2 benchmark results across three evaluation questions, majority vote of 5 labelers. (Bottom) Qualitative comparisons between SDXL and DPO-SDXL. DPOSDXL demonstrates superior prompt following and realism. DPO-SDXL outputs are better aligned with human aesthetic preferences, favoring high contrast, vivid colors, fine detail, and focused composition. They also capture fine-grained textual details more faithfully.</li>
</ul>

<p><img src="../../images/papers/Diffusion-DPO.jpg" alt="" /></p>

<ul>
  <li>Experiments demonstrate the effectiveness of Diffusion-DPO in various scenarios, including image-to-image editing and learning from AI feedback. The method significantly outperforms existing models in human evaluations for general preference, visual appeal, and prompt alignment.</li>
  <li>The paper’s findings indicate that Diffusion-DPO can effectively increase measured human appeal across an open vocabulary with stable training, without increased inference time, and improves generic text-image alignment.</li>
  <li>The authors note ethical considerations and risks associated with text-to-image generation, emphasizing the importance of diverse and representative sets of labelers and the potential biases inherent in the pre-trained models and labeling process.</li>
  <li>In summary, the paper presents a groundbreaking approach to align diffusion models with human preferences, demonstrating notable improvements in visual appeal and prompt alignment. It highlights the potential of direct preference optimization in the realm of text-to-image diffusion models and opens avenues for further research and application in this field.</li>
</ul>

<h2 id="further-reading">Further Reading</h2>

<h3 id="the-illustrated-stable-diffusion"><a href="https://jalammar.github.io/illustrated-stable-diffusion/">The Illustrated Stable Diffusion</a></h3>

<ul>
  <li>Jay Alammar’s (of <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> fame) article explaining Stable Diffusion.</li>
</ul>

<p><a href="https://jalammar.github.io/illustrated-stable-diffusion/"><img src="../assets/diffusion-models/jay_sd.jpg" alt="" /></a></p>

<h3 id="understanding-diffusion-models-a-unified-perspective"><a href="https://arxiv.org/abs/2208.11970">Understanding Diffusion Models: A Unified Perspective</a></h3>

<ul>
  <li>This tutorial paper by Calvin Luo from Google Brain goes from the basics of ELBO, VAE, and hierarchical VAE to diffusion models.</li>
</ul>

<p><a href="https://arxiv.org/abs/2208.11970"><img src="../assets/diffusion-models/tutorial.jpg" alt="" /></a></p>

<h3 id="the-annotated-diffusion-model"><a href="https://huggingface.co/blog/annotated-diffusion">The Annotated Diffusion Model</a></h3>

<ul>
  <li>This blog post by HugginFace takes a deeper look into Denoising Diffusion Probabilistic Models (also known as DDPMs, diffusion models, score-based generative models or simply <a href="https://benanne.github.io/2022/01/31/diffusion.html">autoencoders</a>) as researchers have been able to achieve remarkable results with them for generative models. It goes over the original DDPM paper (<a href="https://arxiv.org/abs/2006.11239">Ho et al., 2020</a>), implementing it step-by-step in PyTorch, based on Phil Wang’s <a href="https://github.com/lucidrains/denoising-diffusion-pytorch">implementation</a> - which itself is based on the <a href="https://github.com/hojonathanho/diffusion">original TensorFlow implementation</a>.</li>
</ul>

<p><a href="https://huggingface.co/blog/annotated-diffusion"><img src="../assets/diffusion-models/annotated_df.jpg" alt="" /></a></p>

<h3 id="lilian-weng-what-are-diffusion-models"><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lilian Weng: What are Diffusion Models?</a></h3>

<ul>
  <li>This tutorial paper by Lilian Weng from OpenAI covers the math behind diffusion models in detail.</li>
</ul>

<p><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"><img src="../assets/diffusion-models/lillog_diff.jpg" alt="" /></a></p>

<h3 id="stable-diffusion---what-why-how"><a href="https://www.youtube.com/watch?v=ltLNYA3lWAQ">Stable Diffusion - What, Why, How?</a></h3>

<ul>
  <li>This YouTube video by Edan Meyer explains how Stable Diffusion works at a high level, briefly talks about how it is different from other Diffusion-based models, compares it to DALL-E 2, and digs into the code.</li>
</ul>

<p><a href="https://www.youtube.com/watch?v=ltLNYA3lWAQ"><img src="../assets/diffusion-models/stable_diff_what_why.jpg" alt="" /></a></p>

<h3 id="how-does-stable-diffusion-work--latent-diffusion-models-explained"><a href="https://www.youtube.com/watch?v=J87hffSMB60">How does Stable Diffusion work? – Latent Diffusion Models Explained</a></h3>

<ul>
  <li>This YouTube video by Letitia covers diffusion models, injecting text to generate images (conditional generation), and stable diffusion as a latent diffusion model.</li>
</ul>

<p><a href="https://www.youtube.com/watch?v=J87hffSMB60"><img src="../assets/diffusion-models/stable_diff_latent.jpg" alt="" /></a></p>

<h3 id="diffusion-explainer"><a href="https://poloclub.github.io/diffusion-explainer/">Diffusion Explainer</a></h3>

<ul>
  <li>
    <p>Diffusion Explainer, an interactive web application, is designed to visually demonstrate the process of transforming a text prompt into high-resolution images within seconds.</p>
  </li>
  <li>The app offers the following features for exploration:
    <ul>
      <li><strong>Text representation generation</strong>: Observe how your text prompt is tokenized and converted into numerical vectors, which guide the creation of images.</li>
      <li><strong>Image representation refinement</strong>: Witness the transformation of random noise into a coherent image through successive steps.</li>
      <li><strong>Image upscaling</strong>: Discover how the final image representation is enhanced into high-resolution output based on your input.</li>
    </ul>
  </li>
  <li>With its interactive controls, you can modify prompts, adjust seeds, and tweak guidance scales, allowing you to see how each factor influences the final image. This tool is ideal for those seeking a deeper understanding of diffusion models and text-to-image generation technologies.</li>
</ul>

<p><a href="https://poloclub.github.io/diffusion-explainer/"><img src="../assets/diffusion-models/diffusion-explainer.jpg" alt="" /></a></p>

<h3 id="jupyter-notebook-on-the-theoretical-and-implementation-aspects-of-score-based-generative-models-sgms"><a href="https://jakiw.com/sgm_intro">Jupyter notebook on the theoretical and implementation aspects of Score-based Generative Models (SGMs)</a></h3>

<ul>
  <li>Great technical explanation and implementation (along with a JAX implementation) of score-based generative models (SGMs), also called diffusion models.</li>
</ul>

<p><a href="https://jakiw.com/sgm_intro"><img src="../assets/diffusion-models/sgm.jpg" alt="" /></a></p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://analyticsindiamag.com/diffusion-models-vs-gans-which-one-to-choose-for-image-synthesis/">Diffusion models Vs GANs: Which one to choose for Image Synthesis</a></li>
  <li><a href="https://medium.com/@monadsblog/diffusion-models-4dbe58489a2f">Diffusion models</a></li>
  <li><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/#references">Introduction to Diffusion Models for Machine Learning</a></li>
  <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">What are Diffusion Models?</a></li>
  <li><a href="https://arxiv.org/pdf/2006.11239.pdf">Denoising Diffusion Probabilistic Models by Ho et. al</a></li>
  <li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">Lil’Log: Diffusion Models</a></li>
  <li><a href="https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/">Assembly AI: Diffusion Models for Machine Learning Introduction</a></li>
  <li><a href="https://developers.google.com/machine-learning/gan/generative">Google ML Crashcourse on GANs</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Markov_chain">Wikipedia: Markov Chain</a></li>
  <li><a href="https://theaisummer.com/latent-variable-models/">AI Summer: Latent Variable Models</a></li>
  <li><a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a></li>
  <li><a href="https://twitter.com/JayAlammar/status/1572297768693006337">Jay Alammar’s Stable Diffusion Twitter thread</a></li>
  <li><a href="https://huggingface.co/blog/stable_diffusion">Hugging Face Stable Diffusion</a></li>
  <li><a href="https://towardsdatascience.com/dall-e-2-0-explained-7b928f3adce7">Towards Data Science: DALL-E 2 Explained</a></li>
  <li><a href="https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb">Hugging Face’s Training with Diffusers notebook</a></li>
  <li><a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb">Hugging Face Diffusers Intro notebook</a></li>
</ul>

<h2 id="citation">Citation</h2>

<p>If you found our work useful, please cite it as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{Chadha2020DistilledDiffusionModels,
  title   = {Diffusion Models},
  author  = {Chadha, Aman and Jain, Vinija},
  journal = {Distilled AI},
  year    = {2020},
  note    = {\url{https://aman.ai}}
}
</code></pre></div></div>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">
   <div align="center" class="wrap">
      <div align="center" class="footer-col-1 column">
         <ul>
            <li>
               
               <span class="icon github">
                  <a href="https://github.com/vinija">
                     <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                        viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                        <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                           c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                           c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                           c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                           C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                           c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                           c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                           c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                           c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                     </svg>
                  </a>
               </span>
               <!-- <span class="username">vinija</span> -->
               
<!--               <a href="">-->
<!--                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"-->
<!--                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">-->
<!--                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAVjBAMAAABzrVjQAAAABGdBTUEAALGPC/xhBQAAACBjSFJN-->
<!--                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAElBMVEX///+xsLCxsLCxsLCx-->
<!--                        sLD///+bxiTSAAAABHRSTlMAAKP3FWDuDwAAAAFiS0dEAIgFHUgAAAAJcEhZcwAACxMAAAsTAQCa-->
<!--                        nBgAAAAHdElNRQfkBwQDMic2f+cwAAA03klEQVR42u2dW3IdOZJEu81mAcMqbOCacQMy0wImVNr/-->
<!--                        msZKKpVeuHkzEA8PIPx8douAh+MkkmKR1H/+QwghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQ-->
<!--                        QgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIeQQ/vt2KOMzyeH/GtiE7rgP/3u+TQPdcRukgU3o-->
<!--                        jtsgb+fbNNAlt+GtgU3ojtsgDWxCd9yGT2/n2zTQJbfhrYFN6I7bIA1sGuiS2/DWwCZ0x214a2DT-->
<!--                        QJfcBelgE7rkNrw1sAndcRukgU0DXXIXvsl0tE3oktvwb+MH2zTQJXdBOtiELrkL32U62KaBbrkL-->
<!--                        P3R+rE1/oEvugnSwCV1yF/76sfRTbRrolrvwU+un2oQuuQvSwaaBbrkLP9d+qE3okrvwS+1n2jTQ-->
<!--                        LTdBOtj0J7rlLvxa/JE2oUvugnSwaaBbbsJvMh1pE7rlLvze/IE2DXTLTZAWNqFbbsKnSfXn2TTQ-->
<!--                        NTdh1v1xNg10y02QFjahW+7CtPzTbBrolpswb/80m9AtN0Fa2DTQNTfhSf2H2YRuuQmPFja9o2vu-->
<!--                        gTzr/yibBrrmJjw9gKNsQrfcBGlh00DX3IPnMh1lE7rmJlycwEE2DXTNPZAeNqFrbsLVEZxj00d0-->
<!--                        zT24PINjbBromnsgPWxC19yE60M4xaaBrrkHL07hFJvQNfdAetg00D334NUxHGITuuYeSA+b0DU3-->
<!--                        oYdNA11zE3rYhG65Cy1sGuiWu9DCJnTJbehg00CX3IYGNg10x31oYBO64kacb9NAV9yI821CN9yJ-->
<!--                        420a6IY7cbxN6IJbcbpNA11wKw63ib8YPJXDbULX24yzbRroeptxtk3odrtxtE0D3W43jrYJXW47-->
<!--                        TrbpHV1uOw62aaC77cfBNqGrbci5Ng10tQ051yZ0sx051qaBbrYjx9qELrYlp9qE7rUnh9o00L32-->
<!--                        5FCb0LU25UybBrrWppxpE7rVrhxp00C32pUjbUKX2pYTbfqILrUtB9o00J325UCb0JW2YEz/1/Ns-->
<!--                        GqpWyBIy5v/zcTahi27Bk2f2OJuGqhWyhDSx6Q900R2QtyY2oYtuwVsTm4aqFbKEdLEJXXQLnj+2-->
<!--                        Z9k0VK2QJS6aPssmdNEdkC42DVUtZImrqk+yaahaIUtIF5vQRXfgr8sn9yCbhqoWssR12QfZhC66-->
<!--                        A//qMq7/7+1tGqpayBIv2j7HJnTRHfhuy3jx/29u01DVQlaQV3WfYtNQ1UKWeNn3KTahi+7Aj66M-->
<!--                        l39iY5uGqhayxOvCD7EJXXQHPnSxaahqISvIjcbPsAnddAfuPL9H2PSObroBv4gy7vyhLW0aqlrI-->
<!--                        Erc6P8EmdNEdeHSxaahqISvIvdIPsAnddAduPsL72zRUtZAV5Gbr+9uEbroDd5/h7W1CF92BSe1j-->
<!--                        +gd3t2moaiEryO3ed7cJ3XQH7j/Fm9s0VLWQFeR+8ZvbhG66AZ8Uj/HeNg1VL2QFTfN724RuugFP-->
<!--                        BBmaP7yHTeimO/Ck+jH9wzvbNFS1kBVE1f3ONqGbbsBTPYbuj9e3aah6ISsoy9/YJnTTDXhux1D+-->
<!--                        +eo2DVUvZAVt+9vaxF8MHs+FHEP7AbVtQjfdgCs3hvojKts0VL2QFfT972oTuukGXKox9B9S16ah-->
<!--                        6oWssHAAm9qEbroBK4/znjYNVS9kAVk5gS1tGqpeyApLR7ClTeimG/DKi7H0URVtGqpeyAJ/rZ3B-->
<!--                        jjahq27A4hO9oU1D1QtZ4KUVb3+ufVw9m9BVN8BwBJvZNNBVn49YTuDyg6vZNG71QQy8kOnlx29k-->
<!--                        E7rqBpgPYBubBrrq8xHzATxfoZhN6Kob4HAAm9g00FWfj1wewLttjVo2oas+H3E5gC1sekd3fT7X-->
<!--                        BzBurvLMyUo23Z2FLCPXJ3B7nQ1sQlfdAK8TqG/TQFd9Pg+vE5DyNqGr/p1fAn5E57EiL05gWFeq-->
<!--                        Y9P9UVLYIqTLTN95v71SeZvQVd+tfaDDLSOOR1Dcpo/oru+XPtAJF3l5Bua1qthU5oReulQqrYbX-->
<!--                        Y5kXq2ITuur7ldfKe58bj4m5pyI2DXTXVx1N+dO+W7XZhnm1Ijahq/6C6DJ/ROf1Hm6Yl6thk2aO-->
<!--                        MPZM7TidZp7KNqGr/nzjR8yK5r7JnWmGYr3CNqGr/qx+yxVK7jfeMC9YwSbNFMi2N9bJ/Rzq2oSu-->
<!--                        2iBThfBu8w3zigVs0gyBLHtfnW7ON8xLFrAJ3bVNpgL5X3JzkGHuDG+TZoYQPlknQA/wCrk5xzCv-->
<!--                        CbcJ/4vB7TOgJ3AacCjWLGoTumoHmQo8EVfI3TGGeVG0TZoJQjhjigtuy3SATft0vatOMUOUtAl9-->
<!--                        Ck4yFf5RQMWEw7ws2CZ01+dM4jDhUCxb0SZNfnDVm+oUdRoFbUJ/x5mYJ1g8i5oTaiYoaBO4avOX-->
<!--                        LevrFDZAPZvQ9XvPAx5ngqjyD/PKSJu2qtr5NFJQfgegJn85m9DlnzeRcUJN/HI2gasW8wDlRjJO-->
<!--                        OMxr42zSZI/gxJlsE2rSy3QFmE3o4h8hU4GH+gnRhh/mxWE27Vb1HmP9gP7rH8PcH8omTfIITp3L-->
<!--                        MqEmu0xXQNkErlrMAxQdzDLhMC8PskkTPIJzJzNMqIku0xVANoGrFvMAZUczTDjM62NsQncdOZvm-->
<!--                        TMKQ6OTzDSA2oQtf6vo24OG+EH4s8wohNm3Z9T7jrT4uw7wDwiZN6jpd7zPf6uOiyT3vEGETumsx-->
<!--                        T1B8wMda7GHuEGCTJnQIp08oCbHnewBswladcDWhR1xNPcwl5tv0EVt1wtUEvpwkI/V8k3SboEWb-->
<!--                        ytaAHDDlaOYtptuE7NlWdtTBlJlPE1qmK2TbBKzZ3LYG2HiynllzNvNtsm2CtezQdtTJuJKUeV5j-->
<!--                        sk2wkj3aVgEaTwyRNYcz3yfXJvwvOrK0reIjZj5L5GHuMdcmTMOvWwhAczR+pEWe95hqE6Zhv7pV-->
<!--                        IKYTU2LN8cx3SrUJUbBn3WFn40Ve4nmRmTYh+r1VQgzbTac5n/lWmTbl1/sb0SOuHo4Lkhh4vlei-->
<!--                        Tent+vetI3u6zAOaN5lnkyZrFGKeovDE5uE0eeeb5dmUW+2c2Aktp1NhOE1ema6QZlNytYoK4kgd-->
<!--                        7kPqEc2rTLMptVlVBXFojqfAbJq48+2ybMos9imRA07ZazbNGcl0hSybkr1RNBCJ5nzws2nSzvdL-->
<!--                        simv1uDGlaTNln1I8y5zbNLkrN24ko87jaY5JZmukGNTvjkTlv6N+sQDMiDmoNqw8x1TbErqNKVy-->
<!--                        JTmj5R/TvMwUmxDuRFUeeELLiDmmOut8ywybUhp9TdR41yQM5vUK15yTTFfIsAlkz635w/kYP5lX-->
<!--                        1GFuM8Gmd4w99+YPR3NE4ME0UeebxtsUX+c9YqZ7zT6DaU5KpivE2wTTJ6r0wDNaQcwJV5LOdw23-->
<!--                        KbrM/NK1xM7l+I/saY5qXme4TUiDfuQRMh2+AcegQ7GtTFeItkmTcJfWK1Ug5nhrQef7RtsENegH-->
<!--                        nP/V1TIdeOYcin1lukKwTViFXk6fg+aUkGNpcs43jrUpsEclD//hCrQg5myrOec7x9qEdug7/rNV-->
<!--                        qME35lDsLNMVQm3SxAvGX5ECPYg52XLM+dahNqEVCus98Jg0AGPOC420KarEBcQ8jImYoR7A85oX-->
<!--                        GmkTWqEfcPcj7pyMB5qUcr55oE1og37E3Y8CXbiHHIrNZbpCnE2abNGIeRojW8ykObH57nE2oQ2K-->
<!--                        bT7woG6CDTlvNMymgP4qNa9lh5E0RybTFcJsQgsUXT24DjEn+p1h3j/KJk2yeAKqB/ch5kS2jPP9-->
<!--                        g2zC/2Lw6Oq1eM8UEHGYKw2yCe3PndFz8Z4pIOIwVxpjkyZXAhF2oCsRcyJTxPn2MTah9fmFCDvg-->
<!--                        nfgnHIrdZbpCiE2aWBnE6KGk/lCaY5PpCiE2oe25NXk2mrPCTKVJON89wibv3ur1vsKf5afSnNt8-->
<!--                        9wCbNKFS8O99Ce+x3ANqDm7eaYBNaHnia48/rPUDTQo439zfJu/S7MQJAi1GzIkMAeeb+9uEdud3-->
<!--                        4gTBNuOdbyj2lukK7jZpIuUg5pmceHcezDuf5ujmpbrbhHbn7uAANKeFGEyTb763t03ejRUsfZ3i-->
<!--                        g2nObr63s02aQFnEGqKh+GSaw5PpCs42oc1J6DzpuACTaeLJdAVfm7zrqth5oXrEnGg53nxrX5vQ-->
<!--                        4kyJdgTXj5gD/cQwb+1qkyZOGs6Vm3h3ns03neb45q262oQWRzE3Bs153QGXbt6qp03vaHEUc4Mo-->
<!--                        Pdsw7+xokyZMIr6NGyk9m+YA5zs72oTW5gkZkoQcWPpwmnAyXcHPJu+iShaeeWDpw2nCyXQFP5vQ-->
<!--                        1qQUbsZ5ODEH+oFh3tjNJk2UVJI0uYnzcGIOtHiE843dbEJLk9J37ollT6fJNt/Yyya0NDl9557Y-->
<!--                        HVDZ5rU62eRdkh9imCoC5/E8o2kOcV6rk01oZ7Rj4yg83jDv62OTJkcyiaIgqhJzoLVo8319bEIr-->
<!--                        c0GiKO5HljyfJppMV3Cxybuhom37UHc+zTHKdAUXm9DGJLXtQ935hmJbma7gYRNamKy2889s+VjD-->
<!--                        k823dbDJuZ6yZQPOLHdATbL5tg42oYXJKtuLsgMO87Z2mzQZ8nEs2wvnCf2CaU5y3qvdJrQvK1ND-->
<!--                        0RzaDTDB5r2abXLuxhtZGioU58Ywwea9Wm2q9YvB706NpeqEw7yr1Sa0La8AyJLcmZjzfGOYdzXa-->
<!--                        pAkAAWFLbmlizrOSa76r0Sa0LC9B2OJ5apkjanLJdAWbTc69BIDx5QVFR9ScpkxXsNmEdiWxak+K-->
<!--                        jjgUm8p0BZNNmu1BgHy5xndEMedZOM75phabvH+fegCfUMK4HdvqwQbHmm9qsQmtSmbTrmiOLXFG-->
<!--                        Taz5pgabfDuJwa1pX3yH9EqlOdB5sQab0KasDw3Hd0ivVMNc7LpNmr1hiG6mLHy7Q6SaF7tuE1oU-->
<!--                        w9BwNOf2GkSqebHLNvkWEoWoZkrDtzyvITWp5nuu2uTbRxheRXtTckjNmc73XLUJrUly0d6UHHKY-->
<!--                        91y0SbMxkgfUmaT+xBxHH2q+56JNaEvugnXG5+Be4vUFf00oma6wZpNrGZGgrckpEBBKpius2YSW-->
<!--                        JLtnfypOORRbynSFJZve0ZJk9+xPxSmHYkuZrrBik2ZXMGhpnlJxSs25ynSFFZvQiuT37I/m5BaP-->
<!--                        NjTTfMsFm1yLCAYtTU6JYo6jzjTfcsEmtCEa0NLktCjmNF8Y5i31Nmn2hIN25jmeU4o5jfpk51vq-->
<!--                        bUILogLtTE6NYk7zhWHeUm3TR7QgKtDO+BzdK/7KjyTTFbQ2eZaQANqZpCLzI8l0Ba1NaD0gNUeg-->
<!--                        ObqkMTWRZLqC0ibXDhJAO3NBvTE1hyvTFZQ2oe3A1BxCvTGHYkeZrqCzSbNfCdDKXFBvTM3pynQF-->
<!--                        nU1oOUA1h6A5u7WzjUw031FlE9oNPWhlnM5u7WwjE8131NjkOX8SaGWS2hRzGm2i+Y4am9BqLIBW-->
<!--                        5gLP3xgq5jR/M8w7KmzSbFYFtDJXOI4p5jDaA57vqLAJbcYKaGOS+hRzmL8Z5h3v26TZqwxoY65w-->
<!--                        HFPMYbQnPN/xtk2areqANiar0fRAMl3htk1oL4AtB6E5vJQ5NYFkusJdmzxHTwRtTFal6YFkusJd-->
<!--                        m9BaIFuOotqcQ7GhTFe4aZNmo0qghbmk2pyaQ5bpCjdtQlsBbTmKanMOxYYyXeGeTZp9SoEWxu30-->
<!--                        MubU5JHpCrds2uAXg0e2HIXm9F7wyM4j0xVu2YR2Yh20MG6n9wIxh1HmmW94xybHqbNBC+N2ei8Q-->
<!--                        cxhlnvmGd2xCK2EALcw1fnOKOctbkk2aTaqB9uUavznFnEV50PMNb9iENsIC2pdr/OYUc5a3HJve-->
<!--                        0UZYQPvid3zXiDmLMs58w5c2+U2MAO2L3/FdI+YsyjjzDV/ahPbBBtoXv+O7RsxZlHHmG76yyW9g-->
<!--                        CGhf/I7vGjFnUcaZb/jKJrQORtC+vMBtTjFHeUuwSbNBRdC6vKDWoEOxn0xXeGET2oYKJQdSa9Ch-->
<!--                        2E+mK1zbhJbBjLyVxm9QjzTDXOylTZrlayJvpfErODvNvNhLm9Au2JG30mjO75rsNPNir2zymxWG-->
<!--                        vJXGr+HsNPNir2xCq+CAvJVGc37XZKeZF3thk9+oOOStNm6DeoQZ5mIvbEKb4IG81cZtUI8ww1zs-->
<!--                        c5vQIrggrxoE4zaoR5hhLvapTZql60KbatiE9sCH6jZpDvCS2ja5jYmFNpWwCa2BE7Spgk1uU6JB-->
<!--                        60KbfH/DJxa0Lq+oNOdQ7KexCe2AH2hbaNM57znaVMAmtAKOoG2hTZpVq4O2hTahDfAEbYvnCUbP-->
<!--                        qcly2yavAUuAtqW7TV7z1QBtS3eb0Ofvi9ytkjZF2OQ1XhFoE9Qm9PE7U90mr75r2qRZcQdoE9Im-->
<!--                        9Ol7Q5uANmkW3ALahLNJs94e0CacTeiz96e8TZojvKCgTU6TlQJtS1+b0CcfAdqWtjY5DVYLtC1t-->
<!--                        bUIffAhoW7ra9D/ogw8BbctL6ow5FPu9tOm/6F5DekYPkTMlbfIcTjt2IWhTCrSJNvlBm2iTH7SJ-->
<!--                        NvnRxKZ32pRBE5t8xsxOMq/1eJvKf4mANm1Ucw+bPmUnkekKtAmNy5iSnWS+IW1CQ5v2qZk2xSSZ-->
<!--                        b0ib4NCmDJxscik6kjJD0ibaRJvch6NNyYXPN6RNcMoMSZtoE21yH+4K9ByvoE0Z0KbkGWnTATZ5-->
<!--                        zJkeRKYr0CY4tGmXlp8PXgfatEvLtCkoyLxU2gSHNu3SMm0KCjIvlTbBoU27tOxXdek5JT3IfEfa-->
<!--                        BIc2bdLyFjY5fDFc0guf70ib8NCmBNxs8uk6jioT0ibaRJvch2tu0yO98HmntAmPfcL8wueddrCp-->
<!--                        +qfhVQakTbSJNrkPR5uSC5fpCrQJT5UBaZNh9DJUmY820SbalNTyJja9F5mPNp1gk+YUadMqbjYV-->
<!--                        /zScNmVAm3LHo020iTbdw8+mD+hRLqFNGfjZJOYskdCmDGhT7nS0iTbRpnv42VT703DalAFtok11-->
<!--                        Wu5iEyKHTFdoYpOYwxSeE5FjXihtwkObdmiZNsXlmBfaxCaXf7+t6JyCyDHftIlNpT8Np00btEyb-->
<!--                        AnPMN6VNeGhTCo42uVUegNEmSI55n7QJD21KgTbRJj8cbar8iZPNJoHkmO9Km/DQphRoE23y493R-->
<!--                        Jr/S3bHZhMkxr7O0TbaaaVNgjnmdtAkPbdqg5rDWS40pmBzzbWkTHtpUv+YmNn3A5JDpCqVtcv0S-->
<!--                        gZjTRGGyCZRj3iZtwkObcvC0qe6rzmKTgHLM96VNeGhTDrSJNtXoObT4MlOicszLpE14aFP9nlvY-->
<!--                        5DoTbdKAniZgSkHlmG9Mm/DQpiRcbXKt3hGDTbAc8y5pEx7DSK45aFNvm3wnok0q0NO4DymuOY63-->
<!--                        STNgQ5twZct0hVY2iTlPCLQpCdpEm/z409Wmoq+6Kk/H8Ta1+DScNpVvesoDPY7vjM45aJMOMecJ-->
<!--                        QHOItAlU9fk2eU9Dm5Sgx3EdUYBB5nvTJjjLIyKDyHSF6jb94WuTmAP5Q5vyoE1ps9AmLehxJrzT-->
<!--                        pjTOt6nMKA1s0oy43AIU2pQHbUqbhDapQc/zO7SpftnH2+QfhDapEXMgZzRnSJus0KasOTrYtNj2-->
<!--                        U9DzOM33AZtEpivQJjSL84GTyHSF+jad/qp7LzMFbapxDvnjRUxBmxZAz+MyXkSSFjZphqRNSUXL-->
<!--                        dIWGNok5kSt1ZqBNC3xCD+Qw3Qd0FJmusIFNZ3/itGYTPIpMV+hok5gTObJkU8wEPWxaKpw2hUaZ-->
<!--                        J+hoU6lXXaEBaFOhw1hjJf9f+J5lusIONh39iVOh/LRpiUJfI9Cc4L8UyCLTFbawaalywHFkjVYg-->
<!--                        i0xX6GmTmCMhR4tK38Smgz9xWrGpQpZ5gz1tqvOqqxS+i03v3jY90BN9YyG7RGXpYpNmTuyJaFnI-->
<!--                        XqLleYF72OT8q3kjj0THymNSIoxMV9jDpmM/cVqwSUqEmafoapOYI7mwYFONMPP+NrFpoXXYoWjQ-->
<!--                        5w78Oj5tameT1Ch5HmMTm0591ZV6CmjTOuiJ/kZzfvEPQR+b9L2faVOVNDJdoa9NYs5kp9Yz0Mem-->
<!--                        M191tR6BRjZpRj3XpjIVy3SFxjaJOVP6TLGRG9l04qtObVOdODJdobNNj+1Gio3TySbNrJZGEikW-->
<!--                        mDaZ2G2iQnlkusI+NgW86sScyYTWpui4rWx6P80mbdzoPK1s0j7KBc7nmmrut7LptFed9ukoFWje-->
<!--                        XG+boJeT0iYpFWieZiebtA9zcZuUUaVUv/M0O9l02KuunPi0yQpuGM3Z5XjfzCblARQ5JJ9hiiWa-->
<!--                        97aVTf4/pBn1a9puUM/6Zjad9KrTHF1SzG42KY9gvZZqo6SkpE12QKMUDNnNpohXnZhDxU+Sk7Gd-->
<!--                        TZqB77LBIPUyyXSFzWwK+Fsd5lswVQklJ1M7m4551akSJmXqZ5Nm4mKHtTyGFAw1z7SbTYdcTu8V-->
<!--                        badNLtSeQrJCNbRJM3K581oaomSqeWXb2XTE5VRT9Y42fQyw6ZE8Q03TO9qkmdnWTYkREqN1tOmA-->
<!--                        V13RZC1t0gxtKyeKosFa2vRHgE2pl5Pm1KrmkukKG9oU8qoTc6qQ/Jmxmtqkmfo2NeOX7VWmK+xo-->
<!--                        0+aXk+LQ8kIpg51kk2bs26Slr5hJXatMV9jSpq0vJ8WZZUXSJ6NNr6iXPbnVrjZp5jYW5I3i6xuP-->
<!--                        wq3Oy9rTpo0vp/tHJtmltrVJM3it4ysm92qp8642tWnby+n+iUl6p31t0kxe6QBLqW3odF7VrjZt-->
<!--                        ejndPzDJr7SxTe8RNok51gsKif07jW3SjF7mDO+HluKVzvNta1PMq07MsZwyIxrtbJNm9iKneD+y-->
<!--                        VG90HnBfm2Iup9DfNVdE6me0tkkzvLWm5MCBIZwKnSfc2KaYyynwWiguU3ObNNNbe0qNu0Gf85Z2-->
<!--                        tmmzywnv8wua26QZH64Ten/fOmW6wtY2BV1OYs414f5RhWzvG/FMmzTzK4iIit3dvU2ZrrC3TUGX-->
<!--                        U8CB3j8pgZXZ3iZNAeauDNz/7a+f9ihz3tDmNu1yOeF2VkCbNA3gdLqfUjbpcp5zd5uiLicxB/sB-->
<!--                        xTEhq6RNYZeTmJN9ByRxZJXzoNvbFHU5OR6sYlNok7Qp7nJyO1lFQNmmyXnS/W2K+Q5xP500Z4Qt-->
<!--                        kjb9TZRNLoerOaIHtkfapG0hWyfNP1olG/U4z3qCTXGXk/kL06rf0YmukTapa8jVSbOX7FTjPOwR-->
<!--                        NsVdTjaddL89GF0ibfqHkF/6bNZJ9w99CrpE2rRShJakTOgKadN3Am1aPGelTIJukDatNZGikzYQ-->
<!--                        ukBlZJmucIpNcV8Rf97dFfE7+EObvhNpk/bq0JzLyvox0Ka1LhaQ+0l0f5fTrl6jwXngc2wK+Uc1-->
<!--                        V058ZW10d1+gTT8SbNO9Mw8VNRbatNpGjE+rCdDN6dPLdIWTbIr9e91ljV+IWDMT2vQzCTZ9nv/H-->
<!--                        Fs1J/Aa6toUZZLrCWTaZznRRAfueYpgY1d4881k2perkBrq0lfJkusJhNiW961wR89BO0KbfQLtB-->
<!--                        m06ySVNJDdCNLVUn0xWOs2k7ncQ8MaK5eerzbNrtXYeu6zu0aQbaDxViHtcN2mRtBQ66rMXeZLrC-->
<!--                        iTbtpJOYh8XUNs99pE0bvevQTf0IbXoCWpK7hP4jQVpo0zPQmpgOBQRt8mgGCLqm5c5kusKpNu2h-->
<!--                        k5jHRFU2T36sTVvohO5ovTGZrnCuTRvoBPxN89bCZLrCwTbV10nMI8L6mkc/2abyOqH7MdQl0xWO-->
<!--                        tqm4TmKeD9fWY7rC2TbV1gldjqWs+QqH21RaJ3Q3v0GbXlFXJ0FX8xu06SVldUIX8zu0ybekRIp9-->
<!--                        sUlZ1HyBDjbV1EnQrfzOsIZvYVNJndCdTKBN3j1lIeaZkC09Sd/Epno6iXmkAG6nf/LxXWwq97ZD-->
<!--                        12Hr6MnH97GpmE7oNkwVPfv7aCObSr3tBF2GqaFn6TvZVOl6Qjcx565NH558fC+b6lxP6CKeYEzf-->
<!--                        zKb43wN9D0HX8ARj+nY21XjdiXmKGAZtiqksFHQFT7Gl72hTAZ/QBZiakacf3dMmtE+CHt/Uy+Pp-->
<!--                        R3e1aenf1mlg051X3fMP7mvTG/DzcfTgFwzLo9DaJtgLDz32FZbwzW16gwgl6JktfVyFp01/c9uD-->
<!--                        Ty6vR0HPaynj6kNp0zdeOvD1j/1xvE3jMvvj6kNp00+8Pv3rru+BnvIFVyPK5UfSJiUNbLq4pl/8-->
<!--                        oA1tUuIgU3mb3laT0yYlDjIJeoblKV99GG3SMXrYNNfp5UfRJh1dbJoMeiM2bdLhYRN6hqVR5c6H-->
<!--                        0CYdDjLtYtMCtEkHbbqCNqnweNHRJvIVD5sEPUQctEkFbbqENqnweNEJeog4aJMK2nQJbVJBmy6h-->
<!--                        TRoGbbqENmlwsQk9RCC0SQNtuoY2afCQiTaRr9Cma2iTBtp0DW1SMGjTNbRJAW16AW1SQJteQJsU-->
<!--                        0KYX0CYFLjLRJvIF2vQC2qSANv3nNv9FRy0PbaJNbgzaRJvcoE20yQ8nmwQ9hwHa5AZtok1+0Cba-->
<!--                        5IePTLSJ/A1tok1+0Cba5IeTTTt/iYA2uUGbaJMftIk2uTG8bHqgJ1mHNnnhZpOgJ1mHNnlBm2iT-->
<!--                        H7SJNvnhZtPGn4bTJi9oE23yw88mQY+yDG3ygjbRJj/cZKJNxPNfKEePsgxt8oI20SY/HG0S9Cyr-->
<!--                        0CYvaBNt8sPRpm1fdbTJC9pEm/zwtEnQwyxCm7zwtOmTPQ4E2uSFp027vupokxeuNgl6mjVokxe0-->
<!--                        iTb54WrTpq862uSFr02CHmcJ2uQFbaJNfvjatOerjjZ54WyToOdZgTZ5QZtokx/ONm35qqNNXnjb-->
<!--                        JOiBFqBNXnjbtOPlRJu8ePe26YGeSA9t8mLwVUeb3HC3acNXHW3ywt8mQY+khjZ54W/TfpcTbfIi-->
<!--                        wCZBz6SFNnkRYNN2lxNt8iLCpg/ooZTQJjcCbBL0TEpokxsBNu32qqNNbkTYJOihdNAmNyJs2uxy-->
<!--                        ok1uhNgk6KlU0CY3Qmza63KiTW4MXk60yY0Ym7a6nGiTG0E2CXouBbTJjSCbdrqcaJMfQTYJeq77-->
<!--                        0CY/gmza6HKiTX5E2STowW5Dm/yIsmmfy4k2+THaX060yY8wm7a5nGiTH3E2CXq0m9AmP+Js2uVy-->
<!--                        ok2OxNkk6NHuQZscibNpk8uJNjkSaNMev4+eNjkyAnUS9HB3oE2ORNq0xbuONjkSapOgp7sBbfIk-->
<!--                        0qYdLifa5EmoTYKe7jW0yZP35pcTbfJkNL+caJMnsTbV14k2uRJrU/l3HW1yJdgmQc/3Atrkysfe-->
<!--                        lxNtcmX0vpxokyvRNhX/lfS0yZdom2q/62iTL+E2CXrCK2iTL6P15USbfIm3SdAjXkCbnAm3qbJO-->
<!--                        tMmZeJsKv+tokzPvnS8n2uTMSLicHughn0GbnMmwqey7jjZ5k2GToId8Am3yJsOmqpcTbfJmNNaJ-->
<!--                        NnnzR4pNgh5zCm1yJ8WmmpcTbXJn9NWJNrmTZJOg55xAm/zJsani5USb/EmyqaBOtMmfkWSToAf9-->
<!--                        DdrkT5ZN9S4n2hRAlk3ldKJNAXzMsknQk/4CbQpgdL2caFMEaTYV04k2RZBnk6BH/QnaFMFoejnR-->
<!--                        pggSbRL0rD9Cm0LIs6mUTrQphMTLqdK7jjaFkGmToIf9Dm2KIdGmQj8QRZtiyLyc6rzraFMMqTYJ-->
<!--                        etpv0KYgMm0qcznRpiBSL6cqOtGmIHJtEvS4X6FNUaTaVORyok1RdLycaFMUuTbV+KITbQoj16YS-->
<!--                        7zraFEby5SToed9oUyDJNlW4nGhTHMk2CXpe2hRJ9uUk6IFpUyTJNuHfdbQpkGybBD0wbQpkdLuc-->
<!--                        aFMk3S4n2hRJ+uX0ATsvbYok3Sbwu442hZJuk0DHpU2hNLucaFMsvS4n2hTLe6vLiTaFkfOPGpTS-->
<!--                        iTYF8CfIo68IbnDa5AvUI/jlRJu8GGiJ/kVgHdAmB+qI9BVYEbTJRjWRviCoNmjTOiVN+sID1Aht-->
<!--                        WqOuSV8AtUKbFkC78hrBFEOblAy0KPfAlEObNGyi0mfU5USbbvMRbYgKSEW06R5oOdQIoiXadAO0-->
<!--                        GUsgiqJNr0BbsYoAuqJNlwy0EwYe+XXRpudgv7HETn5jtOkZaBfsSHpntGnKQJvgQnpttGkC2gIv-->
<!--                        JLs42vQrqG/njiC7O9r0MwMtgCuS3B5t+pGzXPqcfjnRpu8c51L65USbvnGgS5+zLyfa9JUzXcq+-->
<!--                        nGjT35zq0ufky4k2He1S8uVEm452iTblgj7tcDLLbG4T+qgTkMQ6W9s00CedQmKhjW3q4VLq5dTX-->
<!--                        JvQh55HXaVebBvqIE5G0VpvahD7gXNJqbWkT+nSzeWQV29Cm3X94QI9kVdvPJvTRIsjqtptNJ32f-->
<!--                        7n0kqd1mNg30uYJIqreXTehDhfHI6beTTQN9pjgkp+FGNjWWKetV18em7PP79DTJANgkKR23sSnv-->
<!--                        4B638iQr9elWKCtNbEo6u4cmU65OKTX3sClBpoWHP9Umyei5hU1FT2qk6pRRdAebyp5Sqk4ZTTew-->
<!--                        KfCEpG4096x3ON6muO8Y8PhrUqZOCWWfbtOofTZh8WhTAEGnJW4BE23yC/2Us22Kkck1YqJO8X0f-->
<!--                        bVOETN5fVI7I+IRHeOEn2/TufyDinzJPp4Dwv3CwTbscR5pN8a+6c2366H0Wj6CgI82mqAn+5Vib-->
<!--                        vGWSuKhpOgXO8JVTbdrqHLJsCn/VHWrT+0YuJf6EX3TtZ9o0tjqDvHedBM9xpE2uhyMZiZNsin4w-->
<!--                        TrTJU6a/ciJnveuCxzjQJk+ZHjuGvkBipzjPJsfHPOdb87+SY1PwVXueTX7VPzJjjxydYoc4zia3-->
<!--                        3jMvpr/J0UlCZzjNpk1qD41Om5wYXq0jwqfoFDrBWTZ5ySRbp8eNdpZNOzy/8fFpU6XT2H4A2HQn-->
<!--                        2TTqP7wpE9CmMkch0BkSbIoc8BybfH4/KnoK2lSDI2TKeNcFpj/GJo9jEPQQbyE/aEObtJwiU8K7-->
<!--                        LnDMU2yq3bIGj+fimrjsh9jk0PEDPcM3wnWKi36GTQ4HgB7hB6JtkrDkR9h0lkzhl5OEJT/CprNk-->
<!--                        itcpLPgJNtnLR0/wK7QJxnkyReskUbEPsOlAmYLfdRIVe3+bzMWjB5gSalPYyNvbZJbpgZ5gDm3a-->
<!--                        sXVBD/AE81OCGHp3m6ylR/VqhzalY/2mpqhaPaBNmzWe/ROYKkagTkGR97bJWjg6/zW0aau+0fFf-->
<!--                        YH1WLpCYxFvbZKw7qNIy89GmxLKDGvUkzKaga3lnm0oW6orxeaFNaVWj49/iPcomCYm7sU0V6yw2-->
<!--                        JG26yyjYZrUpadNdbG2i0yeNmVzAtjbZHlpBx0+akzbdo2CXMQTpJBFZd7XJ1jE6vQraFI3tewdC-->
<!--                        mgzD9uDQpte8m5pEp1cSYlNICXva1OlqegvSKSLonjbV6zGUEWGTBATd0iZbu+j0C9CmQN6rtRiN-->
<!--                        7fGhTVfYPmtCp18iQqeAmDva9F7tkUyANpVsFh1+kUGbChYr6PSr+NsUUMWGNlV7IHMwPUO0KaTW-->
<!--                        gAazeKdNAZgaRIeHDZ5TxnY2jWKP4yaT06YpxfrbZnTaNMH0gAo6PXD2lDp2s6nYw7jR8LTpN0yP-->
<!--                        p397O01Pm37D1B46PHj8hEL2ssn0cAo6PXh+2uTZJjq8B7SpSJmCDu+B6XGiTX5dosP74GqTeKfb-->
<!--                        yiZLc6V/xeV9TA8UbfJq0r05ELSpQpHo8F6YHina9J1SxW3ZQvQjtpFNpqcSHb5IDbTpG5baBB2+-->
<!--                        SA+06R9Mz6SAw3vyTpvslKoNiemxin3Ietjk3dq+TdCmL5ieSGz0UlXQpr8xtYaNXqsL2sTPwf3K-->
<!--                        oE180f2Ml03ezexiU6XO8NAmE8NSmSCT16uDNlWqrAK0CdYeMngQgzaByhNg8DCcbHLuZg+bKj1/-->
<!--                        NTA9X7SJNv2E7Xd/trbJ9CA6F1YF2gRp7oHLHYnpCaNNq+BiFy6lsU2mx9C5rzr4XE6+mXawqdLT-->
<!--                        VwjalF8bLHU4LpeTb6QNbLK1hkpdvZeuNpnaElTq6sXQJtr0Ex6Xk2+i+jbZOgOFzoE2qTHZJKDQ-->
<!--                        G1QTUVB9mwqVVQ7apMT2/GEyp0GblNCmsHIa2mTq6pBfKBfUDm1CdlUQ8+Xk21B1m2x1+XZVENqU-->
<!--                        WBckciq0Ka8tSORUrJeTuKYpbpOtLN+qamK9nFzD0KbNoU1ZXSESZ2N73mgTbXKsqJNNxgcPkDgf-->
<!--                        2nQXm00CSLxbR51sshUlgMTbleQahTZtD226xyhUVF1sLblGoU3bQ5vuYZPpr/zAGGhTfE1dPm2i-->
<!--                        TfcYtCm+J9cktOkAaFNwSd49laZMS7TpAEaVlmjTAdCm2I4+d/q0yfQrVV2D0KYToE2RFdEm2uRX-->
<!--                        UTebBm16gfX3qyfHhUKbAhtqZ5PhHneNQZuOoEhNdW0yykSbaJNLQV84/tej/MSgTdcYbZLkuFho-->
<!--                        U1Q/tIk2+fXT0ab1m9w1RVmbjDLRJtrkUU9ES/Wp0RNtOoNRoqeqNv1Jm1TQppB2mtq0/F81XVPQ-->
<!--                        pkMo0VNVm95pk44SPVW1ySqTpKYtwOrj5xqCNh3CoE3PoU1KaJN/N7SJNvl109em1dvcNQNtOgXa-->
<!--                        9JSPtEkLbfKuprNNgzY9w2xTZtga0Kan0CY1tMm5mdY2LT6BrhFo0zEs9SSuEWjTMdCmJ9CmBd5p-->
<!--                        0xyzTB1tWnoExTUCbToG2vQE2rQAbXoCbcpqTVwT0KZzoE1TBm1agTZNoU1L4HuiTefwEd4TbTqH-->
<!--                        ldp8E9Cmc6BNUz7SphVo0xS7TLSJNn2DNi1Bm6bQpqzenH9zMW06CH1N4huANh0EbfJphTat9Sa+-->
<!--                        ASraNGjTGrRpAm1ahDZNoE2L6Gt6+AagTQehL845AG06CNrkUgptWizOOQBtOgja5FIKbVoszjlA-->
<!--                        RZscZKJNtxDnALTpIGjTBNq0CG2aQJsWoU0TPGzy7mkL1DY9nAPQpoNQ2+QdgDYdBG2aQJsWoU0T-->
<!--                        aNMitGkCbVpEa5N7SbTpIGjTBNq0CG2a4GFTyy9f0qYJtGkRrU3uAWjTQdCmCbRpEdo0gTYtQpsm-->
<!--                        0KZFlDaJe4BjbfKvqj60aQJtWoQ2TaBNiyhtergHoE0HobTJPwBtOgjaNMHFpo5/qaNNE2jTIjqb-->
<!--                        xD8AbToI2jSBNi1Cmyb42PRIy1uGj+jH7VybJC1vGXQFBQSgTQdBm8yl0Ka14iL6Odemhp+G0yZr-->
<!--                        KbRpsTgJCECbDgJeD206CHg9B9skaYGrQJuspdCmfxnwdmjTOdCmGU42/ZUWuAgqm0ISVLRJ1Up2-->
<!--                        YYWhTeZWaNNabyEJTrZJ0hLXQNNbTDe06Rzw3dCmc9B0E5PgZJu6feKEr4Y2nQO+Gtp0DopmJCZB-->
<!--                        RZv+9LIpqLOiaB7CoGYq2uT1xXDa9JSgCEfb1OtVR5vm0KYVFDZJUATadAy0aY6bTVGtlaRAL7Tp-->
<!--                        GBS9REUoadNw0ykvM54CtdCmY7jfikRFONymsN7qoSgtrBXadAqK0sIyHG5To1cdbbIXQ5u+cb8T-->
<!--                        CctQ0ia/LxE0etVV6IQ2ncL9TuIynG5Tn1ddhUpo0yGM241IXIiaNt2vBtldKWiTQzW0SV1ZYIjj-->
<!--                        beryqrvdx6fAELTpEG73IYEhatrk9nMGwe3V4f7zF5mipk2ef6nrcTnRpgtok5LbNklkigY2hfZX-->
<!--                        hRptFLXpnTbpuN1GaIqiNg3PyykzOIj7f20JjdHBJslMjuF2X7Fd0KYjuN1XbIyiNrl+Gt7gVVek-->
<!--                        ihY2SWp0BEWaoE0nMIo0UdWm2/3cIjU6gNttBefoYZOkZs+nSg+06QSq9FDVJt9PnA5/1d1+9KKD-->
<!--                        NLFJcsMnc9emyG+U+0ITm86+nMo8U2Vtuvu80ab7NoUH6WKT5KavWVV4krI2eX4z7+GX012bJDxJ-->
<!--                        WZu8P3GKrxJGnQeKNu0PbXrNcNYpOX69oiQ+Sh+bEsrEUOhxqmuT96fhx15ON8cP/9LlW2WbvD9x-->
<!--                        OvVyGoXGb2TToZfTe6HpC9t096G7zYfsCVIodDW1siml0KotpQxf2Ca+6u7wsdLsrWyS9BHKlJQz-->
<!--                        emWbBi8nt45y0vSySdJniKbWc1TZJv9X3XmXU63HqJlNSa2mMWo9RaVteufl9IJiD1Fpm+4+eW0v-->
<!--                        p7sFZU1d2qY//G0663KqNnRpmwI+cTrrcqo2c22bBi8nj3rSAvWzSQBzBFFu4to2Rbzqzrmcyl1N-->
<!--                        HW0SxCDIcvISFbfp7uOn4oGYBNeN5EXqaNMh77qC0xa3KeRVd8a7ruDV1NOmIy6nirNWt+nuE9ju-->
<!--                        crr73wlSR61uU9Dl9MAM48h7waupq03bv+tGyUHL23S7Nh2CmcaNmnOWt8n9B8iPuJxqXk31bYp6-->
<!--                        1e2tU82raQObbj+GSpKLxnSSnKuvTTvrVHXE+jaFver2fddVvZp2sCnsctpWp6pXU2+b0tv2oe7T-->
<!--                        soFNca+6PXW6/XTlT7eDTXGX05bvusLDNbdpQ51ujyb52XawKfBVt59O9x8tQLgtbLrfoB7BjbVE-->
<!--                        6cG2sCn0ckK0ntEEIh1t2kqnUXuqPWy6X+IKD+BkYT1A8u1hU+zltM9n4vdHEki+TWxSPJQH61R+-->
<!--                        Itq0j06KEgSTcBObgl91W+ikeaJAEXexSVPloTophhFQxF1sCr+cyuu0wyzb2DSa66SZRFAht7Ep-->
<!--                        6mdXNtFpj0G2sSn+VVdZp6EZQ2Ax97FJVegiD/CMPrPjcu5jU8blVPS/2e0i00426To9SCfd4MgJ-->
<!--                        NrIp5XIqqJPyKUJG3ckmZa2roMe0TS3IrLSpuE7aoaFhd7Ip51WHPpGf2UqmvWzSVruMoCddnRgc-->
<!--                        fCub0i4n9Kl8Q/34gPPuZZO63XXQoy6N+wAH3sumvMupgk4ftZEFnXgzm9RPqwEBz7rhA7CZTZmX-->
<!--                        E/h09HEf6MPZzqbRRKeFOQV9NvvZFPHvkBc8oQWZ8O+5DW3KfdeBzmiboL+wn00rj60FyR9xk5i/-->
<!--                        s59N2ZfT588fcucbKxk/oU/lCxvatNS2jczx6id8zoY25V9Oie+RsZbvA/pMvrKjTYuN7+BT6XCv-->
<!--                        2dGm5K8S5B3ZcjT0gXxjS5sQ77r4QxtFcynY06b14sv6dPdfVp0g6OP4lz1tQl1OcT4ZEgn6ML6z-->
<!--                        qU0JP0ee6ZMljqDP4gc2tQn3rgs4wHJyL7OrTcB3ne8ZGj5fKifTvjYNsE4+52gOIehz+IltbSqg-->
<!--                        k1UohwkEfQo/s69N6HfdPzwW0797bC7oM/iFjW1C/r3uJ/T//d5rZ/QR/MrGNpV4131Dbqd23BR9-->
<!--                        AL+xs01F3nX3jzd3NwBb21RPp3/466eUI2aTB7r939nbpqBz2gJBlz9hb5sa6yTo6mdsblPZd11L-->
<!--                        mba3qalOgq59zvY2DfTBUqbvbG9TS53QnT9jf5savuvQjT/lAJva6YTu+zkn2NRMJ3TbFxxh00Af-->
<!--                        MGX6yhE2ddIJXfUlZ9jURyd00dccYhPox38p08+cYlOLz8T/stcUyzE2NdCpxu9ouuIcm47XSdAF-->
<!--                        v+Ygmw7XSdD13uAkm6w/6FgaQbd7h5NsqvNTLE1lOsumc7/sJOhm73GWTafqhK71LofZdKZO6FJv-->
<!--                        c5pNJ+qErvQ+x9l0nk7oQhWcZ9NhOgm6Tg0H2nTUlzEF3aWKI206R6cHukkdZ9p0ik7oGrUcatMZ-->
<!--                        OqFLVHOqTQd8Li7oCvUca9P2Ogm6wAXOtWlzndDtLXGwTTt/8iTo6tY42qZtdRJ0cYucbdOmbzt0-->
<!--                        a8scbtOOOgm6s3VOt2m/tx26Lwvn27SXToJuy0QDm3Z626GrMtLBpm2uJ0H3ZKWHTXtcT+iS7DSx-->
<!--                        aQOdPqArcqCLTdXfdoKux4U+NpX+UWB0N040sqnu607QxXjRyqaarztBl+JHM5sKXk/oRjzpZlO1-->
<!--                        6wndhi/9bKrkE7oJbzraVOV19wHdgzstbSrhk6A7CKCpTfDXnaDnD6GtTVCfHujZg2hsE8wn9Nhx-->
<!--                        tLYJ8fmToEeOpLlN2b95FT1tMO1test74Ql60HBo098kfHuBoGfMgDb9w6BKdmjTd4KEEvRcedCm-->
<!--                        n3AX6gN6olRo02/4qYSeJB3aNIXvtyVo03N4J2mhTS/gjaSANt2DHt3hvk2EEEIIIYQQQgghhBBC-->
<!--                        CCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCGEEEIIIYQQQgghhBBCCCH/8P/T2g3wTNSy-->
<!--                        bgAAACV0RVh0ZGF0ZTpjcmVhdGUAMjAyMC0wNy0wNFQwMzo1MDozOSswMzowMFesjGwAAAAldEVY-->
<!--                        dGRhdGU6bW9kaWZ5ADIwMjAtMDctMDRUMDM6NTA6MzkrMDM6MDAm8TTQAAAAAElFTkSuQmCC" />-->
<!--                  </svg>-->
<!--               </a>-->

               <a href="mailto:vinija@gmail.com">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuBAMAAABTjO+8AAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAALVBMVEWxsLDGxcW4t7esq6u+
                        vr7Z2NiqqamxsLCvrq7Ozc2ysrK1tbWenZ2dnZ3////zevNgAAAAAXRSTlMAQObYZgAAAAFiS0dE
                        Dm+9ME8AAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkBwQDLRvUSpUpAAALt0lEQVR42u2d
                        PW8jyRGGR7Lk3XXEyNqQwAVKBewtiHMkwAfQoQ4L3B02ImCs98KVuFZzFfECm3Am4A7YDXmR/sQF
                        /gHO7+94R6Somarq7qr+mGra/YZifzzsfmc4XdNdappcOvjixZcX2VrPpj+az1r8QxtDqp/MRmfa
                        IDI93WKbxUgbRaS3D9zmUhtFolfmUUttGL6OOtjm+kIbh61Jl3t/nPLK9LXUBuLpAGDvi1NWkHs/
                        nPKtwVprQ/l1QGDvg1NWFLf5URvLp28NrbUCy+sfzJB6MUqD/SoeRagkk/M0nkOqFA+Rx2+H5zY3
                        8dzfK2CncIrGcBtztYfuvtdtJPdHJe7ZXtrEmPdx2MdK2OZ6P+1tzEUU9+/VuMdR3E/UuM/+L7lP
                        9pR7um3lry+G0iQp93lUKxL9rnJX7spduSt35a7clbtyV+7KXbkrd+Wu3JW7clfuyl25K3flrtyV
                        u3JX7spduSt35a7c+809Men0rnJX7spduSt35a7clVuiOzXuZQz2QyMK3POLcOzjHd/w3DEnBaa7
                        xgH3z1+nE2h6xx3u8McmFNYN4U55dIkOd6hTpkaZO8wp3QaUuEOO93ZdosUdchC86xI1bvmvT7+6
                        GrfUKX2X6HFLndJ3iSK3zCnoWJoet8Qp+EyxHrfEKSemIG6+U4jDi5rcXKdQJ881ublOwS5R5uad
                        SiePuOpyc5xC5yfQ5eakIaFcos7td4rlILQ2t88ptiwW2tw+p9AuKYDb7RTrcXl9bpdT7LlO9Lld
                        TrG5pAhuu1McSRVK4LY5xZURpwRu8y+6ht0lhXDTOSOcqTfK4KaSi7jzJpXBTTnF5RLE/eeEiRXO
                        BNzYKX8wEu6JSad3NPeC5RSLS+ZTNe6XNBFwynPLMOtxnx3SxXtOsbjkjer7tE9k8a5TLC5ZKL8H
                        pHvpOMWSg22kzH1EV9g55XhFfv6m0X7vSieP3CXL+97+sS63JVnn2jXcm9yFytxkctSHAadza82a
                        ArgtTtnQ3FEfbRMAanPTcB/aT55Rn8xvC+E+WFF03YI9bWvpc9MuXjb4lUir3Ztafe7mlKjzgWZ4
                        XMkVwA1fkd0D0j9Ky8bDnVF4vw9l5FvKP51IRQnclJPPmu8cLimEm3jse0fYft1ppQhuYsl+iROW
                        9lYUZXDjZc0NujX0V3CFcCOnzBv4TcYlcjeHPu6/91sphRvmDUbcoHgx3E8qd+Wu3JW7cv9Pcpf6
                        ezn1cLdBwQK54ZoHcy9K5EZrTMxtXhbIPTV+7h5jGdx4DUxxz0vjfrZicZuvCuM+NTzuzpqnBG4q
                        JktzP64xC+C2RDYp7sc1fQHcd4bPvYuh6HPTkXsb90PMSp2bflNi5X6IEapzr4TcW6dYuH/9Tzr9
                        28Vt+zdydu6NUyzcv5l0eufgPrBVsnNvdl0rc08CuHsvUXS47f8pz8XdOkWV+8heycXdvp1S5Z4E
                        cn+uq8n9yVHJzT2/UOQ+dFVyc5srRe63Edxmosa9cFZC3H+iyw3P3dfio4f7jN7Zps09e+LjZp2D
                        GZr7hhFn45w7Gph7MeLEB0+K437DimsyztUN8BzbUbufisFNVQXcGUV0PmJyExsQNLnbfYw8brxV
                        RZF7E2hlceOAnCL3uYAbhbb0uLfhYSY3DCW+DOaQ6hQCiriRU4YacNjvWMgNw1uR/9+WKzjPu4Af
                        m/vA1kJWWUeLzY1CLmM5hViwz3UANwxxhSR/EQrOcWdDoIAbBosYh6oj5RgpAbdj1vII9rfsfCbh
                        hgGMzE6B89tLryDiPnK1lFzO60nEjQJ0Sz6FWK+cJDJuuJszo1Pg3IJkYULuQ3drCQWuJZicTciN
                        gnRnPAqxfgL9zMDnUm7olJh0hg7Beb2CBcTc8KhgHqeA0VncRnOjPcwzP4VYn7x9yLlhWAKPRbTg
                        nN7gInJu1OqVl0MoNDKjJNwogPUmMfdzRvsh3JzxiBAMSC6oQiHcqOWbJqFQWI8clSBuFMBK6RSe
                        C8O4YQAroVPgXM7pYmHcKBqzaBIJueQ8KTcKYDGc8jNMxUsJOtAWYArlRnvhRh6g1+1AXvtKoSyJ
                        toKh3CiQNG+c2q4C5u4hR4HfcXJutD/LGTLcrRXdKw3oPntwKZwb7YdzDeUdAwXPoeNLhnOjMIHD
                        KV3bjqyl0DWztrcYwY2cYh3Knm3tt0zoEldgKYYb7XYaWwqe9ErZbplw/pyXQgw37okuBn8CR/Qo
                        QJcsXV1HcaPdfOTMop9A+jEMus4dVIrjRjue1kShE1iIdAq6yi8ycqOgEuFJIucO8RiGrhVPx5Hc
                        aK8Wcgq5nwIv7aDjfGGCWG6062kJCtD7V2agFHTJ4sLTbyw3ChkAp1gyM4EgALpOZr5uo7nRwfVL
                        56e0UybOT/NwoxHtVrHv+euWglc3IyQTz40c3LmDOfb8uUoxFiEJuNEd4/FeYHOJuxQnPJCA2+4U
                        +/7hVkuLl1iL7BTc6Bdxmw3IsX+41fbOE+KSRNzIKZv7wcTNvbnzoLq80EASbrxXb9bYzyL0nYJ+
                        lzguScWNwgef72ToieP6G8Ip6Npg7mxJxI0W4jf4xMoaz8CPREKQQbnxNqi/wT+0q7g7+Ef4ddn7
                        cVJxk2mlep5oC5Fn4uCXG5b7mQdpfF/Kc63ydxEl47ak0YMD6Z6WsQK3E2l3uTmnRbCjJSG3y7zn
                        nGmRvO1PyO0wb/embJ+WtaCvlNz0UVQDbspUXrh7iXazJOW2Hdcb9UpZMjXL9oQk5bY4BT7gTclS
                        S1FPabnJ46joAY8MTQj3PCXmJpxCLAOIox7S/SCJuYklDrUMOEGlpP2k5j6GQORiETlFvE04NTcc
                        SctiET12S984J+ZGPDNLQbjMkb5xTsttWWhGlRyCG43irbUoWsbP2L0k50YsrtoobHLL7CU9N5z7
                        9wlLZ+SGI+j5LRHNTkZuMQf8jZL8ZibknojnXV4jAzfjFRUUCiAuh+cOYgj4rqm5V6Ai77k0rFZC
                        btHL9Ueh5971sNzB/Qd+31TccL75kZDAmmm4Q0etIaIu6+G4Ud9jNjZ7M0gO7jtQSXZWMKh2Cm60
                        nUuEjUOG42G4Ub/nMu6g752A+xRUkZ89Dmghnlu4cZPSs4l4xqK5j+V9YuG0qtm5p6BC2BZ2cSux
                        3Ik2gqNZG+XlFvdnE+swRjrukyQuCWgpjjvhwRLh4Zoo7qQHeWRjEMXNOdjEl6i1GO7EB9VEB/Yi
                        uJMfDJSMQwR3+oOYghbDub1HaQPEn8FwbtBHkoPG/OPiwdx5DnazWw3l9hzLD9aEOYuh3KD9ZIkL
                        uCkoArnzJYpgthzGnTMxB28mw7hXmVzSypn2JI47b+IZVush3LkT/XBmM4T7LqNL7scFgFPjEsCd
                        Pz0Rowc59xCJw+CMpuD2txkv/9iIuW0J6tLKm1RNyg2DmF+xMOQ6hVyR3L72UsmXqFHIPVz6RE9P
                        Mm4YnsqZrnLqnFkZN2grWVYISnCM+mEJETcMYo5ycrt7k3DDwEPq/DhQrtmVcJ8M6BLfOAm4QQAv
                        cVYfSo4e+dxDu6QVmOFOqJPP/dzaRj7Zx4rNDYJ3GbKEUbL2yuWG33w2CDaa5V0wj8v9yVI/t2zj
                        xeQGgbuBXEL1fCHifkv+dRCBmX4v4abrDiRyzFjcIDyVKTelTYdU7yzuiZ5LWoGQ4SWXm6o3qMC4
                        LXncIDw1QP5sqCNMwOBe4W87tMDy/pLDDerkz/pNCYzd2s+t75JWiMLLfQe/qY7grPu4/1mCS1qB
                        8Tv1cIP5UcP2HGH3cI/1uN1H2N3cw/yvCZtOQ7mzBQN5ch1hd3Kf63K7jrC7uIf73zU2nYZwK7uk
                        lfUIu4v7XJu6sR5hd3FffV2CJmLuslW5K3flLk+Vu3JX7vJUuSt35S5P+8p9vafcc2/y1DJ1RWb8
                        Kl8fmo/aCEE6sy/1i9bal9S4UDXu3Nelqn2B/Z02RIBmjT1pY8HahC5PtDHE2mxf8mXeLU4PkeKn
                        8U0Nir2LFL9eabMIdN0JcB//5cW+6JcN8X8B85vetwnigQ8AAAAldEVYdGRhdGU6Y3JlYXRlADIw
                        MjAtMDctMDRUMDM6NDU6MjcrMDM6MDDsnuMrAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA3LTA0
                        VDAzOjQ1OjI3KzAzOjAwncNblwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>

               <a id="theme-toggle" onclick="modeSwitcher()" style="cursor: pointer;">
                  <svg version="1.1" class="mail-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                     viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                     <image id="image0" width="16" height="16" x="0" y="0" xlink:href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAMAAAAM7l6QAAAABGdBTUEAALGPC/xhBQAAACBjSFJN
                        AAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAACYVBMVEU/PzpEREBBQT1CQj4/
                        PztAQDtHR0NJSUU+PjpISERDQz9AQDw5OTRFRUE8PDhCQj1CQj89PTlKSkY+PjlNTUlLS0hEREBD
                        Qz9aWleHh4WtrazBwcHCwsLCwsLCwsLAwMCurq2IiIZgYFxXV1Sbm5rFxcXCwsKgoKBkZGFERECX
                        l5bExMSPj45LS0empqWpqahUVFCnp6axsbFTU09CQj6lpaSpqahISESRkZCNjYtUVFG/v7/FxcW7
                        u7vExMVhYV6ampmTk5FXV1S3t7eenp1VVVHCwsOYmJd3d3XIyMjCwsJdXVqEhIKrq6uGhoSnp6aX
                        l5aAgH6srKzAwMBdXVq8vLzCwsOZmZhNTUm3t7bDw8PCwsKYmJexsbCYmJawsK/CwsJOTkq2trXD
                        w8K9vb1bW1jBwcK9vb2pqaiXl5aCgoCvr66AgH6jo6OGhoNYWFXAwMB9fXvIyMjGxsZeXluamplM
                        TEi5ubmcnJteXlrCwsLGxsaTk5FDQz+dnZzJycljY2CJiYe+vr5bW1hUVFCcnJuVlZRGRkKmpqW4
                        uLd8fHl/f33AwMCioqFFRUFQUEyurq6wsLCFhYNkZGBSUk9SUk9hYV6JiYenp6bHx8inp6ZKSkZP
                        T0unp6bExMS6urm0tLSzs7O4uLjExMSioqGMjIrExMTKysuVlZRFRUFiYl6hoaDExMTIyMicnJtr
                        a2hhYV6Li4qxsbDDw8O+vr2zs7KHh4VlZWPHx8fGxsbCwsLJycnIyMjDw8PKysvExMTKysrMzMzL
                        y8vFxcXJycrFxcbGxsfHx8jIyMnExMX///9/oPL/AAAAuHRSTlMAAAAAAAAAAAAAAAAAAAAAAAAA
                        AAAAAQEYUJzK4+3kzJxYGSKE5+qSIgJe7GoGlqYMprcLAZapAl1uH/H70vMkhWYP1ZoW7G5G/fMb
                        UbpinWtbrMsd1OVuBtfn7m3IbMnlBNTozhzz1Z5sVq5Yt2YW7zf48iCTD9CiH/DzZwWv9Ctp0QsQ
                        rnABqNFKO82sAg22uF0fBwUfV7T7uw0Lp/PYycnV8qtu7/JxASeZ7vGgLh5hqebTrWUhilEqqgAA
                        AAFiS0dEyvO0NuYAAAAJcEhZcwAACxMAAAsTAQCanBgAAAAHdElNRQfkCBYKLR1KuANWAAACD0lE
                        QVQoz23TZXvUQBAA4CG7R09K4ZANFtwKFHcvTrHi0BZ3d5dCKe4uxd1d7jbZJCQFwr9ik1wud83N
                        hyRP3uzOk5lZIE6IUK95i5atWktSm7bt2ncQQHTfg3MVUMdOnRNJ6kRS7tK1GxZSXEhIqH53SWE0
                        HUzp0TMPe6txUS9Vo1nB1N59wi6HivrqNBByv/7Y5gGRgTmU+6DBAufwkF85kCczhoZFwMOGa0Ed
                        MVKjbNRogOgYOahG8dhxJpXHx2DCxOBiY1I0f/IUykqmwjQllwpig+kJWjsDZiYCWop4yQpm6TQ5
                        G+bkVhKaW8LYPJhfR9UFUafcaOEik5ZBebbqFa4SlLf4Ny2vw/oS5CqJRpbavCxr5wpPScPlK0y6
                        ElZlNNJI5bUjtHoNY2th3R9f1/tKCjbINLkRNtWmdy5t5KsY2/yXWltg6zbNqxXylcS376Bs5y7A
                        u+V0JX2N7dlrUmUfArz/gL384KFMDR8+olBWeTQOJH7M4N2vOp6PPIzi6hN8gIyTSASCTp3mz9qZ
                        s43jYQEhAZoI587zPqkXLtrDRPClyzy9aV25eu1602Y3bt66fYen0+/WhNw5x/fuq8we/wcPHz1+
                        8tSyW2w8q8HeKcGR5y8s/gHTTPOffVdevnodyzhE+M3bd7Lp1JeZ1vsPH53/KEwx/xX06fOXqq+S
                        VPbt+4+fQjx1BP8DniGUSqIRNGsAAAAldEVYdGRhdGU6Y3JlYXRlADIwMjAtMDgtMjJUMTA6NDU6
                        MjkrMDM6MDBYVnojAAAAJXRFWHRkYXRlOm1vZGlmeQAyMDIwLTA4LTIyVDEwOjQ1OjI5KzAzOjAw
                        KQvCnwAAAABJRU5ErkJggg==" />
                  </svg>
               </a>
            </li>
         </ul>
      </div>
      <div align="center" class="footer-col-1 column">
         <a href="../../index.html">www.vinija.ai</a>
      </div>
      <!-- <div class="footer-col-2 column">
         </div>
         
         <div class="footer-col-3 column">
         
         </div> -->
   </div>
   <!-- add permalinks to headers in kramdown -->
   <!-- <script>
      var headings = document.querySelectorAll("h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML =
              '<a href="#' + headings[i].id + '">' +
                  headings[i].innerText +
              '</a>';
      }
   </script>   -->

   <!-- add title case to section headings -->
   <script src="../../js/ap-style-title-case.js" type="text/javascript"></script>   
   <script>
      var headings = document.querySelectorAll("h1, h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]");
      
      for (var i = 0; i < headings.length; i++) {
          headings[i].innerHTML = titleCase(headings[i].innerHTML);
      }
      
      var toc = document.querySelectorAll("a[id^='markdown-toc-']");
      
      for (var i = 0; i < toc.length; i++) {
          toc[i].innerHTML = titleCase(toc[i].innerHTML);
      }      
   </script>        
</footer>

    <script src="../../js/nanobar.min.js"></script>
    <script>
    var options = {
      classname: 'my-class',
        id: 'my-id'
    };
    var nanobar = new Nanobar( options );
    nanobar.go(100);
    </script>     

    <!-- Scroll bar -->
    <div class="progress-bar"></div>
    <!-- Script used to generate --scroll variable with current scroll percentage value -->
    <script>
    var element = document.documentElement,
      body = document.body,
      scrollTop = 'scrollTop',
      scrollHeight = 'scrollHeight',
      progress = document.querySelector('.progress-bar'),
      scroll;

    document.addEventListener('scroll', function() {
      scroll = (element[scrollTop]||body[scrollTop]) / ((element[scrollHeight]||body[scrollHeight]) - element.clientHeight) * 100;
      progress.style.setProperty('--scroll', scroll + '%');
    });
    </script>    
    <!-- theme switcher -->
    <script src="../../js/mode-switcher.js"></script>
    <!-- mathjax -->
    <script type="text/javascript" src="../../../cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJaxdda6.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <!-- make mathjax responsive -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
       "HTML-CSS": { linebreaks: { automatic: true } },
       "SVG": { linebreaks: { automatic: true } },
      });
    </script>
    <!-- Copy button -->
    <script src="../../js/clipboard.min.js"></script>
    <script src="../../js/copy.js"></script>      
    </body>

<!-- Mirrored from vinija.ai/models/diffusion-models/ by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 12 Jul 2025 14:01:40 GMT -->
</html>
